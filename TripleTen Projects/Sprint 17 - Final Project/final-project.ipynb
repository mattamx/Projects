{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Part 1: Project Workplan"]},{"cell_type":"markdown","metadata":{},"source":["**The aim of our project is to predict churn rates* for our customer, Interconnect.**\n","- From our results/final report, the company will be better prepared when it comes to forecasting which clients are in a higher likelihood of disconneting their services (which more than likely is to a competitor with more enticing perks).\n","\n","\n","- Interconnect will then plan on offering a larger scope of benefits to those clients in danger of turning over. This may also prove particularly useful in its market share battle against its competitors as the telecomm industry sprints to expand their perks each year as new cell phones come out (a constant cycle).\n","\n","-----\n","\n","`Step 1 - Initialization`\n","- Import all required and prospective libraries that will be leveraged in future stages of the project.\n","\n","\n","- Download the data and briefly inspect each DataFrame structure.\n","\n","`Step 2 - Preprocessing & EDA`\n","- Determine the necessary process to clean and massage the data.\n","\n","\n","- Create short, concise summaries on each DataFrame along with visualizations (plots) for thought organization and guidance.\n","\n","\n","- Perform EDA, including but not limited to: dtype and naming revisions, class balancing, value scaling, feature engineering, encoding and merging.\n"," - Fill in any gaps due to changes made to the DataFrame(s).\n","\n","\n","- Deploy time series tools/analysis to get a sense of trends and seasonality to paint a more complete picture.\n","\n","`Step 3 - Model Selection, Training and Fine-Tuning`\n","- Select various models based on the goal, binary classification. Compare initial scoring performance across the model selections, incorporate a dummy model benchmark.\n","\n","\n","- Fine-tune models using hyperparameters and gridsearches. Include gradient boosting techniques. \n","\n","`Step 4 - Model Evaluation`\n","- Decide on the optimal model based on cross comparisons and boosting techniques. Perform model evaluation based on a new set of data (test dataset) and document results. \n","\n","\n","- If statisfactory, record why the model was selected, its results (along with speed and accuracy insights) and what needs to be done in order to monitor/manage the model in a go-forward basis.\n","\n","`Step 5 - Comprehensive Report`\n","- Create an extensive report on the process/steps taken, the results and the overall recommendations.\n"," - Illustrate findings/results incorporating 'quick hits' or 'highlights' so the customer has a better handle on the report and can easily share internally.\n","\n","\n","\n","\n","--------\n","\n","**The churn rate measures a company's loss in subscribers for a given period of time. The cost of acquiring new customers is much higher than it is to retain current customers.*"]},{"cell_type":"markdown","metadata":{},"source":["-----------"]},{"cell_type":"markdown","metadata":{},"source":["# Part 2: Solution Code"]},{"cell_type":"markdown","metadata":{},"source":["# Initialization"]},{"cell_type":"code","execution_count":64,"metadata":{"execution":{"iopub.execute_input":"2023-12-01T00:01:09.452543Z","iopub.status.busy":"2023-12-01T00:01:09.451781Z","iopub.status.idle":"2023-12-01T00:01:09.464746Z","shell.execute_reply":"2023-12-01T00:01:09.463770Z","shell.execute_reply.started":"2023-12-01T00:01:09.452511Z"},"trusted":true},"outputs":[],"source":["def warn(*args, **kwargs): # attempt at removing warnings\n","    pass\n","import warnings\n","warnings.warn = warn\n","\n","warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n","warnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n","warnings.filterwarnings(\"ignore\", category=FutureWarning) \n","warnings.filterwarnings(\"ignore\", category=UserWarning) \n","\n","from warnings import simplefilter\n","simplefilter(action='ignore', category=FutureWarning)"]},{"cell_type":"code","execution_count":65,"metadata":{"execution":{"iopub.execute_input":"2023-12-01T00:01:09.466923Z","iopub.status.busy":"2023-12-01T00:01:09.466547Z","iopub.status.idle":"2023-12-01T00:01:16.067426Z","shell.execute_reply":"2023-12-01T00:01:16.066500Z","shell.execute_reply.started":"2023-12-01T00:01:09.466895Z"},"trusted":true},"outputs":[{"data":{"text/html":["        <script type=\"text/javascript\">\n","        window.PlotlyConfig = {MathJaxConfig: 'local'};\n","        if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n","        if (typeof require !== 'undefined') {\n","        require.undef(\"plotly\");\n","        requirejs.config({\n","            paths: {\n","                'plotly': ['https://cdn.plot.ly/plotly-2.27.0.min']\n","            }\n","        });\n","        require(['plotly'], function(Plotly) {\n","            window._Plotly = Plotly;\n","        });\n","        }\n","        </script>\n","        "]},"metadata":{},"output_type":"display_data"}],"source":["# common libraries\n","import pandas as pd\n","import numpy as np\n","from functools import reduce\n","from numpy import unique\n","\n","# other libraries\n","# from scipy.stats import randint as \n","\n","# viz libraries\n","import plotly.express as px\n","px.defaults.template = \"plotly_white\"\n","import plotly.graph_objects as go\n","from plotly.subplots import make_subplots\n","# import plotly.figure_factory as ff\n","from plotly.offline import init_notebook_mode, iplot, plot\n","init_notebook_mode(connected=True)\n","# import pygwalker as pyg # leveraging once DFs are merged\n","\n","# sklearn\n","from sklearn.metrics import roc_auc_score, f1_score\n","from sklearn.metrics import confusion_matrix, classification_report, precision_recall_curve, accuracy_score, ConfusionMatrixDisplay, auc, roc_curve\n","from sklearn.linear_model import LogisticRegression, RidgeClassifier\n","from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n","from sklearn.model_selection import train_test_split, RandomizedSearchCV, TimeSeriesSplit, cross_val_score\n","from sklearn.utils import shuffle, resample\n","from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.dummy import DummyClassifier\n","from sklearn.preprocessing import LabelEncoder, StandardScaler, MinMaxScaler\n","from sklearn.pipeline import Pipeline\n","from sklearn.model_selection import KFold, StratifiedKFold, RepeatedStratifiedKFold\n","\n","# gradient boosting\n","import lightgbm as lgb\n","import xgboost as xgb"]},{"cell_type":"code","execution_count":66,"metadata":{"execution":{"iopub.execute_input":"2023-12-01T00:01:16.069338Z","iopub.status.busy":"2023-12-01T00:01:16.069046Z","iopub.status.idle":"2023-12-01T00:01:16.166613Z","shell.execute_reply":"2023-12-01T00:01:16.165661Z","shell.execute_reply.started":"2023-12-01T00:01:16.069312Z"},"trusted":true},"outputs":[],"source":["try:\n","    contract_data = pd.read_csv('/kaggle/input/final-provider/contract.csv') # index_col=[0], parse_dates=[0]\n","    personal_data = pd.read_csv('/kaggle/input/final-provider/personal.csv') \n","    internet_data = pd.read_csv('/kaggle/input/final-provider/internet.csv') \n","    phone_data = pd.read_csv('/kaggle/input/final-provider/phone.csv') \n","except:\n","    contract_data = pd.read_csv('/Users/dani/Data Science/TripleTen Projects/Data/Final Project/contract.csv') # index_col=[0], parse_dates=[0]\n","    personal_data = pd.read_csv('/Users/dani/Data Science/TripleTen Projects/Data/Final Project/personal.csv') \n","    internet_data = pd.read_csv('/Users/dani/Data Science/TripleTen Projects/Data/Final Project/internet.csv') \n","    phone_data = pd.read_csv('/Users/dani/Data Science/TripleTen Projects/Data/Final Project/phone.csv') "]},{"cell_type":"markdown","metadata":{},"source":["-----------"]},{"cell_type":"markdown","metadata":{},"source":["# Exploratory Data Analysis"]},{"cell_type":"markdown","metadata":{},"source":["`Encoding Function`"]},{"cell_type":"code","execution_count":67,"metadata":{"execution":{"iopub.execute_input":"2023-12-01T00:01:16.168419Z","iopub.status.busy":"2023-12-01T00:01:16.168112Z","iopub.status.idle":"2023-12-01T00:01:16.172765Z","shell.execute_reply":"2023-12-01T00:01:16.171720Z","shell.execute_reply.started":"2023-12-01T00:01:16.168393Z"},"trusted":true},"outputs":[],"source":["encoder = LabelEncoder() "]},{"cell_type":"code","execution_count":68,"metadata":{"execution":{"iopub.execute_input":"2023-12-01T00:01:16.176302Z","iopub.status.busy":"2023-12-01T00:01:16.175779Z","iopub.status.idle":"2023-12-01T00:01:16.188455Z","shell.execute_reply":"2023-12-01T00:01:16.187339Z","shell.execute_reply.started":"2023-12-01T00:01:16.176263Z"},"trusted":true},"outputs":[],"source":["class MultiColumnLabelEncoder:\n","    def __init__(self,columns = None):\n","        self.columns = columns # column names to encode\n","\n","    def fit(self,X,y=None):\n","        return self \n","\n","    def transform(self,X):\n","        '''\n","        Transforms columns of X specified in self.columns using\n","        LabelEncoder(). If no columns specified, transforms all\n","        columns in X.\n","        '''\n","        output = X.copy()\n","        if self.columns is not None:\n","            for col in self.columns:\n","                output[col] = LabelEncoder().fit_transform(output[col])\n","        else:\n","            for colname,col in output.iteritems():\n","                output[colname] = LabelEncoder().fit_transform(col)\n","        return output\n","\n","    def fit_transform(self,X,y=None):\n","        return self.fit(X,y).transform(X)"]},{"cell_type":"markdown","metadata":{},"source":["-----------"]},{"cell_type":"markdown","metadata":{},"source":["# Contract data"]},{"cell_type":"code","execution_count":69,"metadata":{"execution":{"iopub.execute_input":"2023-12-01T00:01:16.190169Z","iopub.status.busy":"2023-12-01T00:01:16.189761Z","iopub.status.idle":"2023-12-01T00:01:16.228222Z","shell.execute_reply":"2023-12-01T00:01:16.227238Z","shell.execute_reply.started":"2023-12-01T00:01:16.190139Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["<class 'pandas.core.frame.DataFrame'>\n","RangeIndex: 7043 entries, 0 to 7042\n","Data columns (total 8 columns):\n"," #   Column            Non-Null Count  Dtype  \n","---  ------            --------------  -----  \n"," 0   customerID        7043 non-null   object \n"," 1   BeginDate         7043 non-null   object \n"," 2   EndDate           7043 non-null   object \n"," 3   Type              7043 non-null   object \n"," 4   PaperlessBilling  7043 non-null   object \n"," 5   PaymentMethod     7043 non-null   object \n"," 6   MonthlyCharges    7043 non-null   float64\n"," 7   TotalCharges      7043 non-null   object \n","dtypes: float64(1), object(7)\n","memory usage: 440.3+ KB\n"]}],"source":["contract_data.info()"]},{"cell_type":"code","execution_count":70,"metadata":{"execution":{"iopub.execute_input":"2023-12-01T00:01:16.229899Z","iopub.status.busy":"2023-12-01T00:01:16.229534Z","iopub.status.idle":"2023-12-01T00:01:16.249827Z","shell.execute_reply":"2023-12-01T00:01:16.248847Z","shell.execute_reply.started":"2023-12-01T00:01:16.229869Z"},"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>MonthlyCharges</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>count</th>\n","      <td>7043.000000</td>\n","    </tr>\n","    <tr>\n","      <th>mean</th>\n","      <td>64.761692</td>\n","    </tr>\n","    <tr>\n","      <th>std</th>\n","      <td>30.090047</td>\n","    </tr>\n","    <tr>\n","      <th>min</th>\n","      <td>18.250000</td>\n","    </tr>\n","    <tr>\n","      <th>25%</th>\n","      <td>35.500000</td>\n","    </tr>\n","    <tr>\n","      <th>50%</th>\n","      <td>70.350000</td>\n","    </tr>\n","    <tr>\n","      <th>75%</th>\n","      <td>89.850000</td>\n","    </tr>\n","    <tr>\n","      <th>max</th>\n","      <td>118.750000</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["       MonthlyCharges\n","count     7043.000000\n","mean        64.761692\n","std         30.090047\n","min         18.250000\n","25%         35.500000\n","50%         70.350000\n","75%         89.850000\n","max        118.750000"]},"execution_count":70,"metadata":{},"output_type":"execute_result"}],"source":["contract_data.describe()"]},{"cell_type":"code","execution_count":71,"metadata":{"execution":{"iopub.execute_input":"2023-12-01T00:01:16.251631Z","iopub.status.busy":"2023-12-01T00:01:16.251230Z","iopub.status.idle":"2023-12-01T00:01:16.266840Z","shell.execute_reply":"2023-12-01T00:01:16.265843Z","shell.execute_reply.started":"2023-12-01T00:01:16.251595Z"},"trusted":true},"outputs":[{"data":{"text/plain":["customerID          0\n","BeginDate           0\n","EndDate             0\n","Type                0\n","PaperlessBilling    0\n","PaymentMethod       0\n","MonthlyCharges      0\n","TotalCharges        0\n","dtype: int64"]},"execution_count":71,"metadata":{},"output_type":"execute_result"}],"source":["contract_data.isna().sum()"]},{"cell_type":"code","execution_count":72,"metadata":{"execution":{"iopub.execute_input":"2023-12-01T00:01:16.268495Z","iopub.status.busy":"2023-12-01T00:01:16.268149Z","iopub.status.idle":"2023-12-01T00:01:16.288021Z","shell.execute_reply":"2023-12-01T00:01:16.286925Z","shell.execute_reply.started":"2023-12-01T00:01:16.268467Z"},"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>customerID</th>\n","      <th>BeginDate</th>\n","      <th>EndDate</th>\n","      <th>Type</th>\n","      <th>PaperlessBilling</th>\n","      <th>PaymentMethod</th>\n","      <th>MonthlyCharges</th>\n","      <th>TotalCharges</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>7590-VHVEG</td>\n","      <td>2020-01-01</td>\n","      <td>No</td>\n","      <td>Month-to-month</td>\n","      <td>Yes</td>\n","      <td>Electronic check</td>\n","      <td>29.85</td>\n","      <td>29.85</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>5575-GNVDE</td>\n","      <td>2017-04-01</td>\n","      <td>No</td>\n","      <td>One year</td>\n","      <td>No</td>\n","      <td>Mailed check</td>\n","      <td>56.95</td>\n","      <td>1889.5</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>3668-QPYBK</td>\n","      <td>2019-10-01</td>\n","      <td>2019-12-01 00:00:00</td>\n","      <td>Month-to-month</td>\n","      <td>Yes</td>\n","      <td>Mailed check</td>\n","      <td>53.85</td>\n","      <td>108.15</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>7795-CFOCW</td>\n","      <td>2016-05-01</td>\n","      <td>No</td>\n","      <td>One year</td>\n","      <td>No</td>\n","      <td>Bank transfer (automatic)</td>\n","      <td>42.30</td>\n","      <td>1840.75</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>9237-HQITU</td>\n","      <td>2019-09-01</td>\n","      <td>2019-11-01 00:00:00</td>\n","      <td>Month-to-month</td>\n","      <td>Yes</td>\n","      <td>Electronic check</td>\n","      <td>70.70</td>\n","      <td>151.65</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>7038</th>\n","      <td>6840-RESVB</td>\n","      <td>2018-02-01</td>\n","      <td>No</td>\n","      <td>One year</td>\n","      <td>Yes</td>\n","      <td>Mailed check</td>\n","      <td>84.80</td>\n","      <td>1990.5</td>\n","    </tr>\n","    <tr>\n","      <th>7039</th>\n","      <td>2234-XADUH</td>\n","      <td>2014-02-01</td>\n","      <td>No</td>\n","      <td>One year</td>\n","      <td>Yes</td>\n","      <td>Credit card (automatic)</td>\n","      <td>103.20</td>\n","      <td>7362.9</td>\n","    </tr>\n","    <tr>\n","      <th>7040</th>\n","      <td>4801-JZAZL</td>\n","      <td>2019-03-01</td>\n","      <td>No</td>\n","      <td>Month-to-month</td>\n","      <td>Yes</td>\n","      <td>Electronic check</td>\n","      <td>29.60</td>\n","      <td>346.45</td>\n","    </tr>\n","    <tr>\n","      <th>7041</th>\n","      <td>8361-LTMKD</td>\n","      <td>2019-07-01</td>\n","      <td>2019-11-01 00:00:00</td>\n","      <td>Month-to-month</td>\n","      <td>Yes</td>\n","      <td>Mailed check</td>\n","      <td>74.40</td>\n","      <td>306.6</td>\n","    </tr>\n","    <tr>\n","      <th>7042</th>\n","      <td>3186-AJIEK</td>\n","      <td>2014-08-01</td>\n","      <td>No</td>\n","      <td>Two year</td>\n","      <td>Yes</td>\n","      <td>Bank transfer (automatic)</td>\n","      <td>105.65</td>\n","      <td>6844.5</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>7043 rows × 8 columns</p>\n","</div>"],"text/plain":["      customerID   BeginDate              EndDate            Type  \\\n","0     7590-VHVEG  2020-01-01                   No  Month-to-month   \n","1     5575-GNVDE  2017-04-01                   No        One year   \n","2     3668-QPYBK  2019-10-01  2019-12-01 00:00:00  Month-to-month   \n","3     7795-CFOCW  2016-05-01                   No        One year   \n","4     9237-HQITU  2019-09-01  2019-11-01 00:00:00  Month-to-month   \n","...          ...         ...                  ...             ...   \n","7038  6840-RESVB  2018-02-01                   No        One year   \n","7039  2234-XADUH  2014-02-01                   No        One year   \n","7040  4801-JZAZL  2019-03-01                   No  Month-to-month   \n","7041  8361-LTMKD  2019-07-01  2019-11-01 00:00:00  Month-to-month   \n","7042  3186-AJIEK  2014-08-01                   No        Two year   \n","\n","     PaperlessBilling              PaymentMethod  MonthlyCharges TotalCharges  \n","0                 Yes           Electronic check           29.85        29.85  \n","1                  No               Mailed check           56.95       1889.5  \n","2                 Yes               Mailed check           53.85       108.15  \n","3                  No  Bank transfer (automatic)           42.30      1840.75  \n","4                 Yes           Electronic check           70.70       151.65  \n","...               ...                        ...             ...          ...  \n","7038              Yes               Mailed check           84.80       1990.5  \n","7039              Yes    Credit card (automatic)          103.20       7362.9  \n","7040              Yes           Electronic check           29.60       346.45  \n","7041              Yes               Mailed check           74.40        306.6  \n","7042              Yes  Bank transfer (automatic)          105.65       6844.5  \n","\n","[7043 rows x 8 columns]"]},"metadata":{},"output_type":"display_data"}],"source":["display(contract_data)"]},{"cell_type":"markdown","metadata":{},"source":["`Preprocessing`"]},{"cell_type":"code","execution_count":73,"metadata":{"execution":{"iopub.execute_input":"2023-12-01T00:01:16.290000Z","iopub.status.busy":"2023-12-01T00:01:16.289422Z","iopub.status.idle":"2023-12-01T00:01:16.458983Z","shell.execute_reply":"2023-12-01T00:01:16.457771Z","shell.execute_reply.started":"2023-12-01T00:01:16.289959Z"},"trusted":true},"outputs":[{"data":{"text/plain":["[' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ']"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["<class 'pandas.core.frame.DataFrame'>\n","Index: 7032 entries, 0 to 7042\n","Data columns (total 8 columns):\n"," #   Column             Non-Null Count  Dtype         \n","---  ------             --------------  -----         \n"," 0   customer_id        7032 non-null   object        \n"," 1   begin_date         7032 non-null   datetime64[ns]\n"," 2   end_date           7032 non-null   object        \n"," 3   contract_type      7032 non-null   object        \n"," 4   paperless_billing  7032 non-null   object        \n"," 5   payment_method     7032 non-null   object        \n"," 6   monthly_charges    7032 non-null   float64       \n"," 7   total_charges      7032 non-null   float64       \n","dtypes: datetime64[ns](1), float64(2), object(5)\n","memory usage: 494.4+ KB\n"]}],"source":["contract_df = contract_data.copy()\n","# column renaming\n","contract_df = contract_df.rename(columns={\"customerID\": \"customer_id\", \"BeginDate\": \"begin_date\", \"EndDate\": \"end_date\", \"Type\": \"contract_type\",\n","                           \"PaperlessBilling\": \"paperless_billing\", \"PaymentMethod\": \"payment_method\", \"MonthlyCharges\": \"monthly_charges\",\n","                           \"TotalCharges\": \"total_charges\"})\n","\n","# datatype conversions\n","contract_df['begin_date'] = pd.to_datetime(contract_df['begin_date'])\n","\n","\n","# display(contract_df.iloc[488]) # first error callout, turns out there are 11 rows without a 'total_charges' value\n","# display(contract_df['total_charges'].isnull().sum()) \n","problem_cells = [ ]\n","\n","for value in contract_df['total_charges']:\n","    try:\n","        pd.to_numeric(value)\n","    except:\n","        problem_cells.append(value)\n","        \n","display(problem_cells)\n","display()\n","\n","contract_df['total_charges'].replace(\" \", np.nan, inplace=True) # replacing empty strings so we can drop the rows\n","contract_df['total_charges'] = pd.to_numeric(contract_df['total_charges']) # conversion to match 'monthly_charges'\n","contract_df = contract_df.dropna(subset=['total_charges']) # 11 rows should not make that big of a dent given it's a tiny percentage of total\n","\n","contract_df.info()"]},{"cell_type":"code","execution_count":74,"metadata":{"execution":{"iopub.execute_input":"2023-12-01T00:01:16.462269Z","iopub.status.busy":"2023-12-01T00:01:16.461895Z","iopub.status.idle":"2023-12-01T00:01:16.492390Z","shell.execute_reply":"2023-12-01T00:01:16.491408Z","shell.execute_reply.started":"2023-12-01T00:01:16.462237Z"},"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>customer_id</th>\n","      <th>begin_date</th>\n","      <th>contract_type</th>\n","      <th>paperless_billing</th>\n","      <th>payment_method</th>\n","      <th>monthly_charges</th>\n","      <th>total_charges</th>\n","      <th>churn_target</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>7590-VHVEG</td>\n","      <td>2020-01-01</td>\n","      <td>Month-to-month</td>\n","      <td>Yes</td>\n","      <td>Electronic check</td>\n","      <td>29.85</td>\n","      <td>29.85</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>5575-GNVDE</td>\n","      <td>2017-04-01</td>\n","      <td>One year</td>\n","      <td>No</td>\n","      <td>Mailed check</td>\n","      <td>56.95</td>\n","      <td>1889.50</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>3668-QPYBK</td>\n","      <td>2019-10-01</td>\n","      <td>Month-to-month</td>\n","      <td>Yes</td>\n","      <td>Mailed check</td>\n","      <td>53.85</td>\n","      <td>108.15</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>7795-CFOCW</td>\n","      <td>2016-05-01</td>\n","      <td>One year</td>\n","      <td>No</td>\n","      <td>Bank transfer (automatic)</td>\n","      <td>42.30</td>\n","      <td>1840.75</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>9237-HQITU</td>\n","      <td>2019-09-01</td>\n","      <td>Month-to-month</td>\n","      <td>Yes</td>\n","      <td>Electronic check</td>\n","      <td>70.70</td>\n","      <td>151.65</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>7038</th>\n","      <td>6840-RESVB</td>\n","      <td>2018-02-01</td>\n","      <td>One year</td>\n","      <td>Yes</td>\n","      <td>Mailed check</td>\n","      <td>84.80</td>\n","      <td>1990.50</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>7039</th>\n","      <td>2234-XADUH</td>\n","      <td>2014-02-01</td>\n","      <td>One year</td>\n","      <td>Yes</td>\n","      <td>Credit card (automatic)</td>\n","      <td>103.20</td>\n","      <td>7362.90</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>7040</th>\n","      <td>4801-JZAZL</td>\n","      <td>2019-03-01</td>\n","      <td>Month-to-month</td>\n","      <td>Yes</td>\n","      <td>Electronic check</td>\n","      <td>29.60</td>\n","      <td>346.45</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>7041</th>\n","      <td>8361-LTMKD</td>\n","      <td>2019-07-01</td>\n","      <td>Month-to-month</td>\n","      <td>Yes</td>\n","      <td>Mailed check</td>\n","      <td>74.40</td>\n","      <td>306.60</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>7042</th>\n","      <td>3186-AJIEK</td>\n","      <td>2014-08-01</td>\n","      <td>Two year</td>\n","      <td>Yes</td>\n","      <td>Bank transfer (automatic)</td>\n","      <td>105.65</td>\n","      <td>6844.50</td>\n","      <td>1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>7032 rows × 8 columns</p>\n","</div>"],"text/plain":["     customer_id begin_date   contract_type paperless_billing  \\\n","0     7590-VHVEG 2020-01-01  Month-to-month               Yes   \n","1     5575-GNVDE 2017-04-01        One year                No   \n","2     3668-QPYBK 2019-10-01  Month-to-month               Yes   \n","3     7795-CFOCW 2016-05-01        One year                No   \n","4     9237-HQITU 2019-09-01  Month-to-month               Yes   \n","...          ...        ...             ...               ...   \n","7038  6840-RESVB 2018-02-01        One year               Yes   \n","7039  2234-XADUH 2014-02-01        One year               Yes   \n","7040  4801-JZAZL 2019-03-01  Month-to-month               Yes   \n","7041  8361-LTMKD 2019-07-01  Month-to-month               Yes   \n","7042  3186-AJIEK 2014-08-01        Two year               Yes   \n","\n","                 payment_method  monthly_charges  total_charges  churn_target  \n","0              Electronic check            29.85          29.85             1  \n","1                  Mailed check            56.95        1889.50             1  \n","2                  Mailed check            53.85         108.15             0  \n","3     Bank transfer (automatic)            42.30        1840.75             1  \n","4              Electronic check            70.70         151.65             0  \n","...                         ...              ...            ...           ...  \n","7038               Mailed check            84.80        1990.50             1  \n","7039    Credit card (automatic)           103.20        7362.90             1  \n","7040           Electronic check            29.60         346.45             1  \n","7041               Mailed check            74.40         306.60             0  \n","7042  Bank transfer (automatic)           105.65        6844.50             1  \n","\n","[7032 rows x 8 columns]"]},"metadata":{},"output_type":"display_data"}],"source":["# target handling\n","contract_df.query('end_date == \" \"') # no empty cells in target column\n","\n","contract_df['churn_target'] = np.where(contract_df['end_date'] == 'No', 1, 0) # 1 = no churn, 0 = churn\n","contract_df = contract_df.drop(['end_date'], axis=1)\n","\n","display(contract_df)"]},{"cell_type":"code","execution_count":75,"metadata":{"execution":{"iopub.execute_input":"2023-12-01T00:01:54.691356Z","iopub.status.busy":"2023-12-01T00:01:54.690923Z","iopub.status.idle":"2023-12-01T00:01:56.224365Z","shell.execute_reply":"2023-12-01T00:01:56.223272Z","shell.execute_reply.started":"2023-12-01T00:01:54.691321Z"},"trusted":true},"outputs":[{"data":{"application/vnd.plotly.v1+json":{"config":{"plotlyServerURL":"https://plot.ly"},"data":[{"alignmentgroup":"True","hovertemplate":"variable=0<br>payment_method=%{x}<br>value=%{y}<extra></extra>","legendgroup":"0","marker":{"color":"#636efa","pattern":{"shape":""}},"name":"0","offsetgroup":"0","orientation":"v","showlegend":true,"textposition":"auto","texttemplate":"%{y}","type":"bar","x":["Bank transfer (automatic)","Credit card (automatic)","Electronic check","Mailed check"],"xaxis":"x","y":[589,580,1850,893],"yaxis":"y"}],"layout":{"barmode":"relative","legend":{"title":{"text":"variable"},"tracegroupgap":0},"showlegend":false,"template":{"data":{"bar":[{"error_x":{"color":"#2a3f5f"},"error_y":{"color":"#2a3f5f"},"marker":{"line":{"color":"white","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"barpolar":[{"marker":{"line":{"color":"white","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"carpet":[{"aaxis":{"endlinecolor":"#2a3f5f","gridcolor":"#C8D4E3","linecolor":"#C8D4E3","minorgridcolor":"#C8D4E3","startlinecolor":"#2a3f5f"},"baxis":{"endlinecolor":"#2a3f5f","gridcolor":"#C8D4E3","linecolor":"#C8D4E3","minorgridcolor":"#C8D4E3","startlinecolor":"#2a3f5f"},"type":"carpet"}],"choropleth":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"choropleth"}],"contour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"contour"}],"contourcarpet":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"contourcarpet"}],"heatmap":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmap"}],"heatmapgl":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmapgl"}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"histogram2d":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2d"}],"histogram2dcontour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2dcontour"}],"mesh3d":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"mesh3d"}],"parcoords":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"parcoords"}],"pie":[{"automargin":true,"type":"pie"}],"scatter":[{"fillpattern":{"fillmode":"overlay","size":10,"solidity":0.2},"type":"scatter"}],"scatter3d":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter3d"}],"scattercarpet":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattercarpet"}],"scattergeo":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergeo"}],"scattergl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergl"}],"scattermapbox":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattermapbox"}],"scatterpolar":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolar"}],"scatterpolargl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolargl"}],"scatterternary":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterternary"}],"surface":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"surface"}],"table":[{"cells":{"fill":{"color":"#EBF0F8"},"line":{"color":"white"}},"header":{"fill":{"color":"#C8D4E3"},"line":{"color":"white"}},"type":"table"}]},"layout":{"annotationdefaults":{"arrowcolor":"#2a3f5f","arrowhead":0,"arrowwidth":1},"autotypenumbers":"strict","coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]],"sequential":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"sequentialminus":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]]},"colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#2a3f5f"},"geo":{"bgcolor":"white","lakecolor":"white","landcolor":"white","showlakes":true,"showland":true,"subunitcolor":"#C8D4E3"},"hoverlabel":{"align":"left"},"hovermode":"closest","mapbox":{"style":"light"},"paper_bgcolor":"white","plot_bgcolor":"white","polar":{"angularaxis":{"gridcolor":"#EBF0F8","linecolor":"#EBF0F8","ticks":""},"bgcolor":"white","radialaxis":{"gridcolor":"#EBF0F8","linecolor":"#EBF0F8","ticks":""}},"scene":{"xaxis":{"backgroundcolor":"white","gridcolor":"#DFE8F3","gridwidth":2,"linecolor":"#EBF0F8","showbackground":true,"ticks":"","zerolinecolor":"#EBF0F8"},"yaxis":{"backgroundcolor":"white","gridcolor":"#DFE8F3","gridwidth":2,"linecolor":"#EBF0F8","showbackground":true,"ticks":"","zerolinecolor":"#EBF0F8"},"zaxis":{"backgroundcolor":"white","gridcolor":"#DFE8F3","gridwidth":2,"linecolor":"#EBF0F8","showbackground":true,"ticks":"","zerolinecolor":"#EBF0F8"}},"shapedefaults":{"line":{"color":"#2a3f5f"}},"ternary":{"aaxis":{"gridcolor":"#DFE8F3","linecolor":"#A2B1C6","ticks":""},"baxis":{"gridcolor":"#DFE8F3","linecolor":"#A2B1C6","ticks":""},"bgcolor":"white","caxis":{"gridcolor":"#DFE8F3","linecolor":"#A2B1C6","ticks":""}},"title":{"x":0.05},"xaxis":{"automargin":true,"gridcolor":"#EBF0F8","linecolor":"#EBF0F8","ticks":"","title":{"standoff":15},"zerolinecolor":"#EBF0F8","zerolinewidth":2},"yaxis":{"automargin":true,"gridcolor":"#EBF0F8","linecolor":"#EBF0F8","ticks":"","title":{"standoff":15},"zerolinecolor":"#EBF0F8","zerolinewidth":2}}},"title":{"text":"Customer Payment Method"},"xaxis":{"anchor":"y","domain":[0,1],"title":{"text":"payment_method"}},"yaxis":{"anchor":"x","domain":[0,1],"title":{"text":"value"}}}},"text/html":["<div>                            <div id=\"f6d597db-d698-44fd-8767-f61888ddd2b5\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                require([\"plotly\"], function(Plotly) {                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"f6d597db-d698-44fd-8767-f61888ddd2b5\")) {                    Plotly.newPlot(                        \"f6d597db-d698-44fd-8767-f61888ddd2b5\",                        [{\"alignmentgroup\":\"True\",\"hovertemplate\":\"variable=0\\u003cbr\\u003epayment_method=%{x}\\u003cbr\\u003evalue=%{y}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"0\",\"marker\":{\"color\":\"#636efa\",\"pattern\":{\"shape\":\"\"}},\"name\":\"0\",\"offsetgroup\":\"0\",\"orientation\":\"v\",\"showlegend\":true,\"textposition\":\"auto\",\"texttemplate\":\"%{y}\",\"x\":[\"Bank transfer (automatic)\",\"Credit card (automatic)\",\"Electronic check\",\"Mailed check\"],\"xaxis\":\"x\",\"y\":[589,580,1850,893],\"yaxis\":\"y\",\"type\":\"bar\"}],                        {\"template\":{\"data\":{\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"white\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"white\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"#C8D4E3\",\"linecolor\":\"#C8D4E3\",\"minorgridcolor\":\"#C8D4E3\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"#C8D4E3\",\"linecolor\":\"#C8D4E3\",\"minorgridcolor\":\"#C8D4E3\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"choropleth\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"choropleth\"}],\"contourcarpet\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"contourcarpet\"}],\"contour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"contour\"}],\"heatmapgl\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmapgl\"}],\"heatmap\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmap\"}],\"histogram2dcontour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2dcontour\"}],\"histogram2d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2d\"}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"mesh3d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"mesh3d\"}],\"parcoords\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"parcoords\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}],\"scatter3d\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatter3d\"}],\"scattercarpet\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattercarpet\"}],\"scattergeo\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergeo\"}],\"scattergl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergl\"}],\"scattermapbox\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattermapbox\"}],\"scatterpolargl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolargl\"}],\"scatterpolar\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolar\"}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"scatterternary\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterternary\"}],\"surface\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"surface\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}]},\"layout\":{\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"autotypenumbers\":\"strict\",\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]],\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]},\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"geo\":{\"bgcolor\":\"white\",\"lakecolor\":\"white\",\"landcolor\":\"white\",\"showlakes\":true,\"showland\":true,\"subunitcolor\":\"#C8D4E3\"},\"hoverlabel\":{\"align\":\"left\"},\"hovermode\":\"closest\",\"mapbox\":{\"style\":\"light\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"white\",\"polar\":{\"angularaxis\":{\"gridcolor\":\"#EBF0F8\",\"linecolor\":\"#EBF0F8\",\"ticks\":\"\"},\"bgcolor\":\"white\",\"radialaxis\":{\"gridcolor\":\"#EBF0F8\",\"linecolor\":\"#EBF0F8\",\"ticks\":\"\"}},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"white\",\"gridcolor\":\"#DFE8F3\",\"gridwidth\":2,\"linecolor\":\"#EBF0F8\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"#EBF0F8\"},\"yaxis\":{\"backgroundcolor\":\"white\",\"gridcolor\":\"#DFE8F3\",\"gridwidth\":2,\"linecolor\":\"#EBF0F8\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"#EBF0F8\"},\"zaxis\":{\"backgroundcolor\":\"white\",\"gridcolor\":\"#DFE8F3\",\"gridwidth\":2,\"linecolor\":\"#EBF0F8\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"#EBF0F8\"}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"ternary\":{\"aaxis\":{\"gridcolor\":\"#DFE8F3\",\"linecolor\":\"#A2B1C6\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"#DFE8F3\",\"linecolor\":\"#A2B1C6\",\"ticks\":\"\"},\"bgcolor\":\"white\",\"caxis\":{\"gridcolor\":\"#DFE8F3\",\"linecolor\":\"#A2B1C6\",\"ticks\":\"\"}},\"title\":{\"x\":0.05},\"xaxis\":{\"automargin\":true,\"gridcolor\":\"#EBF0F8\",\"linecolor\":\"#EBF0F8\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"#EBF0F8\",\"zerolinewidth\":2},\"yaxis\":{\"automargin\":true,\"gridcolor\":\"#EBF0F8\",\"linecolor\":\"#EBF0F8\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"#EBF0F8\",\"zerolinewidth\":2}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"payment_method\"}},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"value\"}},\"legend\":{\"title\":{\"text\":\"variable\"},\"tracegroupgap\":0},\"title\":{\"text\":\"Customer Payment Method\"},\"barmode\":\"relative\",\"showlegend\":false},                        {\"responsive\": true}                    ).then(function(){\n","                            \n","var gd = document.getElementById('f6d597db-d698-44fd-8767-f61888ddd2b5');\n","var x = new MutationObserver(function (mutations, observer) {{\n","        var display = window.getComputedStyle(gd).display;\n","        if (!display || display === 'none') {{\n","            console.log([gd, 'removed!']);\n","            Plotly.purge(gd);\n","            observer.disconnect();\n","        }}\n","}});\n","\n","// Listen for the removal of the full notebook cells\n","var notebookContainer = gd.closest('#notebook-container');\n","if (notebookContainer) {{\n","    x.observe(notebookContainer, {childList: true});\n","}}\n","\n","// Listen for the clearing of the current output cell\n","var outputEl = gd.closest('.output');\n","if (outputEl) {{\n","    x.observe(outputEl, {childList: true});\n","}}\n","\n","                        })                };                });            </script>        </div>"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.plotly.v1+json":{"config":{"plotlyServerURL":"https://plot.ly"},"data":[{"alignmentgroup":"True","hovertemplate":"variable=0<br>contract_type=%{x}<br>value=%{y}<extra></extra>","legendgroup":"0","marker":{"color":"#636efa","pattern":{"shape":""}},"name":"0","offsetgroup":"0","orientation":"v","showlegend":true,"textposition":"auto","texttemplate":"%{y}","type":"bar","x":["Month-to-month","One year","Two year"],"xaxis":"x","y":[1850,398,580],"yaxis":"y"}],"layout":{"barmode":"relative","legend":{"title":{"text":"variable"},"tracegroupgap":0},"showlegend":false,"template":{"data":{"bar":[{"error_x":{"color":"#2a3f5f"},"error_y":{"color":"#2a3f5f"},"marker":{"line":{"color":"white","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"barpolar":[{"marker":{"line":{"color":"white","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"carpet":[{"aaxis":{"endlinecolor":"#2a3f5f","gridcolor":"#C8D4E3","linecolor":"#C8D4E3","minorgridcolor":"#C8D4E3","startlinecolor":"#2a3f5f"},"baxis":{"endlinecolor":"#2a3f5f","gridcolor":"#C8D4E3","linecolor":"#C8D4E3","minorgridcolor":"#C8D4E3","startlinecolor":"#2a3f5f"},"type":"carpet"}],"choropleth":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"choropleth"}],"contour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"contour"}],"contourcarpet":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"contourcarpet"}],"heatmap":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmap"}],"heatmapgl":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmapgl"}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"histogram2d":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2d"}],"histogram2dcontour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2dcontour"}],"mesh3d":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"mesh3d"}],"parcoords":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"parcoords"}],"pie":[{"automargin":true,"type":"pie"}],"scatter":[{"fillpattern":{"fillmode":"overlay","size":10,"solidity":0.2},"type":"scatter"}],"scatter3d":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter3d"}],"scattercarpet":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattercarpet"}],"scattergeo":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergeo"}],"scattergl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergl"}],"scattermapbox":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattermapbox"}],"scatterpolar":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolar"}],"scatterpolargl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolargl"}],"scatterternary":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterternary"}],"surface":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"surface"}],"table":[{"cells":{"fill":{"color":"#EBF0F8"},"line":{"color":"white"}},"header":{"fill":{"color":"#C8D4E3"},"line":{"color":"white"}},"type":"table"}]},"layout":{"annotationdefaults":{"arrowcolor":"#2a3f5f","arrowhead":0,"arrowwidth":1},"autotypenumbers":"strict","coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]],"sequential":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"sequentialminus":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]]},"colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#2a3f5f"},"geo":{"bgcolor":"white","lakecolor":"white","landcolor":"white","showlakes":true,"showland":true,"subunitcolor":"#C8D4E3"},"hoverlabel":{"align":"left"},"hovermode":"closest","mapbox":{"style":"light"},"paper_bgcolor":"white","plot_bgcolor":"white","polar":{"angularaxis":{"gridcolor":"#EBF0F8","linecolor":"#EBF0F8","ticks":""},"bgcolor":"white","radialaxis":{"gridcolor":"#EBF0F8","linecolor":"#EBF0F8","ticks":""}},"scene":{"xaxis":{"backgroundcolor":"white","gridcolor":"#DFE8F3","gridwidth":2,"linecolor":"#EBF0F8","showbackground":true,"ticks":"","zerolinecolor":"#EBF0F8"},"yaxis":{"backgroundcolor":"white","gridcolor":"#DFE8F3","gridwidth":2,"linecolor":"#EBF0F8","showbackground":true,"ticks":"","zerolinecolor":"#EBF0F8"},"zaxis":{"backgroundcolor":"white","gridcolor":"#DFE8F3","gridwidth":2,"linecolor":"#EBF0F8","showbackground":true,"ticks":"","zerolinecolor":"#EBF0F8"}},"shapedefaults":{"line":{"color":"#2a3f5f"}},"ternary":{"aaxis":{"gridcolor":"#DFE8F3","linecolor":"#A2B1C6","ticks":""},"baxis":{"gridcolor":"#DFE8F3","linecolor":"#A2B1C6","ticks":""},"bgcolor":"white","caxis":{"gridcolor":"#DFE8F3","linecolor":"#A2B1C6","ticks":""}},"title":{"x":0.05},"xaxis":{"automargin":true,"gridcolor":"#EBF0F8","linecolor":"#EBF0F8","ticks":"","title":{"standoff":15},"zerolinecolor":"#EBF0F8","zerolinewidth":2},"yaxis":{"automargin":true,"gridcolor":"#EBF0F8","linecolor":"#EBF0F8","ticks":"","title":{"standoff":15},"zerolinecolor":"#EBF0F8","zerolinewidth":2}}},"title":{"text":"Customer Contract Type"},"xaxis":{"anchor":"y","domain":[0,1],"title":{"text":"contract_type"}},"yaxis":{"anchor":"x","domain":[0,1],"title":{"text":"value"}}}},"text/html":["<div>                            <div id=\"ad335a44-74f4-4fba-83f2-0f2196c7b4b8\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                require([\"plotly\"], function(Plotly) {                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"ad335a44-74f4-4fba-83f2-0f2196c7b4b8\")) {                    Plotly.newPlot(                        \"ad335a44-74f4-4fba-83f2-0f2196c7b4b8\",                        [{\"alignmentgroup\":\"True\",\"hovertemplate\":\"variable=0\\u003cbr\\u003econtract_type=%{x}\\u003cbr\\u003evalue=%{y}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"0\",\"marker\":{\"color\":\"#636efa\",\"pattern\":{\"shape\":\"\"}},\"name\":\"0\",\"offsetgroup\":\"0\",\"orientation\":\"v\",\"showlegend\":true,\"textposition\":\"auto\",\"texttemplate\":\"%{y}\",\"x\":[\"Month-to-month\",\"One year\",\"Two year\"],\"xaxis\":\"x\",\"y\":[1850,398,580],\"yaxis\":\"y\",\"type\":\"bar\"}],                        {\"template\":{\"data\":{\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"white\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"white\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"#C8D4E3\",\"linecolor\":\"#C8D4E3\",\"minorgridcolor\":\"#C8D4E3\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"#C8D4E3\",\"linecolor\":\"#C8D4E3\",\"minorgridcolor\":\"#C8D4E3\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"choropleth\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"choropleth\"}],\"contourcarpet\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"contourcarpet\"}],\"contour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"contour\"}],\"heatmapgl\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmapgl\"}],\"heatmap\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmap\"}],\"histogram2dcontour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2dcontour\"}],\"histogram2d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2d\"}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"mesh3d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"mesh3d\"}],\"parcoords\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"parcoords\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}],\"scatter3d\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatter3d\"}],\"scattercarpet\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattercarpet\"}],\"scattergeo\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergeo\"}],\"scattergl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergl\"}],\"scattermapbox\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattermapbox\"}],\"scatterpolargl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolargl\"}],\"scatterpolar\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolar\"}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"scatterternary\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterternary\"}],\"surface\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"surface\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}]},\"layout\":{\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"autotypenumbers\":\"strict\",\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]],\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]},\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"geo\":{\"bgcolor\":\"white\",\"lakecolor\":\"white\",\"landcolor\":\"white\",\"showlakes\":true,\"showland\":true,\"subunitcolor\":\"#C8D4E3\"},\"hoverlabel\":{\"align\":\"left\"},\"hovermode\":\"closest\",\"mapbox\":{\"style\":\"light\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"white\",\"polar\":{\"angularaxis\":{\"gridcolor\":\"#EBF0F8\",\"linecolor\":\"#EBF0F8\",\"ticks\":\"\"},\"bgcolor\":\"white\",\"radialaxis\":{\"gridcolor\":\"#EBF0F8\",\"linecolor\":\"#EBF0F8\",\"ticks\":\"\"}},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"white\",\"gridcolor\":\"#DFE8F3\",\"gridwidth\":2,\"linecolor\":\"#EBF0F8\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"#EBF0F8\"},\"yaxis\":{\"backgroundcolor\":\"white\",\"gridcolor\":\"#DFE8F3\",\"gridwidth\":2,\"linecolor\":\"#EBF0F8\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"#EBF0F8\"},\"zaxis\":{\"backgroundcolor\":\"white\",\"gridcolor\":\"#DFE8F3\",\"gridwidth\":2,\"linecolor\":\"#EBF0F8\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"#EBF0F8\"}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"ternary\":{\"aaxis\":{\"gridcolor\":\"#DFE8F3\",\"linecolor\":\"#A2B1C6\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"#DFE8F3\",\"linecolor\":\"#A2B1C6\",\"ticks\":\"\"},\"bgcolor\":\"white\",\"caxis\":{\"gridcolor\":\"#DFE8F3\",\"linecolor\":\"#A2B1C6\",\"ticks\":\"\"}},\"title\":{\"x\":0.05},\"xaxis\":{\"automargin\":true,\"gridcolor\":\"#EBF0F8\",\"linecolor\":\"#EBF0F8\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"#EBF0F8\",\"zerolinewidth\":2},\"yaxis\":{\"automargin\":true,\"gridcolor\":\"#EBF0F8\",\"linecolor\":\"#EBF0F8\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"#EBF0F8\",\"zerolinewidth\":2}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"contract_type\"}},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"value\"}},\"legend\":{\"title\":{\"text\":\"variable\"},\"tracegroupgap\":0},\"title\":{\"text\":\"Customer Contract Type\"},\"barmode\":\"relative\",\"showlegend\":false},                        {\"responsive\": true}                    ).then(function(){\n","                            \n","var gd = document.getElementById('ad335a44-74f4-4fba-83f2-0f2196c7b4b8');\n","var x = new MutationObserver(function (mutations, observer) {{\n","        var display = window.getComputedStyle(gd).display;\n","        if (!display || display === 'none') {{\n","            console.log([gd, 'removed!']);\n","            Plotly.purge(gd);\n","            observer.disconnect();\n","        }}\n","}});\n","\n","// Listen for the removal of the full notebook cells\n","var notebookContainer = gd.closest('#notebook-container');\n","if (notebookContainer) {{\n","    x.observe(notebookContainer, {childList: true});\n","}}\n","\n","// Listen for the clearing of the current output cell\n","var outputEl = gd.closest('.output');\n","if (outputEl) {{\n","    x.observe(outputEl, {childList: true});\n","}}\n","\n","                        })                };                });            </script>        </div>"]},"metadata":{},"output_type":"display_data"}],"source":["payment_grp = contract_df.groupby(['contract_type','payment_method']).size().reset_index().groupby('payment_method')[[0]].max()\n","# display(payment_grp)\n","fig = px.bar(payment_grp,title=\"Customer Payment Method\",text_auto = True)\n","fig.update_layout(showlegend=False)\n","fig.show()\n","\n","contract_type_grp = contract_df.groupby(['contract_type','payment_method']).size().reset_index().groupby('contract_type')[[0]].max()\n","# display(contract_type_grp)\n","fig = px.bar(contract_type_grp,title=\"Customer Contract Type\",text_auto = True)\n","fig.update_layout(showlegend=False)\n","fig.show()"]},{"cell_type":"code","execution_count":76,"metadata":{"execution":{"iopub.execute_input":"2023-12-01T00:02:36.916012Z","iopub.status.busy":"2023-12-01T00:02:36.915540Z","iopub.status.idle":"2023-12-01T00:02:36.942607Z","shell.execute_reply":"2023-12-01T00:02:36.941607Z","shell.execute_reply.started":"2023-12-01T00:02:36.915974Z"},"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>customer_id</th>\n","      <th>begin_date</th>\n","      <th>contract_type</th>\n","      <th>paperless_billing</th>\n","      <th>payment_method</th>\n","      <th>monthly_charges</th>\n","      <th>total_charges</th>\n","      <th>churn_target</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>7590-VHVEG</td>\n","      <td>2020-01-01</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>2</td>\n","      <td>29.85</td>\n","      <td>29.85</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>5575-GNVDE</td>\n","      <td>2017-04-01</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>3</td>\n","      <td>56.95</td>\n","      <td>1889.50</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>3668-QPYBK</td>\n","      <td>2019-10-01</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>3</td>\n","      <td>53.85</td>\n","      <td>108.15</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>7795-CFOCW</td>\n","      <td>2016-05-01</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>42.30</td>\n","      <td>1840.75</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>9237-HQITU</td>\n","      <td>2019-09-01</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>2</td>\n","      <td>70.70</td>\n","      <td>151.65</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>7038</th>\n","      <td>6840-RESVB</td>\n","      <td>2018-02-01</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>3</td>\n","      <td>84.80</td>\n","      <td>1990.50</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>7039</th>\n","      <td>2234-XADUH</td>\n","      <td>2014-02-01</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>103.20</td>\n","      <td>7362.90</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>7040</th>\n","      <td>4801-JZAZL</td>\n","      <td>2019-03-01</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>2</td>\n","      <td>29.60</td>\n","      <td>346.45</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>7041</th>\n","      <td>8361-LTMKD</td>\n","      <td>2019-07-01</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>3</td>\n","      <td>74.40</td>\n","      <td>306.60</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>7042</th>\n","      <td>3186-AJIEK</td>\n","      <td>2014-08-01</td>\n","      <td>2</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>105.65</td>\n","      <td>6844.50</td>\n","      <td>1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>7032 rows × 8 columns</p>\n","</div>"],"text/plain":["     customer_id begin_date  contract_type  paperless_billing  payment_method  \\\n","0     7590-VHVEG 2020-01-01              0                  1               2   \n","1     5575-GNVDE 2017-04-01              1                  0               3   \n","2     3668-QPYBK 2019-10-01              0                  1               3   \n","3     7795-CFOCW 2016-05-01              1                  0               0   \n","4     9237-HQITU 2019-09-01              0                  1               2   \n","...          ...        ...            ...                ...             ...   \n","7038  6840-RESVB 2018-02-01              1                  1               3   \n","7039  2234-XADUH 2014-02-01              1                  1               1   \n","7040  4801-JZAZL 2019-03-01              0                  1               2   \n","7041  8361-LTMKD 2019-07-01              0                  1               3   \n","7042  3186-AJIEK 2014-08-01              2                  1               0   \n","\n","      monthly_charges  total_charges  churn_target  \n","0               29.85          29.85             1  \n","1               56.95        1889.50             1  \n","2               53.85         108.15             0  \n","3               42.30        1840.75             1  \n","4               70.70         151.65             0  \n","...               ...            ...           ...  \n","7038            84.80        1990.50             1  \n","7039           103.20        7362.90             1  \n","7040            29.60         346.45             1  \n","7041            74.40         306.60             0  \n","7042           105.65        6844.50             1  \n","\n","[7032 rows x 8 columns]"]},"metadata":{},"output_type":"display_data"}],"source":["# label encoding\n","contract_df = MultiColumnLabelEncoder(columns = ['contract_type','paperless_billing', 'payment_method']).fit_transform(contract_df)\n","\n","display(contract_df)"]},{"cell_type":"markdown","metadata":{},"source":["`Summary`\n","\n","Peaking into our contract data, we see issues with column naming, and column datatypes. We attempt amend those two to start and find that one of the features has empty strings in it's value range (through an iterative examination). \n","\n","Once we find the specific culprits, we replace with NaN values so we can then drop the rows themselves. Thse are small in numbers compared to the entire DF and so are more comfortable with the removal and do not expect much, if any, impact down the line. "]},{"cell_type":"markdown","metadata":{},"source":["-----------"]},{"cell_type":"markdown","metadata":{},"source":["# Personal data"]},{"cell_type":"code","execution_count":77,"metadata":{"execution":{"iopub.execute_input":"2023-12-01T00:02:39.542076Z","iopub.status.busy":"2023-12-01T00:02:39.541696Z","iopub.status.idle":"2023-12-01T00:02:39.554628Z","shell.execute_reply":"2023-12-01T00:02:39.553612Z","shell.execute_reply.started":"2023-12-01T00:02:39.542045Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["<class 'pandas.core.frame.DataFrame'>\n","RangeIndex: 7043 entries, 0 to 7042\n","Data columns (total 5 columns):\n"," #   Column         Non-Null Count  Dtype \n","---  ------         --------------  ----- \n"," 0   customerID     7043 non-null   object\n"," 1   gender         7043 non-null   object\n"," 2   SeniorCitizen  7043 non-null   int64 \n"," 3   Partner        7043 non-null   object\n"," 4   Dependents     7043 non-null   object\n","dtypes: int64(1), object(4)\n","memory usage: 275.2+ KB\n"]}],"source":["personal_data.info()"]},{"cell_type":"code","execution_count":78,"metadata":{"execution":{"iopub.execute_input":"2023-12-01T00:02:41.352220Z","iopub.status.busy":"2023-12-01T00:02:41.351772Z","iopub.status.idle":"2023-12-01T00:02:41.368592Z","shell.execute_reply":"2023-12-01T00:02:41.367394Z","shell.execute_reply.started":"2023-12-01T00:02:41.352187Z"},"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>SeniorCitizen</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>count</th>\n","      <td>7043.000000</td>\n","    </tr>\n","    <tr>\n","      <th>mean</th>\n","      <td>0.162147</td>\n","    </tr>\n","    <tr>\n","      <th>std</th>\n","      <td>0.368612</td>\n","    </tr>\n","    <tr>\n","      <th>min</th>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>25%</th>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>50%</th>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>75%</th>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>max</th>\n","      <td>1.000000</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["       SeniorCitizen\n","count    7043.000000\n","mean        0.162147\n","std         0.368612\n","min         0.000000\n","25%         0.000000\n","50%         0.000000\n","75%         0.000000\n","max         1.000000"]},"execution_count":78,"metadata":{},"output_type":"execute_result"}],"source":["personal_data.describe()"]},{"cell_type":"code","execution_count":79,"metadata":{"execution":{"iopub.execute_input":"2023-12-01T00:02:43.186760Z","iopub.status.busy":"2023-12-01T00:02:43.186382Z","iopub.status.idle":"2023-12-01T00:02:43.198043Z","shell.execute_reply":"2023-12-01T00:02:43.197002Z","shell.execute_reply.started":"2023-12-01T00:02:43.186729Z"},"trusted":true},"outputs":[{"data":{"text/plain":["customerID       0\n","gender           0\n","SeniorCitizen    0\n","Partner          0\n","Dependents       0\n","dtype: int64"]},"execution_count":79,"metadata":{},"output_type":"execute_result"}],"source":["personal_data.isna().sum()"]},{"cell_type":"code","execution_count":80,"metadata":{"execution":{"iopub.execute_input":"2023-12-01T00:02:44.472779Z","iopub.status.busy":"2023-12-01T00:02:44.472365Z","iopub.status.idle":"2023-12-01T00:02:44.491860Z","shell.execute_reply":"2023-12-01T00:02:44.490748Z","shell.execute_reply.started":"2023-12-01T00:02:44.472746Z"},"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>customerID</th>\n","      <th>gender</th>\n","      <th>SeniorCitizen</th>\n","      <th>Partner</th>\n","      <th>Dependents</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>7590-VHVEG</td>\n","      <td>Female</td>\n","      <td>0</td>\n","      <td>Yes</td>\n","      <td>No</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>5575-GNVDE</td>\n","      <td>Male</td>\n","      <td>0</td>\n","      <td>No</td>\n","      <td>No</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>3668-QPYBK</td>\n","      <td>Male</td>\n","      <td>0</td>\n","      <td>No</td>\n","      <td>No</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>7795-CFOCW</td>\n","      <td>Male</td>\n","      <td>0</td>\n","      <td>No</td>\n","      <td>No</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>9237-HQITU</td>\n","      <td>Female</td>\n","      <td>0</td>\n","      <td>No</td>\n","      <td>No</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>7038</th>\n","      <td>6840-RESVB</td>\n","      <td>Male</td>\n","      <td>0</td>\n","      <td>Yes</td>\n","      <td>Yes</td>\n","    </tr>\n","    <tr>\n","      <th>7039</th>\n","      <td>2234-XADUH</td>\n","      <td>Female</td>\n","      <td>0</td>\n","      <td>Yes</td>\n","      <td>Yes</td>\n","    </tr>\n","    <tr>\n","      <th>7040</th>\n","      <td>4801-JZAZL</td>\n","      <td>Female</td>\n","      <td>0</td>\n","      <td>Yes</td>\n","      <td>Yes</td>\n","    </tr>\n","    <tr>\n","      <th>7041</th>\n","      <td>8361-LTMKD</td>\n","      <td>Male</td>\n","      <td>1</td>\n","      <td>Yes</td>\n","      <td>No</td>\n","    </tr>\n","    <tr>\n","      <th>7042</th>\n","      <td>3186-AJIEK</td>\n","      <td>Male</td>\n","      <td>0</td>\n","      <td>No</td>\n","      <td>No</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>7043 rows × 5 columns</p>\n","</div>"],"text/plain":["      customerID  gender  SeniorCitizen Partner Dependents\n","0     7590-VHVEG  Female              0     Yes         No\n","1     5575-GNVDE    Male              0      No         No\n","2     3668-QPYBK    Male              0      No         No\n","3     7795-CFOCW    Male              0      No         No\n","4     9237-HQITU  Female              0      No         No\n","...          ...     ...            ...     ...        ...\n","7038  6840-RESVB    Male              0     Yes        Yes\n","7039  2234-XADUH  Female              0     Yes        Yes\n","7040  4801-JZAZL  Female              0     Yes        Yes\n","7041  8361-LTMKD    Male              1     Yes         No\n","7042  3186-AJIEK    Male              0      No         No\n","\n","[7043 rows x 5 columns]"]},"metadata":{},"output_type":"display_data"}],"source":["display(personal_data)"]},{"cell_type":"markdown","metadata":{},"source":["`Preprocessing`"]},{"cell_type":"code","execution_count":81,"metadata":{"execution":{"iopub.execute_input":"2023-12-01T00:02:46.035248Z","iopub.status.busy":"2023-12-01T00:02:46.034611Z","iopub.status.idle":"2023-12-01T00:02:46.042734Z","shell.execute_reply":"2023-12-01T00:02:46.041244Z","shell.execute_reply.started":"2023-12-01T00:02:46.035215Z"},"trusted":true},"outputs":[],"source":["personal_df = personal_data.copy()\n","# column renaming\n","personal_df = personal_df.rename(columns={\"customerID\": \"customer_id\", \"SeniorCitizen\": \"senior_citizen\",\n","                                          \"Partner\": \"partner\", \"Dependents\": \"dependents\"})"]},{"cell_type":"code","execution_count":82,"metadata":{"execution":{"iopub.execute_input":"2023-12-01T00:05:45.267943Z","iopub.status.busy":"2023-12-01T00:05:45.267552Z","iopub.status.idle":"2023-12-01T00:05:45.330839Z","shell.execute_reply":"2023-12-01T00:05:45.329844Z","shell.execute_reply.started":"2023-12-01T00:05:45.267913Z"},"trusted":true},"outputs":[{"data":{"application/vnd.plotly.v1+json":{"config":{"plotlyServerURL":"https://plot.ly"},"data":[{"domain":{"x":[0,1],"y":[0,1]},"hovertemplate":"label=%{label}<extra></extra>","labels":["Male","Female"],"legendgroup":"","name":"","showlegend":true,"type":"pie"}],"layout":{"legend":{"orientation":"h","tracegroupgap":0,"x":1,"xanchor":"right","y":1.02,"yanchor":"bottom"},"template":{"data":{"bar":[{"error_x":{"color":"#2a3f5f"},"error_y":{"color":"#2a3f5f"},"marker":{"line":{"color":"white","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"barpolar":[{"marker":{"line":{"color":"white","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"carpet":[{"aaxis":{"endlinecolor":"#2a3f5f","gridcolor":"#C8D4E3","linecolor":"#C8D4E3","minorgridcolor":"#C8D4E3","startlinecolor":"#2a3f5f"},"baxis":{"endlinecolor":"#2a3f5f","gridcolor":"#C8D4E3","linecolor":"#C8D4E3","minorgridcolor":"#C8D4E3","startlinecolor":"#2a3f5f"},"type":"carpet"}],"choropleth":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"choropleth"}],"contour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"contour"}],"contourcarpet":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"contourcarpet"}],"heatmap":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmap"}],"heatmapgl":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmapgl"}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"histogram2d":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2d"}],"histogram2dcontour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2dcontour"}],"mesh3d":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"mesh3d"}],"parcoords":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"parcoords"}],"pie":[{"automargin":true,"type":"pie"}],"scatter":[{"fillpattern":{"fillmode":"overlay","size":10,"solidity":0.2},"type":"scatter"}],"scatter3d":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter3d"}],"scattercarpet":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattercarpet"}],"scattergeo":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergeo"}],"scattergl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergl"}],"scattermapbox":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattermapbox"}],"scatterpolar":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolar"}],"scatterpolargl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolargl"}],"scatterternary":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterternary"}],"surface":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"surface"}],"table":[{"cells":{"fill":{"color":"#EBF0F8"},"line":{"color":"white"}},"header":{"fill":{"color":"#C8D4E3"},"line":{"color":"white"}},"type":"table"}]},"layout":{"annotationdefaults":{"arrowcolor":"#2a3f5f","arrowhead":0,"arrowwidth":1},"autotypenumbers":"strict","coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]],"sequential":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"sequentialminus":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]]},"colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#2a3f5f"},"geo":{"bgcolor":"white","lakecolor":"white","landcolor":"white","showlakes":true,"showland":true,"subunitcolor":"#C8D4E3"},"hoverlabel":{"align":"left"},"hovermode":"closest","mapbox":{"style":"light"},"paper_bgcolor":"white","plot_bgcolor":"white","polar":{"angularaxis":{"gridcolor":"#EBF0F8","linecolor":"#EBF0F8","ticks":""},"bgcolor":"white","radialaxis":{"gridcolor":"#EBF0F8","linecolor":"#EBF0F8","ticks":""}},"scene":{"xaxis":{"backgroundcolor":"white","gridcolor":"#DFE8F3","gridwidth":2,"linecolor":"#EBF0F8","showbackground":true,"ticks":"","zerolinecolor":"#EBF0F8"},"yaxis":{"backgroundcolor":"white","gridcolor":"#DFE8F3","gridwidth":2,"linecolor":"#EBF0F8","showbackground":true,"ticks":"","zerolinecolor":"#EBF0F8"},"zaxis":{"backgroundcolor":"white","gridcolor":"#DFE8F3","gridwidth":2,"linecolor":"#EBF0F8","showbackground":true,"ticks":"","zerolinecolor":"#EBF0F8"}},"shapedefaults":{"line":{"color":"#2a3f5f"}},"ternary":{"aaxis":{"gridcolor":"#DFE8F3","linecolor":"#A2B1C6","ticks":""},"baxis":{"gridcolor":"#DFE8F3","linecolor":"#A2B1C6","ticks":""},"bgcolor":"white","caxis":{"gridcolor":"#DFE8F3","linecolor":"#A2B1C6","ticks":""}},"title":{"x":0.05},"xaxis":{"automargin":true,"gridcolor":"#EBF0F8","linecolor":"#EBF0F8","ticks":"","title":{"standoff":15},"zerolinecolor":"#EBF0F8","zerolinewidth":2},"yaxis":{"automargin":true,"gridcolor":"#EBF0F8","linecolor":"#EBF0F8","ticks":"","title":{"standoff":15},"zerolinecolor":"#EBF0F8","zerolinewidth":2}}},"title":{"text":"Gender Mix"}}},"text/html":["<div>                            <div id=\"9b6ef481-a95c-46ce-bef9-cd2b7e4fa1ba\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                require([\"plotly\"], function(Plotly) {                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"9b6ef481-a95c-46ce-bef9-cd2b7e4fa1ba\")) {                    Plotly.newPlot(                        \"9b6ef481-a95c-46ce-bef9-cd2b7e4fa1ba\",                        [{\"domain\":{\"x\":[0.0,1.0],\"y\":[0.0,1.0]},\"hovertemplate\":\"label=%{label}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"labels\":[\"Male\",\"Female\"],\"legendgroup\":\"\",\"name\":\"\",\"showlegend\":true,\"type\":\"pie\"}],                        {\"template\":{\"data\":{\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"white\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"white\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"#C8D4E3\",\"linecolor\":\"#C8D4E3\",\"minorgridcolor\":\"#C8D4E3\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"#C8D4E3\",\"linecolor\":\"#C8D4E3\",\"minorgridcolor\":\"#C8D4E3\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"choropleth\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"choropleth\"}],\"contourcarpet\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"contourcarpet\"}],\"contour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"contour\"}],\"heatmapgl\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmapgl\"}],\"heatmap\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmap\"}],\"histogram2dcontour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2dcontour\"}],\"histogram2d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2d\"}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"mesh3d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"mesh3d\"}],\"parcoords\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"parcoords\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}],\"scatter3d\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatter3d\"}],\"scattercarpet\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattercarpet\"}],\"scattergeo\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergeo\"}],\"scattergl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergl\"}],\"scattermapbox\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattermapbox\"}],\"scatterpolargl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolargl\"}],\"scatterpolar\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolar\"}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"scatterternary\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterternary\"}],\"surface\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"surface\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}]},\"layout\":{\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"autotypenumbers\":\"strict\",\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]],\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]},\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"geo\":{\"bgcolor\":\"white\",\"lakecolor\":\"white\",\"landcolor\":\"white\",\"showlakes\":true,\"showland\":true,\"subunitcolor\":\"#C8D4E3\"},\"hoverlabel\":{\"align\":\"left\"},\"hovermode\":\"closest\",\"mapbox\":{\"style\":\"light\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"white\",\"polar\":{\"angularaxis\":{\"gridcolor\":\"#EBF0F8\",\"linecolor\":\"#EBF0F8\",\"ticks\":\"\"},\"bgcolor\":\"white\",\"radialaxis\":{\"gridcolor\":\"#EBF0F8\",\"linecolor\":\"#EBF0F8\",\"ticks\":\"\"}},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"white\",\"gridcolor\":\"#DFE8F3\",\"gridwidth\":2,\"linecolor\":\"#EBF0F8\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"#EBF0F8\"},\"yaxis\":{\"backgroundcolor\":\"white\",\"gridcolor\":\"#DFE8F3\",\"gridwidth\":2,\"linecolor\":\"#EBF0F8\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"#EBF0F8\"},\"zaxis\":{\"backgroundcolor\":\"white\",\"gridcolor\":\"#DFE8F3\",\"gridwidth\":2,\"linecolor\":\"#EBF0F8\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"#EBF0F8\"}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"ternary\":{\"aaxis\":{\"gridcolor\":\"#DFE8F3\",\"linecolor\":\"#A2B1C6\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"#DFE8F3\",\"linecolor\":\"#A2B1C6\",\"ticks\":\"\"},\"bgcolor\":\"white\",\"caxis\":{\"gridcolor\":\"#DFE8F3\",\"linecolor\":\"#A2B1C6\",\"ticks\":\"\"}},\"title\":{\"x\":0.05},\"xaxis\":{\"automargin\":true,\"gridcolor\":\"#EBF0F8\",\"linecolor\":\"#EBF0F8\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"#EBF0F8\",\"zerolinewidth\":2},\"yaxis\":{\"automargin\":true,\"gridcolor\":\"#EBF0F8\",\"linecolor\":\"#EBF0F8\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"#EBF0F8\",\"zerolinewidth\":2}}},\"legend\":{\"tracegroupgap\":0,\"orientation\":\"h\",\"yanchor\":\"bottom\",\"y\":1.02,\"xanchor\":\"right\",\"x\":1},\"title\":{\"text\":\"Gender Mix\"}},                        {\"responsive\": true}                    ).then(function(){\n","                            \n","var gd = document.getElementById('9b6ef481-a95c-46ce-bef9-cd2b7e4fa1ba');\n","var x = new MutationObserver(function (mutations, observer) {{\n","        var display = window.getComputedStyle(gd).display;\n","        if (!display || display === 'none') {{\n","            console.log([gd, 'removed!']);\n","            Plotly.purge(gd);\n","            observer.disconnect();\n","        }}\n","}});\n","\n","// Listen for the removal of the full notebook cells\n","var notebookContainer = gd.closest('#notebook-container');\n","if (notebookContainer) {{\n","    x.observe(notebookContainer, {childList: true});\n","}}\n","\n","// Listen for the clearing of the current output cell\n","var outputEl = gd.closest('.output');\n","if (outputEl) {{\n","    x.observe(outputEl, {childList: true});\n","}}\n","\n","                        })                };                });            </script>        </div>"]},"metadata":{},"output_type":"display_data"}],"source":["personal_grp = personal_df.groupby(['partner','gender']).size().reset_index().groupby('gender')[[0]].max()\n","\n","labels = ['Male', 'Female']\n","fig = px.pie(personal_grp, names=labels, title='Gender Mix')\n","fig.update_layout(legend=dict(\n","    orientation=\"h\",\n","    yanchor=\"bottom\",\n","    y=1.02,\n","    xanchor=\"right\",\n","    x=1\n","))\n","\n","fig.show()"]},{"cell_type":"code","execution_count":83,"metadata":{"execution":{"iopub.execute_input":"2023-12-01T00:04:37.551263Z","iopub.status.busy":"2023-12-01T00:04:37.550905Z","iopub.status.idle":"2023-12-01T00:04:37.573245Z","shell.execute_reply":"2023-12-01T00:04:37.572312Z","shell.execute_reply.started":"2023-12-01T00:04:37.551237Z"},"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>customer_id</th>\n","      <th>gender</th>\n","      <th>senior_citizen</th>\n","      <th>partner</th>\n","      <th>dependents</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>7590-VHVEG</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>5575-GNVDE</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>3668-QPYBK</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>7795-CFOCW</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>9237-HQITU</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>7038</th>\n","      <td>6840-RESVB</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>7039</th>\n","      <td>2234-XADUH</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>7040</th>\n","      <td>4801-JZAZL</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>7041</th>\n","      <td>8361-LTMKD</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>7042</th>\n","      <td>3186-AJIEK</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>7043 rows × 5 columns</p>\n","</div>"],"text/plain":["     customer_id  gender  senior_citizen  partner  dependents\n","0     7590-VHVEG       0               0        1           0\n","1     5575-GNVDE       1               0        0           0\n","2     3668-QPYBK       1               0        0           0\n","3     7795-CFOCW       1               0        0           0\n","4     9237-HQITU       0               0        0           0\n","...          ...     ...             ...      ...         ...\n","7038  6840-RESVB       1               0        1           1\n","7039  2234-XADUH       0               0        1           1\n","7040  4801-JZAZL       0               0        1           1\n","7041  8361-LTMKD       1               1        1           0\n","7042  3186-AJIEK       1               0        0           0\n","\n","[7043 rows x 5 columns]"]},"metadata":{},"output_type":"display_data"}],"source":["# label encoding\n","personal_df = MultiColumnLabelEncoder(columns = ['gender','senior_citizen', 'partner', 'dependents']).fit_transform(personal_df)\n","\n","display(personal_df)"]},{"cell_type":"markdown","metadata":{},"source":["`Summary`\n","\n","Similar instances of column revisions for personal_data but we preemtively being feature encoding through a function that handles multiple column encoding. These values then become inputs the eventual models can handle and make sense of."]},{"cell_type":"markdown","metadata":{},"source":["-----------"]},{"cell_type":"markdown","metadata":{},"source":["# Internet data"]},{"cell_type":"code","execution_count":84,"metadata":{"execution":{"iopub.execute_input":"2023-12-01T00:05:49.500662Z","iopub.status.busy":"2023-12-01T00:05:49.500284Z","iopub.status.idle":"2023-12-01T00:05:49.516111Z","shell.execute_reply":"2023-12-01T00:05:49.514940Z","shell.execute_reply.started":"2023-12-01T00:05:49.500630Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["<class 'pandas.core.frame.DataFrame'>\n","RangeIndex: 5517 entries, 0 to 5516\n","Data columns (total 8 columns):\n"," #   Column            Non-Null Count  Dtype \n","---  ------            --------------  ----- \n"," 0   customerID        5517 non-null   object\n"," 1   InternetService   5517 non-null   object\n"," 2   OnlineSecurity    5517 non-null   object\n"," 3   OnlineBackup      5517 non-null   object\n"," 4   DeviceProtection  5517 non-null   object\n"," 5   TechSupport       5517 non-null   object\n"," 6   StreamingTV       5517 non-null   object\n"," 7   StreamingMovies   5517 non-null   object\n","dtypes: object(8)\n","memory usage: 344.9+ KB\n"]}],"source":["internet_data.info()"]},{"cell_type":"code","execution_count":85,"metadata":{"execution":{"iopub.execute_input":"2023-12-01T00:05:51.309934Z","iopub.status.busy":"2023-12-01T00:05:51.308826Z","iopub.status.idle":"2023-12-01T00:05:51.348867Z","shell.execute_reply":"2023-12-01T00:05:51.347729Z","shell.execute_reply.started":"2023-12-01T00:05:51.309882Z"},"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>customerID</th>\n","      <th>InternetService</th>\n","      <th>OnlineSecurity</th>\n","      <th>OnlineBackup</th>\n","      <th>DeviceProtection</th>\n","      <th>TechSupport</th>\n","      <th>StreamingTV</th>\n","      <th>StreamingMovies</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>count</th>\n","      <td>5517</td>\n","      <td>5517</td>\n","      <td>5517</td>\n","      <td>5517</td>\n","      <td>5517</td>\n","      <td>5517</td>\n","      <td>5517</td>\n","      <td>5517</td>\n","    </tr>\n","    <tr>\n","      <th>unique</th>\n","      <td>5517</td>\n","      <td>2</td>\n","      <td>2</td>\n","      <td>2</td>\n","      <td>2</td>\n","      <td>2</td>\n","      <td>2</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>top</th>\n","      <td>7590-VHVEG</td>\n","      <td>Fiber optic</td>\n","      <td>No</td>\n","      <td>No</td>\n","      <td>No</td>\n","      <td>No</td>\n","      <td>No</td>\n","      <td>No</td>\n","    </tr>\n","    <tr>\n","      <th>freq</th>\n","      <td>1</td>\n","      <td>3096</td>\n","      <td>3498</td>\n","      <td>3088</td>\n","      <td>3095</td>\n","      <td>3473</td>\n","      <td>2810</td>\n","      <td>2785</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["        customerID InternetService OnlineSecurity OnlineBackup  \\\n","count         5517            5517           5517         5517   \n","unique        5517               2              2            2   \n","top     7590-VHVEG     Fiber optic             No           No   \n","freq             1            3096           3498         3088   \n","\n","       DeviceProtection TechSupport StreamingTV StreamingMovies  \n","count              5517        5517        5517            5517  \n","unique                2           2           2               2  \n","top                  No          No          No              No  \n","freq               3095        3473        2810            2785  "]},"execution_count":85,"metadata":{},"output_type":"execute_result"}],"source":["internet_data.describe()"]},{"cell_type":"code","execution_count":86,"metadata":{"execution":{"iopub.execute_input":"2023-12-01T00:05:52.393204Z","iopub.status.busy":"2023-12-01T00:05:52.392734Z","iopub.status.idle":"2023-12-01T00:05:52.408559Z","shell.execute_reply":"2023-12-01T00:05:52.407412Z","shell.execute_reply.started":"2023-12-01T00:05:52.393168Z"},"trusted":true},"outputs":[{"data":{"text/plain":["customerID          0\n","InternetService     0\n","OnlineSecurity      0\n","OnlineBackup        0\n","DeviceProtection    0\n","TechSupport         0\n","StreamingTV         0\n","StreamingMovies     0\n","dtype: int64"]},"execution_count":86,"metadata":{},"output_type":"execute_result"}],"source":["internet_data.isna().sum()"]},{"cell_type":"code","execution_count":87,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>customerID</th>\n","      <th>InternetService</th>\n","      <th>OnlineSecurity</th>\n","      <th>OnlineBackup</th>\n","      <th>DeviceProtection</th>\n","      <th>TechSupport</th>\n","      <th>StreamingTV</th>\n","      <th>StreamingMovies</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>7590-VHVEG</td>\n","      <td>DSL</td>\n","      <td>No</td>\n","      <td>Yes</td>\n","      <td>No</td>\n","      <td>No</td>\n","      <td>No</td>\n","      <td>No</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>5575-GNVDE</td>\n","      <td>DSL</td>\n","      <td>Yes</td>\n","      <td>No</td>\n","      <td>Yes</td>\n","      <td>No</td>\n","      <td>No</td>\n","      <td>No</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>3668-QPYBK</td>\n","      <td>DSL</td>\n","      <td>Yes</td>\n","      <td>Yes</td>\n","      <td>No</td>\n","      <td>No</td>\n","      <td>No</td>\n","      <td>No</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>7795-CFOCW</td>\n","      <td>DSL</td>\n","      <td>Yes</td>\n","      <td>No</td>\n","      <td>Yes</td>\n","      <td>Yes</td>\n","      <td>No</td>\n","      <td>No</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>9237-HQITU</td>\n","      <td>Fiber optic</td>\n","      <td>No</td>\n","      <td>No</td>\n","      <td>No</td>\n","      <td>No</td>\n","      <td>No</td>\n","      <td>No</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>5512</th>\n","      <td>6840-RESVB</td>\n","      <td>DSL</td>\n","      <td>Yes</td>\n","      <td>No</td>\n","      <td>Yes</td>\n","      <td>Yes</td>\n","      <td>Yes</td>\n","      <td>Yes</td>\n","    </tr>\n","    <tr>\n","      <th>5513</th>\n","      <td>2234-XADUH</td>\n","      <td>Fiber optic</td>\n","      <td>No</td>\n","      <td>Yes</td>\n","      <td>Yes</td>\n","      <td>No</td>\n","      <td>Yes</td>\n","      <td>Yes</td>\n","    </tr>\n","    <tr>\n","      <th>5514</th>\n","      <td>4801-JZAZL</td>\n","      <td>DSL</td>\n","      <td>Yes</td>\n","      <td>No</td>\n","      <td>No</td>\n","      <td>No</td>\n","      <td>No</td>\n","      <td>No</td>\n","    </tr>\n","    <tr>\n","      <th>5515</th>\n","      <td>8361-LTMKD</td>\n","      <td>Fiber optic</td>\n","      <td>No</td>\n","      <td>No</td>\n","      <td>No</td>\n","      <td>No</td>\n","      <td>No</td>\n","      <td>No</td>\n","    </tr>\n","    <tr>\n","      <th>5516</th>\n","      <td>3186-AJIEK</td>\n","      <td>Fiber optic</td>\n","      <td>Yes</td>\n","      <td>No</td>\n","      <td>Yes</td>\n","      <td>Yes</td>\n","      <td>Yes</td>\n","      <td>Yes</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>5517 rows × 8 columns</p>\n","</div>"],"text/plain":["      customerID InternetService OnlineSecurity OnlineBackup DeviceProtection  \\\n","0     7590-VHVEG             DSL             No          Yes               No   \n","1     5575-GNVDE             DSL            Yes           No              Yes   \n","2     3668-QPYBK             DSL            Yes          Yes               No   \n","3     7795-CFOCW             DSL            Yes           No              Yes   \n","4     9237-HQITU     Fiber optic             No           No               No   \n","...          ...             ...            ...          ...              ...   \n","5512  6840-RESVB             DSL            Yes           No              Yes   \n","5513  2234-XADUH     Fiber optic             No          Yes              Yes   \n","5514  4801-JZAZL             DSL            Yes           No               No   \n","5515  8361-LTMKD     Fiber optic             No           No               No   \n","5516  3186-AJIEK     Fiber optic            Yes           No              Yes   \n","\n","     TechSupport StreamingTV StreamingMovies  \n","0             No          No              No  \n","1             No          No              No  \n","2             No          No              No  \n","3            Yes          No              No  \n","4             No          No              No  \n","...          ...         ...             ...  \n","5512         Yes         Yes             Yes  \n","5513          No         Yes             Yes  \n","5514          No          No              No  \n","5515          No          No              No  \n","5516         Yes         Yes             Yes  \n","\n","[5517 rows x 8 columns]"]},"metadata":{},"output_type":"display_data"}],"source":["display(internet_data)"]},{"cell_type":"markdown","metadata":{},"source":["`Preprocessing`"]},{"cell_type":"code","execution_count":88,"metadata":{"execution":{"iopub.execute_input":"2023-12-01T00:07:35.879986Z","iopub.status.busy":"2023-12-01T00:07:35.879598Z","iopub.status.idle":"2023-12-01T00:07:35.899097Z","shell.execute_reply":"2023-12-01T00:07:35.898124Z","shell.execute_reply.started":"2023-12-01T00:07:35.879954Z"},"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>customer_id</th>\n","      <th>internet_service</th>\n","      <th>online_security</th>\n","      <th>online_backup</th>\n","      <th>device_protection</th>\n","      <th>tech_support</th>\n","      <th>streaming_tv</th>\n","      <th>streaming_movies</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>7590-VHVEG</td>\n","      <td>DSL</td>\n","      <td>No</td>\n","      <td>Yes</td>\n","      <td>No</td>\n","      <td>No</td>\n","      <td>No</td>\n","      <td>No</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>5575-GNVDE</td>\n","      <td>DSL</td>\n","      <td>Yes</td>\n","      <td>No</td>\n","      <td>Yes</td>\n","      <td>No</td>\n","      <td>No</td>\n","      <td>No</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>3668-QPYBK</td>\n","      <td>DSL</td>\n","      <td>Yes</td>\n","      <td>Yes</td>\n","      <td>No</td>\n","      <td>No</td>\n","      <td>No</td>\n","      <td>No</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>7795-CFOCW</td>\n","      <td>DSL</td>\n","      <td>Yes</td>\n","      <td>No</td>\n","      <td>Yes</td>\n","      <td>Yes</td>\n","      <td>No</td>\n","      <td>No</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>9237-HQITU</td>\n","      <td>Fiber optic</td>\n","      <td>No</td>\n","      <td>No</td>\n","      <td>No</td>\n","      <td>No</td>\n","      <td>No</td>\n","      <td>No</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>5512</th>\n","      <td>6840-RESVB</td>\n","      <td>DSL</td>\n","      <td>Yes</td>\n","      <td>No</td>\n","      <td>Yes</td>\n","      <td>Yes</td>\n","      <td>Yes</td>\n","      <td>Yes</td>\n","    </tr>\n","    <tr>\n","      <th>5513</th>\n","      <td>2234-XADUH</td>\n","      <td>Fiber optic</td>\n","      <td>No</td>\n","      <td>Yes</td>\n","      <td>Yes</td>\n","      <td>No</td>\n","      <td>Yes</td>\n","      <td>Yes</td>\n","    </tr>\n","    <tr>\n","      <th>5514</th>\n","      <td>4801-JZAZL</td>\n","      <td>DSL</td>\n","      <td>Yes</td>\n","      <td>No</td>\n","      <td>No</td>\n","      <td>No</td>\n","      <td>No</td>\n","      <td>No</td>\n","    </tr>\n","    <tr>\n","      <th>5515</th>\n","      <td>8361-LTMKD</td>\n","      <td>Fiber optic</td>\n","      <td>No</td>\n","      <td>No</td>\n","      <td>No</td>\n","      <td>No</td>\n","      <td>No</td>\n","      <td>No</td>\n","    </tr>\n","    <tr>\n","      <th>5516</th>\n","      <td>3186-AJIEK</td>\n","      <td>Fiber optic</td>\n","      <td>Yes</td>\n","      <td>No</td>\n","      <td>Yes</td>\n","      <td>Yes</td>\n","      <td>Yes</td>\n","      <td>Yes</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>5517 rows × 8 columns</p>\n","</div>"],"text/plain":["     customer_id internet_service online_security online_backup  \\\n","0     7590-VHVEG              DSL              No           Yes   \n","1     5575-GNVDE              DSL             Yes            No   \n","2     3668-QPYBK              DSL             Yes           Yes   \n","3     7795-CFOCW              DSL             Yes            No   \n","4     9237-HQITU      Fiber optic              No            No   \n","...          ...              ...             ...           ...   \n","5512  6840-RESVB              DSL             Yes            No   \n","5513  2234-XADUH      Fiber optic              No           Yes   \n","5514  4801-JZAZL              DSL             Yes            No   \n","5515  8361-LTMKD      Fiber optic              No            No   \n","5516  3186-AJIEK      Fiber optic             Yes            No   \n","\n","     device_protection tech_support streaming_tv streaming_movies  \n","0                   No           No           No               No  \n","1                  Yes           No           No               No  \n","2                   No           No           No               No  \n","3                  Yes          Yes           No               No  \n","4                   No           No           No               No  \n","...                ...          ...          ...              ...  \n","5512               Yes          Yes          Yes              Yes  \n","5513               Yes           No          Yes              Yes  \n","5514                No           No           No               No  \n","5515                No           No           No               No  \n","5516               Yes          Yes          Yes              Yes  \n","\n","[5517 rows x 8 columns]"]},"metadata":{},"output_type":"display_data"}],"source":["internet_df = internet_data.copy()\n","# column renaming\n","internet_df = internet_df.rename(columns={\"customerID\": \"customer_id\", \"InternetService\": \"internet_service\", \"OnlineSecurity\": \"online_security\", \"OnlineBackup\": \"online_backup\",\n","                                         \"DeviceProtection\": \"device_protection\", \"TechSupport\": \"tech_support\", \"StreamingTV\": \"streaming_tv\",\n","                                         \"StreamingMovies\": \"streaming_movies\"})\n","\n","display(internet_df)"]},{"cell_type":"code","execution_count":89,"metadata":{"execution":{"iopub.execute_input":"2023-12-01T00:07:37.945942Z","iopub.status.busy":"2023-12-01T00:07:37.945531Z","iopub.status.idle":"2023-12-01T00:07:38.030203Z","shell.execute_reply":"2023-12-01T00:07:38.029209Z","shell.execute_reply.started":"2023-12-01T00:07:37.945894Z"},"trusted":true},"outputs":[{"data":{"application/vnd.plotly.v1+json":{"config":{"plotlyServerURL":"https://plot.ly"},"data":[{"alignmentgroup":"True","hovertemplate":"variable=0<br>internet_service=%{x}<br>value=%{y}<extra></extra>","legendgroup":"0","marker":{"color":"#636efa","pattern":{"shape":""}},"name":"0","offsetgroup":"0","orientation":"v","showlegend":true,"textposition":"auto","texttemplate":"%{y}","type":"bar","x":["DSL","Fiber optic"],"xaxis":"x","y":[1241,2257],"yaxis":"y"}],"layout":{"barmode":"relative","legend":{"title":{"text":"variable"},"tracegroupgap":0},"showlegend":false,"template":{"data":{"bar":[{"error_x":{"color":"#2a3f5f"},"error_y":{"color":"#2a3f5f"},"marker":{"line":{"color":"white","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"barpolar":[{"marker":{"line":{"color":"white","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"carpet":[{"aaxis":{"endlinecolor":"#2a3f5f","gridcolor":"#C8D4E3","linecolor":"#C8D4E3","minorgridcolor":"#C8D4E3","startlinecolor":"#2a3f5f"},"baxis":{"endlinecolor":"#2a3f5f","gridcolor":"#C8D4E3","linecolor":"#C8D4E3","minorgridcolor":"#C8D4E3","startlinecolor":"#2a3f5f"},"type":"carpet"}],"choropleth":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"choropleth"}],"contour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"contour"}],"contourcarpet":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"contourcarpet"}],"heatmap":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmap"}],"heatmapgl":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmapgl"}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"histogram2d":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2d"}],"histogram2dcontour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2dcontour"}],"mesh3d":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"mesh3d"}],"parcoords":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"parcoords"}],"pie":[{"automargin":true,"type":"pie"}],"scatter":[{"fillpattern":{"fillmode":"overlay","size":10,"solidity":0.2},"type":"scatter"}],"scatter3d":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter3d"}],"scattercarpet":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattercarpet"}],"scattergeo":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergeo"}],"scattergl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergl"}],"scattermapbox":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattermapbox"}],"scatterpolar":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolar"}],"scatterpolargl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolargl"}],"scatterternary":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterternary"}],"surface":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"surface"}],"table":[{"cells":{"fill":{"color":"#EBF0F8"},"line":{"color":"white"}},"header":{"fill":{"color":"#C8D4E3"},"line":{"color":"white"}},"type":"table"}]},"layout":{"annotationdefaults":{"arrowcolor":"#2a3f5f","arrowhead":0,"arrowwidth":1},"autotypenumbers":"strict","coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]],"sequential":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"sequentialminus":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]]},"colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#2a3f5f"},"geo":{"bgcolor":"white","lakecolor":"white","landcolor":"white","showlakes":true,"showland":true,"subunitcolor":"#C8D4E3"},"hoverlabel":{"align":"left"},"hovermode":"closest","mapbox":{"style":"light"},"paper_bgcolor":"white","plot_bgcolor":"white","polar":{"angularaxis":{"gridcolor":"#EBF0F8","linecolor":"#EBF0F8","ticks":""},"bgcolor":"white","radialaxis":{"gridcolor":"#EBF0F8","linecolor":"#EBF0F8","ticks":""}},"scene":{"xaxis":{"backgroundcolor":"white","gridcolor":"#DFE8F3","gridwidth":2,"linecolor":"#EBF0F8","showbackground":true,"ticks":"","zerolinecolor":"#EBF0F8"},"yaxis":{"backgroundcolor":"white","gridcolor":"#DFE8F3","gridwidth":2,"linecolor":"#EBF0F8","showbackground":true,"ticks":"","zerolinecolor":"#EBF0F8"},"zaxis":{"backgroundcolor":"white","gridcolor":"#DFE8F3","gridwidth":2,"linecolor":"#EBF0F8","showbackground":true,"ticks":"","zerolinecolor":"#EBF0F8"}},"shapedefaults":{"line":{"color":"#2a3f5f"}},"ternary":{"aaxis":{"gridcolor":"#DFE8F3","linecolor":"#A2B1C6","ticks":""},"baxis":{"gridcolor":"#DFE8F3","linecolor":"#A2B1C6","ticks":""},"bgcolor":"white","caxis":{"gridcolor":"#DFE8F3","linecolor":"#A2B1C6","ticks":""}},"title":{"x":0.05},"xaxis":{"automargin":true,"gridcolor":"#EBF0F8","linecolor":"#EBF0F8","ticks":"","title":{"standoff":15},"zerolinecolor":"#EBF0F8","zerolinewidth":2},"yaxis":{"automargin":true,"gridcolor":"#EBF0F8","linecolor":"#EBF0F8","ticks":"","title":{"standoff":15},"zerolinecolor":"#EBF0F8","zerolinewidth":2}}},"title":{"text":"Customer Internet Service Type"},"xaxis":{"anchor":"y","domain":[0,1],"title":{"text":"internet_service"}},"yaxis":{"anchor":"x","domain":[0,1],"title":{"text":"value"}}}},"text/html":["<div>                            <div id=\"421f4b97-5c9f-4d84-9db1-019a8eee4e06\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                require([\"plotly\"], function(Plotly) {                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"421f4b97-5c9f-4d84-9db1-019a8eee4e06\")) {                    Plotly.newPlot(                        \"421f4b97-5c9f-4d84-9db1-019a8eee4e06\",                        [{\"alignmentgroup\":\"True\",\"hovertemplate\":\"variable=0\\u003cbr\\u003einternet_service=%{x}\\u003cbr\\u003evalue=%{y}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"0\",\"marker\":{\"color\":\"#636efa\",\"pattern\":{\"shape\":\"\"}},\"name\":\"0\",\"offsetgroup\":\"0\",\"orientation\":\"v\",\"showlegend\":true,\"textposition\":\"auto\",\"texttemplate\":\"%{y}\",\"x\":[\"DSL\",\"Fiber optic\"],\"xaxis\":\"x\",\"y\":[1241,2257],\"yaxis\":\"y\",\"type\":\"bar\"}],                        {\"template\":{\"data\":{\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"white\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"white\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"#C8D4E3\",\"linecolor\":\"#C8D4E3\",\"minorgridcolor\":\"#C8D4E3\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"#C8D4E3\",\"linecolor\":\"#C8D4E3\",\"minorgridcolor\":\"#C8D4E3\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"choropleth\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"choropleth\"}],\"contourcarpet\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"contourcarpet\"}],\"contour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"contour\"}],\"heatmapgl\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmapgl\"}],\"heatmap\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmap\"}],\"histogram2dcontour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2dcontour\"}],\"histogram2d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2d\"}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"mesh3d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"mesh3d\"}],\"parcoords\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"parcoords\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}],\"scatter3d\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatter3d\"}],\"scattercarpet\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattercarpet\"}],\"scattergeo\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergeo\"}],\"scattergl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergl\"}],\"scattermapbox\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattermapbox\"}],\"scatterpolargl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolargl\"}],\"scatterpolar\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolar\"}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"scatterternary\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterternary\"}],\"surface\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"surface\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}]},\"layout\":{\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"autotypenumbers\":\"strict\",\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]],\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]},\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"geo\":{\"bgcolor\":\"white\",\"lakecolor\":\"white\",\"landcolor\":\"white\",\"showlakes\":true,\"showland\":true,\"subunitcolor\":\"#C8D4E3\"},\"hoverlabel\":{\"align\":\"left\"},\"hovermode\":\"closest\",\"mapbox\":{\"style\":\"light\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"white\",\"polar\":{\"angularaxis\":{\"gridcolor\":\"#EBF0F8\",\"linecolor\":\"#EBF0F8\",\"ticks\":\"\"},\"bgcolor\":\"white\",\"radialaxis\":{\"gridcolor\":\"#EBF0F8\",\"linecolor\":\"#EBF0F8\",\"ticks\":\"\"}},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"white\",\"gridcolor\":\"#DFE8F3\",\"gridwidth\":2,\"linecolor\":\"#EBF0F8\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"#EBF0F8\"},\"yaxis\":{\"backgroundcolor\":\"white\",\"gridcolor\":\"#DFE8F3\",\"gridwidth\":2,\"linecolor\":\"#EBF0F8\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"#EBF0F8\"},\"zaxis\":{\"backgroundcolor\":\"white\",\"gridcolor\":\"#DFE8F3\",\"gridwidth\":2,\"linecolor\":\"#EBF0F8\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"#EBF0F8\"}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"ternary\":{\"aaxis\":{\"gridcolor\":\"#DFE8F3\",\"linecolor\":\"#A2B1C6\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"#DFE8F3\",\"linecolor\":\"#A2B1C6\",\"ticks\":\"\"},\"bgcolor\":\"white\",\"caxis\":{\"gridcolor\":\"#DFE8F3\",\"linecolor\":\"#A2B1C6\",\"ticks\":\"\"}},\"title\":{\"x\":0.05},\"xaxis\":{\"automargin\":true,\"gridcolor\":\"#EBF0F8\",\"linecolor\":\"#EBF0F8\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"#EBF0F8\",\"zerolinewidth\":2},\"yaxis\":{\"automargin\":true,\"gridcolor\":\"#EBF0F8\",\"linecolor\":\"#EBF0F8\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"#EBF0F8\",\"zerolinewidth\":2}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"internet_service\"}},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"value\"}},\"legend\":{\"title\":{\"text\":\"variable\"},\"tracegroupgap\":0},\"title\":{\"text\":\"Customer Internet Service Type\"},\"barmode\":\"relative\",\"showlegend\":false},                        {\"responsive\": true}                    ).then(function(){\n","                            \n","var gd = document.getElementById('421f4b97-5c9f-4d84-9db1-019a8eee4e06');\n","var x = new MutationObserver(function (mutations, observer) {{\n","        var display = window.getComputedStyle(gd).display;\n","        if (!display || display === 'none') {{\n","            console.log([gd, 'removed!']);\n","            Plotly.purge(gd);\n","            observer.disconnect();\n","        }}\n","}});\n","\n","// Listen for the removal of the full notebook cells\n","var notebookContainer = gd.closest('#notebook-container');\n","if (notebookContainer) {{\n","    x.observe(notebookContainer, {childList: true});\n","}}\n","\n","// Listen for the clearing of the current output cell\n","var outputEl = gd.closest('.output');\n","if (outputEl) {{\n","    x.observe(outputEl, {childList: true});\n","}}\n","\n","                        })                };                });            </script>        </div>"]},"metadata":{},"output_type":"display_data"}],"source":["internet_grp = internet_df.groupby(['online_security','internet_service']).size().reset_index().groupby('internet_service')[[0]].max()\n","\n","fig = px.bar(internet_grp, title=\"Customer Internet Service Type\",text_auto = True)\n","fig.update_layout(showlegend=False)\n","fig.show()"]},{"cell_type":"code","execution_count":90,"metadata":{"execution":{"iopub.execute_input":"2023-12-01T00:08:00.452624Z","iopub.status.busy":"2023-12-01T00:08:00.452241Z","iopub.status.idle":"2023-12-01T00:08:00.485532Z","shell.execute_reply":"2023-12-01T00:08:00.484493Z","shell.execute_reply.started":"2023-12-01T00:08:00.452593Z"},"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>customer_id</th>\n","      <th>internet_service</th>\n","      <th>online_security</th>\n","      <th>online_backup</th>\n","      <th>device_protection</th>\n","      <th>tech_support</th>\n","      <th>streaming_tv</th>\n","      <th>streaming_movies</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>7590-VHVEG</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>5575-GNVDE</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>3668-QPYBK</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>7795-CFOCW</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>9237-HQITU</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>5512</th>\n","      <td>6840-RESVB</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>5513</th>\n","      <td>2234-XADUH</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>5514</th>\n","      <td>4801-JZAZL</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>5515</th>\n","      <td>8361-LTMKD</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>5516</th>\n","      <td>3186-AJIEK</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>5517 rows × 8 columns</p>\n","</div>"],"text/plain":["     customer_id  internet_service  online_security  online_backup  \\\n","0     7590-VHVEG                 0                0              1   \n","1     5575-GNVDE                 0                1              0   \n","2     3668-QPYBK                 0                1              1   \n","3     7795-CFOCW                 0                1              0   \n","4     9237-HQITU                 1                0              0   \n","...          ...               ...              ...            ...   \n","5512  6840-RESVB                 0                1              0   \n","5513  2234-XADUH                 1                0              1   \n","5514  4801-JZAZL                 0                1              0   \n","5515  8361-LTMKD                 1                0              0   \n","5516  3186-AJIEK                 1                1              0   \n","\n","      device_protection  tech_support  streaming_tv  streaming_movies  \n","0                     0             0             0                 0  \n","1                     1             0             0                 0  \n","2                     0             0             0                 0  \n","3                     1             1             0                 0  \n","4                     0             0             0                 0  \n","...                 ...           ...           ...               ...  \n","5512                  1             1             1                 1  \n","5513                  1             0             1                 1  \n","5514                  0             0             0                 0  \n","5515                  0             0             0                 0  \n","5516                  1             1             1                 1  \n","\n","[5517 rows x 8 columns]"]},"metadata":{},"output_type":"display_data"}],"source":["# label encoding\n","internet_df = MultiColumnLabelEncoder(columns = ['internet_service','online_security', 'online_backup', 'device_protection','tech_support', 'streaming_tv', 'streaming_movies']).fit_transform(internet_df)\n","\n","display(internet_df)"]},{"cell_type":"markdown","metadata":{},"source":["`Summary`\n","\n","Looking into our internet_data, we see a smaller number of rows and therefore have a smaller number of customer_ids (compared to all the other sets). \n","\n","We perform some renaming and label encoding to the columns. After doing so, we are making the assumption that those customer_ids that are not included in this dataset did not sign up for the internet services therefore we will also make the assumption that where we find NaNs in our final, merged set we can replace those with 'No' or 'Not subscribed'."]},{"cell_type":"markdown","metadata":{},"source":["-----------"]},{"cell_type":"markdown","metadata":{},"source":["# Phone data"]},{"cell_type":"code","execution_count":91,"metadata":{"execution":{"iopub.execute_input":"2023-11-30T17:45:14.722976Z","iopub.status.busy":"2023-11-30T17:45:14.722603Z","iopub.status.idle":"2023-11-30T17:45:14.734147Z","shell.execute_reply":"2023-11-30T17:45:14.733169Z","shell.execute_reply.started":"2023-11-30T17:45:14.722944Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["<class 'pandas.core.frame.DataFrame'>\n","RangeIndex: 6361 entries, 0 to 6360\n","Data columns (total 2 columns):\n"," #   Column         Non-Null Count  Dtype \n","---  ------         --------------  ----- \n"," 0   customerID     6361 non-null   object\n"," 1   MultipleLines  6361 non-null   object\n","dtypes: object(2)\n","memory usage: 99.5+ KB\n"]}],"source":["phone_data.info()"]},{"cell_type":"code","execution_count":92,"metadata":{"execution":{"iopub.execute_input":"2023-11-30T17:45:14.736282Z","iopub.status.busy":"2023-11-30T17:45:14.735380Z","iopub.status.idle":"2023-11-30T17:45:14.754870Z","shell.execute_reply":"2023-11-30T17:45:14.754005Z","shell.execute_reply.started":"2023-11-30T17:45:14.736256Z"},"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>customerID</th>\n","      <th>MultipleLines</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>count</th>\n","      <td>6361</td>\n","      <td>6361</td>\n","    </tr>\n","    <tr>\n","      <th>unique</th>\n","      <td>6361</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>top</th>\n","      <td>5575-GNVDE</td>\n","      <td>No</td>\n","    </tr>\n","    <tr>\n","      <th>freq</th>\n","      <td>1</td>\n","      <td>3390</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["        customerID MultipleLines\n","count         6361          6361\n","unique        6361             2\n","top     5575-GNVDE            No\n","freq             1          3390"]},"execution_count":92,"metadata":{},"output_type":"execute_result"}],"source":["phone_data.describe()"]},{"cell_type":"code","execution_count":93,"metadata":{"execution":{"iopub.execute_input":"2023-11-30T17:45:14.756310Z","iopub.status.busy":"2023-11-30T17:45:14.755987Z","iopub.status.idle":"2023-11-30T17:45:14.764593Z","shell.execute_reply":"2023-11-30T17:45:14.763774Z","shell.execute_reply.started":"2023-11-30T17:45:14.756278Z"},"trusted":true},"outputs":[{"data":{"text/plain":["customerID       0\n","MultipleLines    0\n","dtype: int64"]},"execution_count":93,"metadata":{},"output_type":"execute_result"}],"source":["phone_data.isna().sum()"]},{"cell_type":"code","execution_count":94,"metadata":{"execution":{"iopub.execute_input":"2023-11-30T17:45:14.766178Z","iopub.status.busy":"2023-11-30T17:45:14.765825Z","iopub.status.idle":"2023-11-30T17:45:14.778487Z","shell.execute_reply":"2023-11-30T17:45:14.777532Z","shell.execute_reply.started":"2023-11-30T17:45:14.766147Z"},"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>customerID</th>\n","      <th>MultipleLines</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>5575-GNVDE</td>\n","      <td>No</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>3668-QPYBK</td>\n","      <td>No</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>9237-HQITU</td>\n","      <td>No</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>9305-CDSKC</td>\n","      <td>Yes</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>1452-KIOVK</td>\n","      <td>Yes</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>6356</th>\n","      <td>2569-WGERO</td>\n","      <td>No</td>\n","    </tr>\n","    <tr>\n","      <th>6357</th>\n","      <td>6840-RESVB</td>\n","      <td>Yes</td>\n","    </tr>\n","    <tr>\n","      <th>6358</th>\n","      <td>2234-XADUH</td>\n","      <td>Yes</td>\n","    </tr>\n","    <tr>\n","      <th>6359</th>\n","      <td>8361-LTMKD</td>\n","      <td>Yes</td>\n","    </tr>\n","    <tr>\n","      <th>6360</th>\n","      <td>3186-AJIEK</td>\n","      <td>No</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>6361 rows × 2 columns</p>\n","</div>"],"text/plain":["      customerID MultipleLines\n","0     5575-GNVDE            No\n","1     3668-QPYBK            No\n","2     9237-HQITU            No\n","3     9305-CDSKC           Yes\n","4     1452-KIOVK           Yes\n","...          ...           ...\n","6356  2569-WGERO            No\n","6357  6840-RESVB           Yes\n","6358  2234-XADUH           Yes\n","6359  8361-LTMKD           Yes\n","6360  3186-AJIEK            No\n","\n","[6361 rows x 2 columns]"]},"metadata":{},"output_type":"display_data"}],"source":["display(phone_data)"]},{"cell_type":"markdown","metadata":{},"source":["`Preprocessing`"]},{"cell_type":"code","execution_count":95,"metadata":{"execution":{"iopub.execute_input":"2023-11-30T17:45:14.780406Z","iopub.status.busy":"2023-11-30T17:45:14.779800Z","iopub.status.idle":"2023-11-30T17:45:14.793297Z","shell.execute_reply":"2023-11-30T17:45:14.792451Z","shell.execute_reply.started":"2023-11-30T17:45:14.780345Z"},"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>customer_id</th>\n","      <th>multiple_lines</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>5575-GNVDE</td>\n","      <td>No</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>3668-QPYBK</td>\n","      <td>No</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>9237-HQITU</td>\n","      <td>No</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>9305-CDSKC</td>\n","      <td>Yes</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>1452-KIOVK</td>\n","      <td>Yes</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>6356</th>\n","      <td>2569-WGERO</td>\n","      <td>No</td>\n","    </tr>\n","    <tr>\n","      <th>6357</th>\n","      <td>6840-RESVB</td>\n","      <td>Yes</td>\n","    </tr>\n","    <tr>\n","      <th>6358</th>\n","      <td>2234-XADUH</td>\n","      <td>Yes</td>\n","    </tr>\n","    <tr>\n","      <th>6359</th>\n","      <td>8361-LTMKD</td>\n","      <td>Yes</td>\n","    </tr>\n","    <tr>\n","      <th>6360</th>\n","      <td>3186-AJIEK</td>\n","      <td>No</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>6361 rows × 2 columns</p>\n","</div>"],"text/plain":["     customer_id multiple_lines\n","0     5575-GNVDE             No\n","1     3668-QPYBK             No\n","2     9237-HQITU             No\n","3     9305-CDSKC            Yes\n","4     1452-KIOVK            Yes\n","...          ...            ...\n","6356  2569-WGERO             No\n","6357  6840-RESVB            Yes\n","6358  2234-XADUH            Yes\n","6359  8361-LTMKD            Yes\n","6360  3186-AJIEK             No\n","\n","[6361 rows x 2 columns]"]},"metadata":{},"output_type":"display_data"}],"source":["phone_df = phone_data.copy()\n","# column renaming\n","phone_df = phone_df.rename(columns={\"customerID\": \"customer_id\", \"MultipleLines\": \"multiple_lines\"})\n","\n","display(phone_df)"]},{"cell_type":"code","execution_count":96,"metadata":{"execution":{"iopub.execute_input":"2023-11-30T17:45:14.794678Z","iopub.status.busy":"2023-11-30T17:45:14.794395Z","iopub.status.idle":"2023-11-30T17:45:14.811233Z","shell.execute_reply":"2023-11-30T17:45:14.810336Z","shell.execute_reply.started":"2023-11-30T17:45:14.794633Z"},"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>customer_id</th>\n","      <th>multiple_lines</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>5575-GNVDE</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>3668-QPYBK</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>9237-HQITU</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>9305-CDSKC</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>1452-KIOVK</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>6356</th>\n","      <td>2569-WGERO</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>6357</th>\n","      <td>6840-RESVB</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>6358</th>\n","      <td>2234-XADUH</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>6359</th>\n","      <td>8361-LTMKD</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>6360</th>\n","      <td>3186-AJIEK</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>6361 rows × 2 columns</p>\n","</div>"],"text/plain":["     customer_id  multiple_lines\n","0     5575-GNVDE               0\n","1     3668-QPYBK               0\n","2     9237-HQITU               0\n","3     9305-CDSKC               1\n","4     1452-KIOVK               1\n","...          ...             ...\n","6356  2569-WGERO               0\n","6357  6840-RESVB               1\n","6358  2234-XADUH               1\n","6359  8361-LTMKD               1\n","6360  3186-AJIEK               0\n","\n","[6361 rows x 2 columns]"]},"metadata":{},"output_type":"display_data"}],"source":["# label encoding\n","phone_df.multiple_lines = encoder.fit_transform(phone_df.multiple_lines.values)\n","display(phone_df)"]},{"cell_type":"markdown","metadata":{},"source":["`Summary`\n","\n","Our last DF in question, phone_data, has a smaller subset of data and one feature we are insterested in called 'multiple_lines'. We fix the column naming convention to mirror the edits we made to the other DFs and we encode our selected feature using label encoding. "]},{"cell_type":"markdown","metadata":{},"source":["-----------"]},{"cell_type":"markdown","metadata":{},"source":["# Merging"]},{"cell_type":"code","execution_count":97,"metadata":{"execution":{"iopub.execute_input":"2023-11-30T17:45:14.812782Z","iopub.status.busy":"2023-11-30T17:45:14.812442Z","iopub.status.idle":"2023-11-30T17:45:14.867926Z","shell.execute_reply":"2023-11-30T17:45:14.867055Z","shell.execute_reply.started":"2023-11-30T17:45:14.812756Z"},"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>customer_id</th>\n","      <th>begin_date</th>\n","      <th>contract_type</th>\n","      <th>paperless_billing</th>\n","      <th>payment_method</th>\n","      <th>monthly_charges</th>\n","      <th>total_charges</th>\n","      <th>churn_target</th>\n","      <th>gender</th>\n","      <th>senior_citizen</th>\n","      <th>partner</th>\n","      <th>dependents</th>\n","      <th>internet_service</th>\n","      <th>online_security</th>\n","      <th>online_backup</th>\n","      <th>device_protection</th>\n","      <th>tech_support</th>\n","      <th>streaming_tv</th>\n","      <th>streaming_movies</th>\n","      <th>multiple_lines</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>7590-VHVEG</td>\n","      <td>2020-01-01</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>2</td>\n","      <td>29.85</td>\n","      <td>29.85</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>5575-GNVDE</td>\n","      <td>2017-04-01</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>3</td>\n","      <td>56.95</td>\n","      <td>1889.50</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>3668-QPYBK</td>\n","      <td>2019-10-01</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>3</td>\n","      <td>53.85</td>\n","      <td>108.15</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>7795-CFOCW</td>\n","      <td>2016-05-01</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>42.30</td>\n","      <td>1840.75</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>9237-HQITU</td>\n","      <td>2019-09-01</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>2</td>\n","      <td>70.70</td>\n","      <td>151.65</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>7027</th>\n","      <td>6840-RESVB</td>\n","      <td>2018-02-01</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>3</td>\n","      <td>84.80</td>\n","      <td>1990.50</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","    </tr>\n","    <tr>\n","      <th>7028</th>\n","      <td>2234-XADUH</td>\n","      <td>2014-02-01</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>103.20</td>\n","      <td>7362.90</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","    </tr>\n","    <tr>\n","      <th>7029</th>\n","      <td>4801-JZAZL</td>\n","      <td>2019-03-01</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>2</td>\n","      <td>29.60</td>\n","      <td>346.45</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>7030</th>\n","      <td>8361-LTMKD</td>\n","      <td>2019-07-01</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>3</td>\n","      <td>74.40</td>\n","      <td>306.60</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","    </tr>\n","    <tr>\n","      <th>7031</th>\n","      <td>3186-AJIEK</td>\n","      <td>2014-08-01</td>\n","      <td>2</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>105.65</td>\n","      <td>6844.50</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>7032 rows × 20 columns</p>\n","</div>"],"text/plain":["     customer_id begin_date  contract_type  paperless_billing  payment_method  \\\n","0     7590-VHVEG 2020-01-01              0                  1               2   \n","1     5575-GNVDE 2017-04-01              1                  0               3   \n","2     3668-QPYBK 2019-10-01              0                  1               3   \n","3     7795-CFOCW 2016-05-01              1                  0               0   \n","4     9237-HQITU 2019-09-01              0                  1               2   \n","...          ...        ...            ...                ...             ...   \n","7027  6840-RESVB 2018-02-01              1                  1               3   \n","7028  2234-XADUH 2014-02-01              1                  1               1   \n","7029  4801-JZAZL 2019-03-01              0                  1               2   \n","7030  8361-LTMKD 2019-07-01              0                  1               3   \n","7031  3186-AJIEK 2014-08-01              2                  1               0   \n","\n","      monthly_charges  total_charges  churn_target  gender  senior_citizen  \\\n","0               29.85          29.85             1       0               0   \n","1               56.95        1889.50             1       1               0   \n","2               53.85         108.15             0       1               0   \n","3               42.30        1840.75             1       1               0   \n","4               70.70         151.65             0       0               0   \n","...               ...            ...           ...     ...             ...   \n","7027            84.80        1990.50             1       1               0   \n","7028           103.20        7362.90             1       0               0   \n","7029            29.60         346.45             1       0               0   \n","7030            74.40         306.60             0       1               1   \n","7031           105.65        6844.50             1       1               0   \n","\n","      partner  dependents  internet_service  online_security  online_backup  \\\n","0           1           0               0.0              0.0            1.0   \n","1           0           0               0.0              1.0            0.0   \n","2           0           0               0.0              1.0            1.0   \n","3           0           0               0.0              1.0            0.0   \n","4           0           0               1.0              0.0            0.0   \n","...       ...         ...               ...              ...            ...   \n","7027        1           1               0.0              1.0            0.0   \n","7028        1           1               1.0              0.0            1.0   \n","7029        1           1               0.0              1.0            0.0   \n","7030        1           0               1.0              0.0            0.0   \n","7031        0           0               1.0              1.0            0.0   \n","\n","      device_protection  tech_support  streaming_tv  streaming_movies  \\\n","0                   0.0           0.0           0.0               0.0   \n","1                   1.0           0.0           0.0               0.0   \n","2                   0.0           0.0           0.0               0.0   \n","3                   1.0           1.0           0.0               0.0   \n","4                   0.0           0.0           0.0               0.0   \n","...                 ...           ...           ...               ...   \n","7027                1.0           1.0           1.0               1.0   \n","7028                1.0           0.0           1.0               1.0   \n","7029                0.0           0.0           0.0               0.0   \n","7030                0.0           0.0           0.0               0.0   \n","7031                1.0           1.0           1.0               1.0   \n","\n","      multiple_lines  \n","0                NaN  \n","1                0.0  \n","2                0.0  \n","3                NaN  \n","4                0.0  \n","...              ...  \n","7027             1.0  \n","7028             1.0  \n","7029             NaN  \n","7030             1.0  \n","7031             0.0  \n","\n","[7032 rows x 20 columns]"]},"metadata":{},"output_type":"display_data"}],"source":["# merge data based off of customer_id, main DF should be contract_df\n","# contract, personal, phone merge to start\n","# null values from phone_df will need to be monitored with options to 1) remove (if small impact), 2) replace (average,std) or, 3) keep in place\n","\n","# merged_df = contract_df.merge(personal_df,\n","#                              on='customer_id',\n","#                              ).merge(phone_df, on='customer_id', how='left')\n","\n","# display(merged_df)\n","\n","# display(merged_df.query('multiple_lines.isna()')) # attempting to find commonality for NaN values under 'multiple_lines' -- nothing in common \n","\n","data_frames = [contract_df, personal_df, internet_df, phone_df]\n","\n","merged_df = reduce(lambda  left, right: pd.merge(left, right, on=['customer_id'],\n","                                            how='left'), data_frames)\n","\n","display(merged_df) # should include most of our 'customer_id' instances"]},{"cell_type":"code","execution_count":98,"metadata":{"execution":{"iopub.execute_input":"2023-11-30T17:45:14.869214Z","iopub.status.busy":"2023-11-30T17:45:14.868956Z","iopub.status.idle":"2023-11-30T17:45:14.874991Z","shell.execute_reply":"2023-11-30T17:45:14.874103Z","shell.execute_reply.started":"2023-11-30T17:45:14.869192Z"},"trusted":true},"outputs":[],"source":["# dropping columns that don't impact analysis \n","merged_df = merged_df.drop('customer_id', axis=1)"]},{"cell_type":"code","execution_count":99,"metadata":{"execution":{"iopub.execute_input":"2023-11-30T17:45:14.876580Z","iopub.status.busy":"2023-11-30T17:45:14.876261Z","iopub.status.idle":"2023-11-30T17:45:14.911323Z","shell.execute_reply":"2023-11-30T17:45:14.910491Z","shell.execute_reply.started":"2023-11-30T17:45:14.876522Z"},"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>begin_date</th>\n","      <th>contract_type</th>\n","      <th>paperless_billing</th>\n","      <th>payment_method</th>\n","      <th>monthly_charges</th>\n","      <th>total_charges</th>\n","      <th>churn_target</th>\n","      <th>gender</th>\n","      <th>senior_citizen</th>\n","      <th>partner</th>\n","      <th>dependents</th>\n","      <th>internet_service</th>\n","      <th>online_security</th>\n","      <th>online_backup</th>\n","      <th>device_protection</th>\n","      <th>tech_support</th>\n","      <th>streaming_tv</th>\n","      <th>streaming_movies</th>\n","      <th>multiple_lines</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>2020-01-01</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>2</td>\n","      <td>29.85</td>\n","      <td>29.85</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>2017-04-01</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>3</td>\n","      <td>56.95</td>\n","      <td>1889.50</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2019-10-01</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>3</td>\n","      <td>53.85</td>\n","      <td>108.15</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>2016-05-01</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>42.30</td>\n","      <td>1840.75</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>2019-09-01</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>2</td>\n","      <td>70.70</td>\n","      <td>151.65</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>7027</th>\n","      <td>2018-02-01</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>3</td>\n","      <td>84.80</td>\n","      <td>1990.50</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","    </tr>\n","    <tr>\n","      <th>7028</th>\n","      <td>2014-02-01</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>103.20</td>\n","      <td>7362.90</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","    </tr>\n","    <tr>\n","      <th>7029</th>\n","      <td>2019-03-01</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>2</td>\n","      <td>29.60</td>\n","      <td>346.45</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>7030</th>\n","      <td>2019-07-01</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>3</td>\n","      <td>74.40</td>\n","      <td>306.60</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","    </tr>\n","    <tr>\n","      <th>7031</th>\n","      <td>2014-08-01</td>\n","      <td>2</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>105.65</td>\n","      <td>6844.50</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>7032 rows × 19 columns</p>\n","</div>"],"text/plain":["     begin_date  contract_type  paperless_billing  payment_method  \\\n","0    2020-01-01              0                  1               2   \n","1    2017-04-01              1                  0               3   \n","2    2019-10-01              0                  1               3   \n","3    2016-05-01              1                  0               0   \n","4    2019-09-01              0                  1               2   \n","...         ...            ...                ...             ...   \n","7027 2018-02-01              1                  1               3   \n","7028 2014-02-01              1                  1               1   \n","7029 2019-03-01              0                  1               2   \n","7030 2019-07-01              0                  1               3   \n","7031 2014-08-01              2                  1               0   \n","\n","      monthly_charges  total_charges  churn_target  gender  senior_citizen  \\\n","0               29.85          29.85             1       0               0   \n","1               56.95        1889.50             1       1               0   \n","2               53.85         108.15             0       1               0   \n","3               42.30        1840.75             1       1               0   \n","4               70.70         151.65             0       0               0   \n","...               ...            ...           ...     ...             ...   \n","7027            84.80        1990.50             1       1               0   \n","7028           103.20        7362.90             1       0               0   \n","7029            29.60         346.45             1       0               0   \n","7030            74.40         306.60             0       1               1   \n","7031           105.65        6844.50             1       1               0   \n","\n","      partner  dependents  internet_service  online_security  online_backup  \\\n","0           1           0               0.0              0.0            1.0   \n","1           0           0               0.0              1.0            0.0   \n","2           0           0               0.0              1.0            1.0   \n","3           0           0               0.0              1.0            0.0   \n","4           0           0               1.0              0.0            0.0   \n","...       ...         ...               ...              ...            ...   \n","7027        1           1               0.0              1.0            0.0   \n","7028        1           1               1.0              0.0            1.0   \n","7029        1           1               0.0              1.0            0.0   \n","7030        1           0               1.0              0.0            0.0   \n","7031        0           0               1.0              1.0            0.0   \n","\n","      device_protection  tech_support  streaming_tv  streaming_movies  \\\n","0                   0.0           0.0           0.0               0.0   \n","1                   1.0           0.0           0.0               0.0   \n","2                   0.0           0.0           0.0               0.0   \n","3                   1.0           1.0           0.0               0.0   \n","4                   0.0           0.0           0.0               0.0   \n","...                 ...           ...           ...               ...   \n","7027                1.0           1.0           1.0               1.0   \n","7028                1.0           0.0           1.0               1.0   \n","7029                0.0           0.0           0.0               0.0   \n","7030                0.0           0.0           0.0               0.0   \n","7031                1.0           1.0           1.0               1.0   \n","\n","      multiple_lines  \n","0                0.0  \n","1                0.0  \n","2                0.0  \n","3                0.0  \n","4                0.0  \n","...              ...  \n","7027             1.0  \n","7028             1.0  \n","7029             0.0  \n","7030             1.0  \n","7031             0.0  \n","\n","[7032 rows x 19 columns]"]},"metadata":{},"output_type":"display_data"}],"source":["# dropping rows with NaNs under 'multiple_lines' as a test, reverting back to this and filling in if there are score issues with models\n","# lines_filter = merged_df.query('multiple_lines.isna()')\n","# merged_df.drop(merged_df[merged_df['multiple_lines'].isna()].index, inplace=True)\n","\n","# dropping rows with NaNs under 'internet_service' as a test, reverting back to this and filling in if there are score issues with models\n","# service_filter = merged_df.query('internet_service.isna()') #1520 rows with NaNs, removal might be required \n","\n","# we are making the assumption that NaN under multiple_lines == No and NaN under the internet_df means they did not sign up, so == No as well\n","merged_df = merged_df.fillna(0)\n","\n","display(merged_df)"]},{"cell_type":"markdown","metadata":{},"source":["`Summary`\n","\n","We merged all but one of our DFs in our project with our 'contract_data' being the lead DF given the substantial information included. \n","We decide to keep all the matches on the DFs which, knowingly, gives us some NaNs based off of our 'phone_data' -- we keep in our merged DF for now to see how our models react to seeing such values. \n","\n","If issues arise, we will revisit this section and run through a few options for our NaNs. This includes removing the rows or replacing them."]},{"cell_type":"markdown","metadata":{},"source":["-----------"]},{"cell_type":"markdown","metadata":{},"source":["# Target Frequency"]},{"cell_type":"code","execution_count":100,"metadata":{"execution":{"iopub.execute_input":"2023-11-30T17:45:14.912507Z","iopub.status.busy":"2023-11-30T17:45:14.912265Z","iopub.status.idle":"2023-11-30T17:45:14.970784Z","shell.execute_reply":"2023-11-30T17:45:14.969840Z","shell.execute_reply.started":"2023-11-30T17:45:14.912486Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Checking frequency:\n","churn_target\n","1    5163\n","0    1869\n","Name: count, dtype: int64\n"]},{"data":{"application/vnd.plotly.v1+json":{"config":{"plotlyServerURL":"https://plot.ly"},"data":[{"domain":{"x":[0,1],"y":[0,1]},"hovertemplate":"label=%{label}<br>count=%{value}<extra></extra>","labels":["No Churn","Churn"],"legendgroup":"","name":"","showlegend":true,"type":"pie","values":[5163,1869]}],"layout":{"legend":{"orientation":"h","tracegroupgap":0,"x":1,"xanchor":"right","y":1.02,"yanchor":"bottom"},"template":{"data":{"bar":[{"error_x":{"color":"#2a3f5f"},"error_y":{"color":"#2a3f5f"},"marker":{"line":{"color":"white","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"barpolar":[{"marker":{"line":{"color":"white","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"carpet":[{"aaxis":{"endlinecolor":"#2a3f5f","gridcolor":"#C8D4E3","linecolor":"#C8D4E3","minorgridcolor":"#C8D4E3","startlinecolor":"#2a3f5f"},"baxis":{"endlinecolor":"#2a3f5f","gridcolor":"#C8D4E3","linecolor":"#C8D4E3","minorgridcolor":"#C8D4E3","startlinecolor":"#2a3f5f"},"type":"carpet"}],"choropleth":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"choropleth"}],"contour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"contour"}],"contourcarpet":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"contourcarpet"}],"heatmap":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmap"}],"heatmapgl":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmapgl"}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"histogram2d":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2d"}],"histogram2dcontour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2dcontour"}],"mesh3d":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"mesh3d"}],"parcoords":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"parcoords"}],"pie":[{"automargin":true,"type":"pie"}],"scatter":[{"fillpattern":{"fillmode":"overlay","size":10,"solidity":0.2},"type":"scatter"}],"scatter3d":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter3d"}],"scattercarpet":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattercarpet"}],"scattergeo":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergeo"}],"scattergl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergl"}],"scattermapbox":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattermapbox"}],"scatterpolar":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolar"}],"scatterpolargl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolargl"}],"scatterternary":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterternary"}],"surface":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"surface"}],"table":[{"cells":{"fill":{"color":"#EBF0F8"},"line":{"color":"white"}},"header":{"fill":{"color":"#C8D4E3"},"line":{"color":"white"}},"type":"table"}]},"layout":{"annotationdefaults":{"arrowcolor":"#2a3f5f","arrowhead":0,"arrowwidth":1},"autotypenumbers":"strict","coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]],"sequential":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"sequentialminus":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]]},"colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#2a3f5f"},"geo":{"bgcolor":"white","lakecolor":"white","landcolor":"white","showlakes":true,"showland":true,"subunitcolor":"#C8D4E3"},"hoverlabel":{"align":"left"},"hovermode":"closest","mapbox":{"style":"light"},"paper_bgcolor":"white","plot_bgcolor":"white","polar":{"angularaxis":{"gridcolor":"#EBF0F8","linecolor":"#EBF0F8","ticks":""},"bgcolor":"white","radialaxis":{"gridcolor":"#EBF0F8","linecolor":"#EBF0F8","ticks":""}},"scene":{"xaxis":{"backgroundcolor":"white","gridcolor":"#DFE8F3","gridwidth":2,"linecolor":"#EBF0F8","showbackground":true,"ticks":"","zerolinecolor":"#EBF0F8"},"yaxis":{"backgroundcolor":"white","gridcolor":"#DFE8F3","gridwidth":2,"linecolor":"#EBF0F8","showbackground":true,"ticks":"","zerolinecolor":"#EBF0F8"},"zaxis":{"backgroundcolor":"white","gridcolor":"#DFE8F3","gridwidth":2,"linecolor":"#EBF0F8","showbackground":true,"ticks":"","zerolinecolor":"#EBF0F8"}},"shapedefaults":{"line":{"color":"#2a3f5f"}},"ternary":{"aaxis":{"gridcolor":"#DFE8F3","linecolor":"#A2B1C6","ticks":""},"baxis":{"gridcolor":"#DFE8F3","linecolor":"#A2B1C6","ticks":""},"bgcolor":"white","caxis":{"gridcolor":"#DFE8F3","linecolor":"#A2B1C6","ticks":""}},"title":{"x":0.05},"xaxis":{"automargin":true,"gridcolor":"#EBF0F8","linecolor":"#EBF0F8","ticks":"","title":{"standoff":15},"zerolinecolor":"#EBF0F8","zerolinewidth":2},"yaxis":{"automargin":true,"gridcolor":"#EBF0F8","linecolor":"#EBF0F8","ticks":"","title":{"standoff":15},"zerolinecolor":"#EBF0F8","zerolinewidth":2}}},"title":{"text":"Class Frequency"}}},"text/html":["<div>                            <div id=\"aae26258-662d-496a-97fa-e5315d315f07\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                require([\"plotly\"], function(Plotly) {                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"aae26258-662d-496a-97fa-e5315d315f07\")) {                    Plotly.newPlot(                        \"aae26258-662d-496a-97fa-e5315d315f07\",                        [{\"domain\":{\"x\":[0.0,1.0],\"y\":[0.0,1.0]},\"hovertemplate\":\"label=%{label}\\u003cbr\\u003ecount=%{value}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"labels\":[\"No Churn\",\"Churn\"],\"legendgroup\":\"\",\"name\":\"\",\"showlegend\":true,\"values\":[5163,1869],\"type\":\"pie\"}],                        {\"template\":{\"data\":{\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"white\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"white\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"#C8D4E3\",\"linecolor\":\"#C8D4E3\",\"minorgridcolor\":\"#C8D4E3\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"#C8D4E3\",\"linecolor\":\"#C8D4E3\",\"minorgridcolor\":\"#C8D4E3\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"choropleth\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"choropleth\"}],\"contourcarpet\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"contourcarpet\"}],\"contour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"contour\"}],\"heatmapgl\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmapgl\"}],\"heatmap\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmap\"}],\"histogram2dcontour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2dcontour\"}],\"histogram2d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2d\"}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"mesh3d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"mesh3d\"}],\"parcoords\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"parcoords\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}],\"scatter3d\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatter3d\"}],\"scattercarpet\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattercarpet\"}],\"scattergeo\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergeo\"}],\"scattergl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergl\"}],\"scattermapbox\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattermapbox\"}],\"scatterpolargl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolargl\"}],\"scatterpolar\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolar\"}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"scatterternary\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterternary\"}],\"surface\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"surface\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}]},\"layout\":{\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"autotypenumbers\":\"strict\",\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]],\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]},\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"geo\":{\"bgcolor\":\"white\",\"lakecolor\":\"white\",\"landcolor\":\"white\",\"showlakes\":true,\"showland\":true,\"subunitcolor\":\"#C8D4E3\"},\"hoverlabel\":{\"align\":\"left\"},\"hovermode\":\"closest\",\"mapbox\":{\"style\":\"light\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"white\",\"polar\":{\"angularaxis\":{\"gridcolor\":\"#EBF0F8\",\"linecolor\":\"#EBF0F8\",\"ticks\":\"\"},\"bgcolor\":\"white\",\"radialaxis\":{\"gridcolor\":\"#EBF0F8\",\"linecolor\":\"#EBF0F8\",\"ticks\":\"\"}},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"white\",\"gridcolor\":\"#DFE8F3\",\"gridwidth\":2,\"linecolor\":\"#EBF0F8\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"#EBF0F8\"},\"yaxis\":{\"backgroundcolor\":\"white\",\"gridcolor\":\"#DFE8F3\",\"gridwidth\":2,\"linecolor\":\"#EBF0F8\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"#EBF0F8\"},\"zaxis\":{\"backgroundcolor\":\"white\",\"gridcolor\":\"#DFE8F3\",\"gridwidth\":2,\"linecolor\":\"#EBF0F8\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"#EBF0F8\"}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"ternary\":{\"aaxis\":{\"gridcolor\":\"#DFE8F3\",\"linecolor\":\"#A2B1C6\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"#DFE8F3\",\"linecolor\":\"#A2B1C6\",\"ticks\":\"\"},\"bgcolor\":\"white\",\"caxis\":{\"gridcolor\":\"#DFE8F3\",\"linecolor\":\"#A2B1C6\",\"ticks\":\"\"}},\"title\":{\"x\":0.05},\"xaxis\":{\"automargin\":true,\"gridcolor\":\"#EBF0F8\",\"linecolor\":\"#EBF0F8\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"#EBF0F8\",\"zerolinewidth\":2},\"yaxis\":{\"automargin\":true,\"gridcolor\":\"#EBF0F8\",\"linecolor\":\"#EBF0F8\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"#EBF0F8\",\"zerolinewidth\":2}}},\"legend\":{\"tracegroupgap\":0,\"orientation\":\"h\",\"yanchor\":\"bottom\",\"y\":1.02,\"xanchor\":\"right\",\"x\":1},\"title\":{\"text\":\"Class Frequency\"}},                        {\"responsive\": true}                    ).then(function(){\n","                            \n","var gd = document.getElementById('aae26258-662d-496a-97fa-e5315d315f07');\n","var x = new MutationObserver(function (mutations, observer) {{\n","        var display = window.getComputedStyle(gd).display;\n","        if (!display || display === 'none') {{\n","            console.log([gd, 'removed!']);\n","            Plotly.purge(gd);\n","            observer.disconnect();\n","        }}\n","}});\n","\n","// Listen for the removal of the full notebook cells\n","var notebookContainer = gd.closest('#notebook-container');\n","if (notebookContainer) {{\n","    x.observe(notebookContainer, {childList: true});\n","}}\n","\n","// Listen for the clearing of the current output cell\n","var outputEl = gd.closest('.output');\n","if (outputEl) {{\n","    x.observe(outputEl, {childList: true});\n","}}\n","\n","                        })                };                });            </script>        </div>"]},"metadata":{},"output_type":"display_data"}],"source":["class_frequency = merged_df['churn_target'].value_counts()\n","print('Checking frequency:')\n","print(class_frequency)\n","\n","#class_frequency.plot.pie(autopct='%.2f',textprops={'fontsize':12}, colors=['cyan', 'teal']) # calls for upsampling 'rare' or churn target values (26.58%)\n","\n","labels = ['No Churn', 'Churn']\n","fig = px.pie(class_frequency, values='count', names=labels, title='Class Frequency')\n","fig.update_layout(legend=dict(\n","    orientation=\"h\",\n","    yanchor=\"bottom\",\n","    y=1.02,\n","    xanchor=\"right\",\n","    x=1\n","))\n","\n","fig.show()"]},{"cell_type":"markdown","metadata":{},"source":["-----------"]},{"cell_type":"markdown","metadata":{},"source":["# Model Preparation"]},{"cell_type":"markdown","metadata":{},"source":["`Fixed Parameter`"]},{"cell_type":"code","execution_count":101,"metadata":{"execution":{"iopub.execute_input":"2023-11-30T17:45:14.972240Z","iopub.status.busy":"2023-11-30T17:45:14.971954Z","iopub.status.idle":"2023-11-30T17:45:14.976422Z","shell.execute_reply":"2023-11-30T17:45:14.975545Z","shell.execute_reply.started":"2023-11-30T17:45:14.972215Z"},"trusted":true},"outputs":[],"source":["# random_state parameter for all models\n","random_state = 12345"]},{"cell_type":"markdown","metadata":{},"source":["`CV`"]},{"cell_type":"markdown","metadata":{},"source":["Rearranging the data so as to ensure that each fold is a good representative of the whole"]},{"cell_type":"code","execution_count":102,"metadata":{"execution":{"iopub.execute_input":"2023-11-30T17:45:14.977975Z","iopub.status.busy":"2023-11-30T17:45:14.977635Z","iopub.status.idle":"2023-11-30T17:45:14.985544Z","shell.execute_reply":"2023-11-30T17:45:14.984645Z","shell.execute_reply.started":"2023-11-30T17:45:14.977949Z"},"trusted":true},"outputs":[],"source":["cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=5, random_state=random_state)"]},{"cell_type":"markdown","metadata":{},"source":["`Feature Engineering`"]},{"cell_type":"code","execution_count":103,"metadata":{"execution":{"iopub.execute_input":"2023-11-30T17:45:14.986987Z","iopub.status.busy":"2023-11-30T17:45:14.986721Z","iopub.status.idle":"2023-11-30T17:45:15.026035Z","shell.execute_reply":"2023-11-30T17:45:15.025164Z","shell.execute_reply.started":"2023-11-30T17:45:14.986966Z"},"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>contract_type</th>\n","      <th>paperless_billing</th>\n","      <th>payment_method</th>\n","      <th>monthly_charges</th>\n","      <th>total_charges</th>\n","      <th>churn_target</th>\n","      <th>gender</th>\n","      <th>senior_citizen</th>\n","      <th>partner</th>\n","      <th>dependents</th>\n","      <th>...</th>\n","      <th>online_security</th>\n","      <th>online_backup</th>\n","      <th>device_protection</th>\n","      <th>tech_support</th>\n","      <th>streaming_tv</th>\n","      <th>streaming_movies</th>\n","      <th>multiple_lines</th>\n","      <th>begin_year</th>\n","      <th>begin_month</th>\n","      <th>begin_dayofweek</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>2</td>\n","      <td>29.85</td>\n","      <td>29.85</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>2020</td>\n","      <td>1</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>3</td>\n","      <td>56.95</td>\n","      <td>1889.50</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>2017</td>\n","      <td>4</td>\n","      <td>5</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>3</td>\n","      <td>53.85</td>\n","      <td>108.15</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>2019</td>\n","      <td>10</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>42.30</td>\n","      <td>1840.75</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>2016</td>\n","      <td>5</td>\n","      <td>6</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>2</td>\n","      <td>70.70</td>\n","      <td>151.65</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>2019</td>\n","      <td>9</td>\n","      <td>6</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>7027</th>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>3</td>\n","      <td>84.80</td>\n","      <td>1990.50</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>...</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>2018</td>\n","      <td>2</td>\n","      <td>3</td>\n","    </tr>\n","    <tr>\n","      <th>7028</th>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>103.20</td>\n","      <td>7362.90</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>2014</td>\n","      <td>2</td>\n","      <td>5</td>\n","    </tr>\n","    <tr>\n","      <th>7029</th>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>2</td>\n","      <td>29.60</td>\n","      <td>346.45</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>...</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>2019</td>\n","      <td>3</td>\n","      <td>4</td>\n","    </tr>\n","    <tr>\n","      <th>7030</th>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>3</td>\n","      <td>74.40</td>\n","      <td>306.60</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>2019</td>\n","      <td>7</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>7031</th>\n","      <td>2</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>105.65</td>\n","      <td>6844.50</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>2014</td>\n","      <td>8</td>\n","      <td>4</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>7032 rows × 21 columns</p>\n","</div>"],"text/plain":["      contract_type  paperless_billing  payment_method  monthly_charges  \\\n","0                 0                  1               2            29.85   \n","1                 1                  0               3            56.95   \n","2                 0                  1               3            53.85   \n","3                 1                  0               0            42.30   \n","4                 0                  1               2            70.70   \n","...             ...                ...             ...              ...   \n","7027              1                  1               3            84.80   \n","7028              1                  1               1           103.20   \n","7029              0                  1               2            29.60   \n","7030              0                  1               3            74.40   \n","7031              2                  1               0           105.65   \n","\n","      total_charges  churn_target  gender  senior_citizen  partner  \\\n","0             29.85             1       0               0        1   \n","1           1889.50             1       1               0        0   \n","2            108.15             0       1               0        0   \n","3           1840.75             1       1               0        0   \n","4            151.65             0       0               0        0   \n","...             ...           ...     ...             ...      ...   \n","7027        1990.50             1       1               0        1   \n","7028        7362.90             1       0               0        1   \n","7029         346.45             1       0               0        1   \n","7030         306.60             0       1               1        1   \n","7031        6844.50             1       1               0        0   \n","\n","      dependents  ...  online_security  online_backup  device_protection  \\\n","0              0  ...              0.0            1.0                0.0   \n","1              0  ...              1.0            0.0                1.0   \n","2              0  ...              1.0            1.0                0.0   \n","3              0  ...              1.0            0.0                1.0   \n","4              0  ...              0.0            0.0                0.0   \n","...          ...  ...              ...            ...                ...   \n","7027           1  ...              1.0            0.0                1.0   \n","7028           1  ...              0.0            1.0                1.0   \n","7029           1  ...              1.0            0.0                0.0   \n","7030           0  ...              0.0            0.0                0.0   \n","7031           0  ...              1.0            0.0                1.0   \n","\n","      tech_support  streaming_tv  streaming_movies  multiple_lines  \\\n","0              0.0           0.0               0.0             0.0   \n","1              0.0           0.0               0.0             0.0   \n","2              0.0           0.0               0.0             0.0   \n","3              1.0           0.0               0.0             0.0   \n","4              0.0           0.0               0.0             0.0   \n","...            ...           ...               ...             ...   \n","7027           1.0           1.0               1.0             1.0   \n","7028           0.0           1.0               1.0             1.0   \n","7029           0.0           0.0               0.0             0.0   \n","7030           0.0           0.0               0.0             1.0   \n","7031           1.0           1.0               1.0             0.0   \n","\n","      begin_year  begin_month  begin_dayofweek  \n","0           2020            1                2  \n","1           2017            4                5  \n","2           2019           10                1  \n","3           2016            5                6  \n","4           2019            9                6  \n","...          ...          ...              ...  \n","7027        2018            2                3  \n","7028        2014            2                5  \n","7029        2019            3                4  \n","7030        2019            7                0  \n","7031        2014            8                4  \n","\n","[7032 rows x 21 columns]"]},"metadata":{},"output_type":"display_data"}],"source":["# extracting date features from datetime column\n","def make_features(data, col):\n","    data['begin_year'] = data[col].dt.year\n","    data['begin_month'] = data[col].dt.month\n","    # data['begin_day'] = data[col].dt.day # no impact to model outputs\n","    data['begin_dayofweek'] = data[col].dt.dayofweek\n","    \n","#     for lag in range(1, max_lag + 1):\n","#         data['lag_{}'.format(lag)] = data['PJME_MW'].shift(lag)\n","\n","#     data['rolling_mean'] = data['PJME_MW'].shift().rolling(rolling_mean_size).mean()\n","#     #data['rolling_mean'] = data['PJME_MW'].rolling(rolling_mean_size).mean()\n","\n","make_features(merged_df, 'begin_date')\n","merged_df = merged_df.drop('begin_date', axis = 1) # removing as this has been replaced by the newly created features\n","\n","display(merged_df)"]},{"cell_type":"markdown","metadata":{},"source":["`Features and Target`"]},{"cell_type":"code","execution_count":104,"metadata":{"execution":{"iopub.execute_input":"2023-11-30T17:45:15.027635Z","iopub.status.busy":"2023-11-30T17:45:15.027289Z","iopub.status.idle":"2023-11-30T17:45:15.033636Z","shell.execute_reply":"2023-11-30T17:45:15.032692Z","shell.execute_reply.started":"2023-11-30T17:45:15.027603Z"},"trusted":true},"outputs":[],"source":["features = merged_df.drop('churn_target', axis=1)\n","target = merged_df['churn_target']"]},{"cell_type":"markdown","metadata":{},"source":["`Data Splitting`"]},{"cell_type":"code","execution_count":105,"metadata":{"execution":{"iopub.execute_input":"2023-11-30T17:45:15.035612Z","iopub.status.busy":"2023-11-30T17:45:15.035009Z","iopub.status.idle":"2023-11-30T17:45:15.053330Z","shell.execute_reply":"2023-11-30T17:45:15.052375Z","shell.execute_reply.started":"2023-11-30T17:45:15.035580Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["(4218, 20)\n","(4218,)\n","(1407, 20)\n","(1407, 20)\n"]}],"source":["# splitting the data into a training, validation and test dataset\n","features_train, features_test, target_train, target_test = train_test_split(features, target, \n","                                                                            test_size=0.2, random_state=12345, stratify=target)\n","\n","features_train, features_valid, target_train, target_valid = train_test_split(features_train, target_train, \n","                                                                          test_size=0.25, random_state=12345, stratify=target_train)\n","\n","print(features_train.shape)\n","print(target_train.shape)\n","print(features_valid.shape)\n","print(features_test.shape)"]},{"cell_type":"code","execution_count":106,"metadata":{"execution":{"iopub.execute_input":"2023-11-30T17:45:15.054620Z","iopub.status.busy":"2023-11-30T17:45:15.054353Z","iopub.status.idle":"2023-11-30T17:45:15.061776Z","shell.execute_reply":"2023-11-30T17:45:15.060865Z","shell.execute_reply.started":"2023-11-30T17:45:15.054597Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["> Class = 0 : 1121/4218 (26.6%)\n","> Class = 1 : 3097/4218 (73.4%)\n"]}],"source":["# target_train summary\n","classes = unique(target_train)\n","total = len(target_train)\n","for c in classes:\n","    n_examples = len(target_train[target_train==c])\n","    percent = n_examples / total * 100\n","    print('> Class = %d : %d/%d (%.1f%%)' % (c, n_examples, total, percent))"]},{"cell_type":"code","execution_count":107,"metadata":{"execution":{"iopub.execute_input":"2023-11-30T17:45:15.063304Z","iopub.status.busy":"2023-11-30T17:45:15.063022Z","iopub.status.idle":"2023-11-30T17:45:15.072464Z","shell.execute_reply":"2023-11-30T17:45:15.071457Z","shell.execute_reply.started":"2023-11-30T17:45:15.063279Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Train: 0=1121, 1=3097, \n","Valid: 0=374, 1=1033, \n","Test: 0=374, 1=1033\n"]}],"source":["# summarize by set\n","train_0, train_1 = len(target_train[target_train==0]), len(target_train[target_train==1])\n","valid_0, valid_1 = len(target_valid[target_valid==0]), len(target_valid[target_valid==1])\n","test_0, test_1 = len(target_test[target_test==0]), len(target_test[target_test==1])\n","print('Train: 0=%d, 1=%d, \\nValid: 0=%d, 1=%d, \\nTest: 0=%d, 1=%d' % (train_0, train_1, valid_0, valid_1 , test_0, test_1))"]},{"cell_type":"markdown","metadata":{},"source":["`Scaling`"]},{"cell_type":"code","execution_count":108,"metadata":{"execution":{"iopub.execute_input":"2023-11-30T17:45:15.073894Z","iopub.status.busy":"2023-11-30T17:45:15.073571Z","iopub.status.idle":"2023-11-30T17:45:15.128923Z","shell.execute_reply":"2023-11-30T17:45:15.127933Z","shell.execute_reply.started":"2023-11-30T17:45:15.073863Z"},"trusted":true},"outputs":[{"data":{"text/plain":["(      contract_type  paperless_billing  payment_method  monthly_charges  \\\n"," 95                0                  1               2         0.604582   \n"," 4644              1                  1               1         0.013944   \n"," 5562              1                  1               2         0.613048   \n"," 320               1                  0               0         0.639442   \n"," 303               2                  0               1         0.418825   \n"," ...             ...                ...             ...              ...   \n"," 5421              0                  1               1         0.536853   \n"," 5945              0                  1               1         0.366036   \n"," 945               0                  1               2         0.804781   \n"," 6246              1                  1               3         0.919323   \n"," 6745              0                  1               0         0.712151   \n"," \n","       total_charges  gender  senior_citizen  partner  dependents  \\\n"," 95         0.104836       0               0        0           0   \n"," 4644       0.052995       0               0        0           0   \n"," 5562       0.564964       1               0        1           0   \n"," 320        0.499801       0               1        0           0   \n"," 303        0.471979       0               0        1           1   \n"," ...             ...     ...             ...      ...         ...   \n"," 5421       0.089477       1               0        0           0   \n"," 5945       0.132322       1               0        0           0   \n"," 945        0.389680       1               1        0           0   \n"," 6246       0.875986       1               1        1           0   \n"," 6745       0.221563       1               1        1           0   \n"," \n","       internet_service  online_security  online_backup  device_protection  \\\n"," 95                 1.0              1.0            0.0                0.0   \n"," 4644               0.0              0.0            0.0                0.0   \n"," 5562               0.0              0.0            1.0                1.0   \n"," 320                0.0              1.0            1.0                0.0   \n"," 303                0.0              0.0            1.0                1.0   \n"," ...                ...              ...            ...                ...   \n"," 5421               0.0              0.0            0.0                0.0   \n"," 5945               0.0              1.0            1.0                1.0   \n"," 945                1.0              0.0            0.0                1.0   \n"," 6246               1.0              1.0            0.0                1.0   \n"," 6745               1.0              0.0            0.0                0.0   \n"," \n","       tech_support  streaming_tv  streaming_movies  multiple_lines  \\\n"," 95             0.0           0.0               0.0             1.0   \n"," 4644           0.0           0.0               0.0             0.0   \n"," 5562           0.0           1.0               1.0             1.0   \n"," 320            1.0           1.0               1.0             1.0   \n"," 303            1.0           1.0               1.0             0.0   \n"," ...            ...           ...               ...             ...   \n"," 5421           0.0           1.0               1.0             1.0   \n"," 5945           1.0           1.0               0.0             0.0   \n"," 945            0.0           1.0               1.0             1.0   \n"," 6246           1.0           1.0               1.0             1.0   \n"," 6745           0.0           1.0               1.0             0.0   \n"," \n","       begin_year  begin_month  begin_dayofweek  \n"," 95      0.714286     1.000000         0.833333  \n"," 4644    0.714286     0.181818         0.500000  \n"," 5562    0.285714     0.000000         0.500000  \n"," 320     0.285714     0.363636         0.666667  \n"," 303     0.142857     0.454545         1.000000  \n"," ...          ...          ...              ...  \n"," 5421    0.857143     0.090909         0.666667  \n"," 5945    0.714286     0.454545         0.666667  \n"," 945     0.571429     0.090909         0.333333  \n"," 6246    0.142857     0.363636         0.500000  \n"," 6745    0.714286     0.272727         1.000000  \n"," \n"," [4218 rows x 20 columns],\n","       contract_type  paperless_billing  payment_method  monthly_charges  \\\n"," 469               0                  0               2         0.375498   \n"," 4590              0                  0               3         0.378486   \n"," 5822              0                  1               0         0.722610   \n"," 5084              0                  0               3         0.021912   \n"," 2003              0                  1               3         0.685757   \n"," ...             ...                ...             ...              ...   \n"," 1386              0                  1               2         0.672809   \n"," 1597              0                  1               2         0.717131   \n"," 4111              0                  0               1         0.563247   \n"," 6510              1                  1               2         0.728088   \n"," 6387              0                  1               1         0.734064   \n"," \n","       total_charges  gender  senior_citizen  partner  dependents  \\\n"," 469        0.082565       0               0        1           1   \n"," 4590       0.109740       1               0        0           0   \n"," 5822       0.091265       0               0        0           0   \n"," 5084       0.015220       0               0        1           1   \n"," 2003       0.037226       0               0        0           1   \n"," ...             ...     ...             ...      ...         ...   \n"," 1386       0.279415       1               0        1           0   \n"," 1597       0.315776       1               0        0           0   \n"," 4111       0.340742       0               0        0           0   \n"," 6510       0.663038       1               0        1           1   \n"," 6387       0.378747       0               1        1           0   \n"," \n","       internet_service  online_security  online_backup  device_protection  \\\n"," 469                0.0              0.0            0.0                0.0   \n"," 4590               0.0              1.0            0.0                0.0   \n"," 5822               1.0              0.0            0.0                0.0   \n"," 5084               0.0              0.0            0.0                0.0   \n"," 2003               1.0              0.0            0.0                1.0   \n"," ...                ...              ...            ...                ...   \n"," 1386               1.0              0.0            1.0                1.0   \n"," 1597               1.0              0.0            0.0                0.0   \n"," 4111               1.0              0.0            1.0                0.0   \n"," 6510               1.0              0.0            1.0                0.0   \n"," 6387               1.0              0.0            1.0                0.0   \n"," \n","       tech_support  streaming_tv  streaming_movies  multiple_lines  \\\n"," 469            0.0           0.0               1.0             0.0   \n"," 4590           0.0           0.0               0.0             1.0   \n"," 5822           0.0           1.0               1.0             0.0   \n"," 5084           0.0           0.0               0.0             0.0   \n"," 2003           1.0           0.0               0.0             1.0   \n"," ...            ...           ...               ...             ...   \n"," 1386           0.0           0.0               0.0             1.0   \n"," 1597           0.0           1.0               1.0             0.0   \n"," 4111           0.0           0.0               0.0             0.0   \n"," 6510           0.0           0.0               1.0             1.0   \n"," 6387           0.0           1.0               0.0             1.0   \n"," \n","       begin_year  begin_month  begin_dayofweek  \n"," 469     0.714286     1.000000         0.833333  \n"," 4590    0.714286     0.636364         0.333333  \n"," 5822    0.857143     0.181818         0.666667  \n"," 5084    0.857143     0.545455         0.000000  \n"," 2003    0.857143     0.454545         0.833333  \n"," ...          ...          ...              ...  \n"," 1386    0.571429     0.727273         0.666667  \n"," 1597    0.571429     0.272727         0.833333  \n"," 4111    0.428571     0.818182         0.833333  \n"," 6510    0.142857     0.909091         0.833333  \n"," 6387    0.571429     0.090909         0.333333  \n"," \n"," [1407 rows x 20 columns],\n","       contract_type  paperless_billing  payment_method  monthly_charges  \\\n"," 3307              0                  0               3         0.020418   \n"," 679               0                  0               2         0.615040   \n"," 4724              2                  0               1         0.016434   \n"," 569               2                  1               1         0.009960   \n"," 2928              2                  1               0         0.057271   \n"," ...             ...                ...             ...              ...   \n"," 2991              0                  1               3         0.421813   \n"," 6725              2                  0               1         0.687251   \n"," 5068              1                  1               2         0.752490   \n"," 4212              1                  0               2         0.566733   \n"," 205               1                  1               0         0.614542   \n"," \n","       total_charges  gender  senior_citizen  partner  dependents  \\\n"," 3307       0.000167       1               0        0           0   \n"," 679        0.007056       1               0        1           1   \n"," 4724       0.159065       0               0        1           1   \n"," 569        0.125133       0               0        1           1   \n"," 2928       0.189875       1               0        1           1   \n"," ...             ...     ...             ...      ...         ...   \n"," 2991       0.131261       0               0        0           0   \n"," 6725       0.728120       0               0        1           1   \n"," 5068       0.358374       0               0        1           1   \n"," 4212       0.433167       0               0        0           0   \n"," 205        0.307641       0               0        0           0   \n"," \n","       internet_service  online_security  online_backup  device_protection  \\\n"," 3307               0.0              0.0            0.0                0.0   \n"," 679                1.0              0.0            0.0                0.0   \n"," 4724               0.0              0.0            0.0                0.0   \n"," 569                0.0              0.0            0.0                0.0   \n"," 2928               0.0              0.0            0.0                0.0   \n"," ...                ...              ...            ...                ...   \n"," 2991               0.0              1.0            0.0                1.0   \n"," 6725               0.0              1.0            1.0                1.0   \n"," 5068               1.0              0.0            0.0                1.0   \n"," 4212               1.0              0.0            1.0                0.0   \n"," 205                1.0              0.0            1.0                0.0   \n"," \n","       tech_support  streaming_tv  streaming_movies  multiple_lines  \\\n"," 3307           0.0           0.0               0.0             0.0   \n"," 679            0.0           0.0               1.0             0.0   \n"," 4724           0.0           0.0               0.0             0.0   \n"," 569            0.0           0.0               0.0             0.0   \n"," 2928           0.0           0.0               0.0             1.0   \n"," ...            ...           ...               ...             ...   \n"," 2991           0.0           0.0               0.0             1.0   \n"," 6725           1.0           1.0               1.0             0.0   \n"," 5068           0.0           1.0               1.0             0.0   \n"," 4212           0.0           0.0               0.0             0.0   \n"," 205            1.0           0.0               0.0             0.0   \n"," \n","       begin_year  begin_month  begin_dayofweek  \n"," 3307    1.000000     0.000000         0.333333  \n"," 679     0.857143     1.000000         1.000000  \n"," 4724    0.142857     0.181818         0.833333  \n"," 569     0.285714     0.090909         1.000000  \n"," 2928    0.142857     0.454545         1.000000  \n"," ...          ...          ...              ...  \n"," 2991    0.714286     0.636364         0.333333  \n"," 6725    0.142857     0.181818         0.833333  \n"," 5068    0.571429     0.000000         1.000000  \n"," 4212    0.428571     0.090909         0.000000  \n"," 205     0.571429     0.363636         0.000000  \n"," \n"," [1407 rows x 20 columns])"]},"execution_count":108,"metadata":{},"output_type":"execute_result"}],"source":["numeric = ['monthly_charges', 'total_charges', 'begin_year','begin_month', 'begin_dayofweek']\n","\n","def scaling(x_train, x_valid, x_test):\n","    scaler = MinMaxScaler()\n","    scaler.fit(x_train[numeric])\n","    x_train[numeric] = scaler.transform(x_train[numeric])\n","    x_valid[numeric] = scaler.transform(x_valid[numeric])\n","    x_test[numeric] = scaler.transform(x_test[numeric])\n","    return x_train, x_valid, x_test\n","\n","scaling(features_train, features_valid, features_test)"]},{"cell_type":"markdown","metadata":{},"source":["`Summary`\n","\n","After taking into account class imbalancing, we've split the data into Train, Validation and Test datasets where each has been scaled in order to take value magnitute into account and create good inputs for our model training."]},{"cell_type":"markdown","metadata":{},"source":["# Dummy Model"]},{"cell_type":"code","execution_count":109,"metadata":{"execution":{"iopub.execute_input":"2023-11-30T17:45:15.130534Z","iopub.status.busy":"2023-11-30T17:45:15.130173Z","iopub.status.idle":"2023-11-30T17:45:15.139236Z","shell.execute_reply":"2023-11-30T17:45:15.138270Z","shell.execute_reply.started":"2023-11-30T17:45:15.130498Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Dummy Model Score: 0.7341862117981521\n"]}],"source":["dummy = DummyClassifier(random_state=random_state,strategy=\"most_frequent\")\n","dummy.fit(features_train, target_train)\n","DummyClassifier(strategy='most_frequent')\n","dummy.predict(features_valid)\n","print('Dummy Model Score:', dummy.score(features_valid, target_valid))"]},{"cell_type":"markdown","metadata":{},"source":["# Random Forest"]},{"cell_type":"code","execution_count":110,"metadata":{"execution":{"iopub.execute_input":"2023-11-30T17:47:07.682082Z","iopub.status.busy":"2023-11-30T17:47:07.681288Z","iopub.status.idle":"2023-11-30T17:47:21.988070Z","shell.execute_reply":"2023-11-30T17:47:21.987028Z","shell.execute_reply.started":"2023-11-30T17:47:07.682047Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Runtime:\n","CPU times: user 1.18 s, sys: 451 ms, total: 1.63 s\n","Wall time: 22.1 s\n"]}],"source":["%%time\n","forest_model = RandomForestClassifier(random_state=random_state)\n","forest_parameters = [{'max_depth': [2,6,12,18,30],\n","                     'min_samples_split': [2,6,12],\n","                     \"criterion\": ['gini', 'entropy', 'log_loss'],\n","                     \"warm_start\": [True, False],\n","                     'n_estimators': [50,100,200]}]\n","\n","forest_clf = RandomizedSearchCV(forest_model, forest_parameters, scoring='roc_auc', n_jobs=-1, cv=cv)\n","forest_clf.fit(features_train, target_train)\n","# create a variable for the best model\n","best_for = forest_clf.best_estimator_\n","for_pred = best_for.predict(features_valid)\n","print('Runtime:')"]},{"cell_type":"code","execution_count":111,"metadata":{"execution":{"iopub.execute_input":"2023-11-30T16:51:02.675236Z","iopub.status.busy":"2023-11-30T16:51:02.674939Z","iopub.status.idle":"2023-11-30T16:51:28.806061Z","shell.execute_reply":"2023-11-30T16:51:28.805200Z","shell.execute_reply.started":"2023-11-30T16:51:02.675211Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Cross Validation Scores: [0.88701779 0.86640265 0.88940092 0.84344758 0.87862903 0.87919067\n"," 0.86424251 0.84251152 0.89613673 0.88196371 0.83515193 0.86029666\n"," 0.89203629 0.87079493 0.88859447 0.87049251 0.89919355 0.86601382\n"," 0.90471856 0.87202381 0.84351462 0.896803   0.8889977  0.8749424\n"," 0.87351671 0.90600518 0.88781682 0.88974654 0.87455213 0.83183079\n"," 0.87531861 0.85427707 0.87151498 0.89668779 0.89811348 0.8905962\n"," 0.865625   0.865553   0.87138812 0.87749942 0.87534725 0.88326613\n"," 0.85733007 0.84854551 0.87587846 0.8812212  0.88701037 0.90756048\n"," 0.8802589  0.884911  ]\n"]}],"source":["forest_scores = cross_val_score(forest_model, features_train, target_train, cv=cv, scoring='roc_auc')\n","print('Cross Validation Scores: {}'.format(forest_scores))"]},{"cell_type":"code","execution_count":112,"metadata":{"execution":{"iopub.execute_input":"2023-11-30T16:51:28.815144Z","iopub.status.busy":"2023-11-30T16:51:28.814760Z","iopub.status.idle":"2023-11-30T16:51:28.886083Z","shell.execute_reply":"2023-11-30T16:51:28.885261Z","shell.execute_reply.started":"2023-11-30T16:51:28.815112Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Best hyperparameters: {'warm_start': True, 'n_estimators': 200, 'min_samples_split': 12, 'max_depth': 18, 'criterion': 'log_loss'}\n","\n","Best score: 0.8757086077414787\n","\n","Average Cross Validation Score: 0.8760777711552042\n","\n","ROC AUC Score - Validation Dataset: 0.8994414275434718\n"]}],"source":["# summary\n","print('Best hyperparameters:',  forest_clf.best_params_)\n","print()\n","print('Best score:', forest_clf.best_score_)\n","print()\n","print('Average Cross Validation Score: {}'.format(forest_scores.mean()))\n","print()\n","print('ROC AUC Score - Validation Dataset:',  roc_auc_score(target_valid, forest_clf.predict_proba(features_valid)[:, 1]))"]},{"cell_type":"markdown","metadata":{},"source":["# ROC AUC Curve - RandomForestClassifier"]},{"cell_type":"code","execution_count":113,"metadata":{"execution":{"iopub.execute_input":"2023-11-30T16:51:28.887485Z","iopub.status.busy":"2023-11-30T16:51:28.887134Z","iopub.status.idle":"2023-11-30T16:51:29.109214Z","shell.execute_reply":"2023-11-30T16:51:29.108341Z","shell.execute_reply.started":"2023-11-30T16:51:28.887453Z"},"trusted":true},"outputs":[{"data":{"application/vnd.plotly.v1+json":{"config":{"plotlyServerURL":"https://plot.ly"},"data":[{"fillpattern":{"shape":""},"hovertemplate":"False Positive Rate=%{x}<br>True Positive Rate=%{y}<extra></extra>","legendgroup":"","line":{"color":"#636efa"},"marker":{"symbol":"circle"},"mode":"lines","name":"","orientation":"v","showlegend":false,"stackgroup":"1","type":"scatter","x":[0,0,0,0,0,0,0,0.00267379679144385,0.00267379679144385,0.00267379679144385,0.00267379679144385,0.00267379679144385,0.00267379679144385,0.0053475935828877,0.0053475935828877,0.008021390374331552,0.008021390374331552,0.0106951871657754,0.0106951871657754,0.013368983957219251,0.013368983957219251,0.016042780748663103,0.016042780748663103,0.01871657754010695,0.01871657754010695,0.0213903743315508,0.0213903743315508,0.02406417112299465,0.02406417112299465,0.026737967914438502,0.026737967914438502,0.029411764705882353,0.029411764705882353,0.03208556149732621,0.03208556149732621,0.034759358288770054,0.034759358288770054,0.0374331550802139,0.0374331550802139,0.040106951871657755,0.040106951871657755,0.0427807486631016,0.0427807486631016,0.045454545454545456,0.045454545454545456,0.0481283422459893,0.0481283422459893,0.05080213903743316,0.05080213903743316,0.05614973262032086,0.05614973262032086,0.058823529411764705,0.058823529411764705,0.06149732620320856,0.06149732620320856,0.06417112299465241,0.06417112299465241,0.06684491978609626,0.06684491978609626,0.06951871657754011,0.06951871657754011,0.07219251336898395,0.07219251336898395,0.0748663101604278,0.0748663101604278,0.07754010695187166,0.07754010695187166,0.08021390374331551,0.08021390374331551,0.08288770053475936,0.08288770053475936,0.0855614973262032,0.0855614973262032,0.08823529411764706,0.08823529411764706,0.09090909090909091,0.09090909090909091,0.09358288770053476,0.09358288770053476,0.0962566844919786,0.0962566844919786,0.09893048128342247,0.09893048128342247,0.10160427807486631,0.10160427807486631,0.10427807486631016,0.10427807486631016,0.10695187165775401,0.10695187165775401,0.10962566844919786,0.10962566844919786,0.11229946524064172,0.11229946524064172,0.11497326203208556,0.11497326203208556,0.11764705882352941,0.11764705882352941,0.12032085561497326,0.12032085561497326,0.12299465240641712,0.12299465240641712,0.12566844919786097,0.12566844919786097,0.13101604278074866,0.13101604278074866,0.13368983957219252,0.13368983957219252,0.13636363636363635,0.13636363636363635,0.13903743315508021,0.13903743315508021,0.1443850267379679,0.1443850267379679,0.14705882352941177,0.14705882352941177,0.1497326203208556,0.1497326203208556,0.15240641711229946,0.15240641711229946,0.15508021390374332,0.15508021390374332,0.16310160427807488,0.16310160427807488,0.16844919786096257,0.16844919786096257,0.1711229946524064,0.1711229946524064,0.17379679144385027,0.17379679144385027,0.17914438502673796,0.17914438502673796,0.18181818181818182,0.18181818181818182,0.18449197860962566,0.18449197860962566,0.18716577540106952,0.18716577540106952,0.18983957219251338,0.18983957219251338,0.19518716577540107,0.19518716577540107,0.20320855614973263,0.20320855614973263,0.20855614973262032,0.20855614973262032,0.21122994652406418,0.21122994652406418,0.21390374331550802,0.21390374331550802,0.21657754010695188,0.21657754010695188,0.2192513368983957,0.2192513368983957,0.22192513368983957,0.22192513368983957,0.22459893048128343,0.22459893048128343,0.232620320855615,0.232620320855615,0.23529411764705882,0.23529411764705882,0.23796791443850268,0.23796791443850268,0.24064171122994651,0.24064171122994651,0.24331550802139038,0.24331550802139038,0.24598930481283424,0.24598930481283424,0.25133689839572193,0.25133689839572193,0.2540106951871658,0.2540106951871658,0.25668449197860965,0.25668449197860965,0.25935828877005346,0.25935828877005346,0.2620320855614973,0.2620320855614973,0.2647058823529412,0.2647058823529412,0.26737967914438504,0.26737967914438504,0.2700534759358289,0.2700534759358289,0.27807486631016043,0.27807486631016043,0.2914438502673797,0.2914438502673797,0.2967914438502674,0.2967914438502674,0.2994652406417112,0.2994652406417112,0.3074866310160428,0.3074866310160428,0.31283422459893045,0.31283422459893045,0.3155080213903743,0.3155080213903743,0.3181818181818182,0.3181818181818182,0.32887700534759357,0.32887700534759357,0.3315508021390374,0.3315508021390374,0.33689839572192515,0.33689839572192515,0.3422459893048128,0.3422459893048128,0.3449197860962567,0.3449197860962567,0.34759358288770054,0.34759358288770054,0.35294117647058826,0.35294117647058826,0.35561497326203206,0.35561497326203206,0.3689839572192513,0.3689839572192513,0.3716577540106952,0.3716577540106952,0.37433155080213903,0.37433155080213903,0.38235294117647056,0.38235294117647056,0.3850267379679144,0.3850267379679144,0.3877005347593583,0.3877005347593583,0.39037433155080214,0.39037433155080214,0.3983957219251337,0.3983957219251337,0.40106951871657753,0.40106951871657753,0.41711229946524064,0.41711229946524064,0.4385026737967914,0.4385026737967914,0.44385026737967914,0.44385026737967914,0.44919786096256686,0.44919786096256686,0.45989304812834225,0.45989304812834225,0.4625668449197861,0.4625668449197861,0.4679144385026738,0.4679144385026738,0.4786096256684492,0.4786096256684492,0.48128342245989303,0.48128342245989303,0.48663101604278075,0.48663101604278075,0.4919786096256685,0.4919786096256685,0.49732620320855614,0.49732620320855614,0.5,0.5,0.5106951871657754,0.5106951871657754,0.516042780748663,0.516042780748663,0.5213903743315508,0.5213903743315508,0.5427807486631016,0.5427807486631016,0.5454545454545454,0.5454545454545454,0.5481283422459893,0.5481283422459893,0.56951871657754,0.56951871657754,0.5802139037433155,0.5802139037433155,0.5855614973262032,0.5855614973262032,0.6176470588235294,0.6176470588235294,0.6443850267379679,0.6443850267379679,0.6550802139037433,0.6550802139037433,0.6711229946524064,0.6711229946524064,0.732620320855615,0.732620320855615,0.7352941176470589,0.7352941176470589,1],"xaxis":"x","y":[0,0.010648596321393998,0.03872216844143272,0.04065827686350436,0.0484027105517909,0.05033881897386254,0.057115198451113264,0.057115198451113264,0.060019361084220714,0.061955469506292354,0.08131655372700872,0.08325266214908035,0.11713455953533398,0.11713455953533398,0.1713455953533398,0.1713455953533398,0.22555663117134558,0.22555663117134558,0.24007744433688286,0.24007744433688286,0.2468538238141336,0.2468538238141336,0.2942884801548887,0.2942884801548887,0.33204259438528555,0.33204259438528555,0.3552758954501452,0.3552758954501452,0.4085188770571152,0.4085188770571152,0.4762826718296225,0.4762826718296225,0.48693126815101645,0.48693126815101645,0.4995159728944821,0.4995159728944821,0.5014520813165537,0.5014520813165537,0.5062923523717329,0.5062923523717329,0.5072604065827686,0.5072604065827686,0.5188770571151985,0.5188770571151985,0.5227492739593417,0.5227492739593417,0.542110358180058,0.542110358180058,0.5440464666021297,0.5440464666021297,0.547918683446273,0.547918683446273,0.558567279767667,0.558567279767667,0.5614714424007744,0.5614714424007744,0.5624394966118103,0.5624394966118103,0.5643756050338818,0.5643756050338818,0.5953533397870281,0.5953533397870281,0.5963213939980639,0.5963213939980639,0.6021297192642788,0.6021297192642788,0.6050338818973863,0.6050338818973863,0.6176185866408519,0.6176185866408519,0.6195546950629235,0.6195546950629235,0.6234269119070668,0.6234269119070668,0.6282671829622459,0.6282671829622459,0.633107454017425,0.633107454017425,0.6340755082284608,0.6340755082284608,0.6360116166505324,0.6360116166505324,0.6602129719264279,0.6602129719264279,0.6611810261374637,0.6611810261374637,0.6689254598257502,0.6689254598257502,0.6698935140367861,0.6698935140367861,0.6727976766698935,0.6727976766698935,0.6882865440464666,0.6882865440464666,0.7028073572120038,0.7028073572120038,0.7289448209099709,0.7289448209099709,0.7308809293320426,0.7308809293320426,0.7328170377541142,0.7328170377541142,0.7366892545982575,0.7366892545982575,0.7415295256534365,0.7415295256534365,0.7434656340755083,0.7434656340755083,0.7618586640851888,0.7618586640851888,0.7647628267182962,0.7647628267182962,0.7657308809293321,0.7657308809293321,0.7666989351403679,0.7666989351403679,0.7686350435624395,0.7686350435624395,0.771539206195547,0.771539206195547,0.7773475314617618,0.7773475314617618,0.7802516940948693,0.7802516940948693,0.782187802516941,0.782187802516941,0.7879961277831559,0.7879961277831559,0.7909002904162633,0.7909002904162633,0.8054211035818006,0.8054211035818006,0.8063891577928364,0.8063891577928364,0.8121974830590513,0.8121974830590513,0.814133591481123,0.814133591481123,0.8170377541142304,0.8170377541142304,0.8209099709583737,0.8209099709583737,0.8276863504356244,0.8276863504356244,0.829622458857696,0.829622458857696,0.8305905130687319,0.8305905130687319,0.8354307841239109,0.8354307841239109,0.8393030009680542,0.8393030009680542,0.8412391093901258,0.8412391093901258,0.8489835430784124,0.8489835430784124,0.8499515972894482,0.8499515972894482,0.8518877057115198,0.8518877057115198,0.8538238141335914,0.8538238141335914,0.856727976766699,0.856727976766699,0.8606001936108422,0.8606001936108422,0.861568247821878,0.861568247821878,0.8664085188770572,0.8664085188770572,0.872216844143272,0.872216844143272,0.8760890609874153,0.8760890609874153,0.8780251694094869,0.8780251694094869,0.882865440464666,0.882865440464666,0.888673765730881,0.888673765730881,0.8906098741529526,0.8906098741529526,0.8925459825750242,0.8925459825750242,0.8973862536302033,0.8973862536302033,0.9022265246853823,0.9022265246853823,0.9070667957405615,0.9070667957405615,0.9099709583736689,0.9099709583736689,0.9119070667957405,0.9119070667957405,0.9167473378509197,0.9167473378509197,0.9177153920619555,0.9177153920619555,0.9186834462729913,0.9186834462729913,0.920619554695063,0.920619554695063,0.9225556631171346,0.9225556631171346,0.9235237173281704,0.9235237173281704,0.9244917715392061,0.9244917715392061,0.9273959341723137,0.9273959341723137,0.9341723136495643,0.9341723136495643,0.9351403678606002,0.9351403678606002,0.9370764762826719,0.9370764762826719,0.9390125847047435,0.9390125847047435,0.9419167473378509,0.9419167473378509,0.9438528557599225,0.9438528557599225,0.9448209099709584,0.9448209099709584,0.9457889641819942,0.9457889641819942,0.9477250726040658,0.9477250726040658,0.9486931268151017,0.9486931268151017,0.9496611810261375,0.9496611810261375,0.9603097773475314,0.9603097773475314,0.9651500484027106,0.9651500484027106,0.9661181026137464,0.9661181026137464,0.9670861568247822,0.9670861568247822,0.9690222652468539,0.9690222652468539,0.9699903194578896,0.9699903194578896,0.9719264278799613,0.9719264278799613,0.972894482090997,0.972894482090997,0.9738625363020329,0.9738625363020329,0.9748305905130688,0.9748305905130688,0.9757986447241046,0.9757986447241046,0.9777347531461762,0.9777347531461762,0.978702807357212,0.978702807357212,0.9796708615682478,0.9796708615682478,0.9816069699903195,0.9816069699903195,0.9835430784123911,0.9835430784123911,0.9845111326234269,0.9845111326234269,0.9854791868344628,0.9854791868344628,0.9864472410454985,0.9864472410454985,0.9874152952565344,0.9874152952565344,0.989351403678606,0.989351403678606,0.9903194578896418,0.9903194578896418,0.9932236205227493,0.9932236205227493,0.9941916747337851,0.9941916747337851,0.995159728944821,0.995159728944821,0.9961277831558567,0.9961277831558567,0.9970958373668926,0.9970958373668926,0.9980638915779284,0.9980638915779284,0.9990319457889641,0.9990319457889641,1,1],"yaxis":"y"}],"layout":{"height":600,"legend":{"tracegroupgap":0},"shapes":[{"line":{"dash":"dash"},"type":"line","x0":0,"x1":1,"y0":0,"y1":1}],"showlegend":false,"template":{"data":{"bar":[{"error_x":{"color":"#2a3f5f"},"error_y":{"color":"#2a3f5f"},"marker":{"line":{"color":"white","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"barpolar":[{"marker":{"line":{"color":"white","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"carpet":[{"aaxis":{"endlinecolor":"#2a3f5f","gridcolor":"#C8D4E3","linecolor":"#C8D4E3","minorgridcolor":"#C8D4E3","startlinecolor":"#2a3f5f"},"baxis":{"endlinecolor":"#2a3f5f","gridcolor":"#C8D4E3","linecolor":"#C8D4E3","minorgridcolor":"#C8D4E3","startlinecolor":"#2a3f5f"},"type":"carpet"}],"choropleth":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"choropleth"}],"contour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"contour"}],"contourcarpet":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"contourcarpet"}],"heatmap":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmap"}],"heatmapgl":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmapgl"}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"histogram2d":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2d"}],"histogram2dcontour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2dcontour"}],"mesh3d":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"mesh3d"}],"parcoords":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"parcoords"}],"pie":[{"automargin":true,"type":"pie"}],"scatter":[{"fillpattern":{"fillmode":"overlay","size":10,"solidity":0.2},"type":"scatter"}],"scatter3d":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter3d"}],"scattercarpet":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattercarpet"}],"scattergeo":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergeo"}],"scattergl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergl"}],"scattermapbox":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattermapbox"}],"scatterpolar":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolar"}],"scatterpolargl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolargl"}],"scatterternary":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterternary"}],"surface":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"surface"}],"table":[{"cells":{"fill":{"color":"#EBF0F8"},"line":{"color":"white"}},"header":{"fill":{"color":"#C8D4E3"},"line":{"color":"white"}},"type":"table"}]},"layout":{"annotationdefaults":{"arrowcolor":"#2a3f5f","arrowhead":0,"arrowwidth":1},"autotypenumbers":"strict","coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]],"sequential":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"sequentialminus":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]]},"colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#2a3f5f"},"geo":{"bgcolor":"white","lakecolor":"white","landcolor":"white","showlakes":true,"showland":true,"subunitcolor":"#C8D4E3"},"hoverlabel":{"align":"left"},"hovermode":"closest","mapbox":{"style":"light"},"paper_bgcolor":"white","plot_bgcolor":"white","polar":{"angularaxis":{"gridcolor":"#EBF0F8","linecolor":"#EBF0F8","ticks":""},"bgcolor":"white","radialaxis":{"gridcolor":"#EBF0F8","linecolor":"#EBF0F8","ticks":""}},"scene":{"xaxis":{"backgroundcolor":"white","gridcolor":"#DFE8F3","gridwidth":2,"linecolor":"#EBF0F8","showbackground":true,"ticks":"","zerolinecolor":"#EBF0F8"},"yaxis":{"backgroundcolor":"white","gridcolor":"#DFE8F3","gridwidth":2,"linecolor":"#EBF0F8","showbackground":true,"ticks":"","zerolinecolor":"#EBF0F8"},"zaxis":{"backgroundcolor":"white","gridcolor":"#DFE8F3","gridwidth":2,"linecolor":"#EBF0F8","showbackground":true,"ticks":"","zerolinecolor":"#EBF0F8"}},"shapedefaults":{"line":{"color":"#2a3f5f"}},"ternary":{"aaxis":{"gridcolor":"#DFE8F3","linecolor":"#A2B1C6","ticks":""},"baxis":{"gridcolor":"#DFE8F3","linecolor":"#A2B1C6","ticks":""},"bgcolor":"white","caxis":{"gridcolor":"#DFE8F3","linecolor":"#A2B1C6","ticks":""}},"title":{"x":0.05},"xaxis":{"automargin":true,"gridcolor":"#EBF0F8","linecolor":"#EBF0F8","ticks":"","title":{"standoff":15},"zerolinecolor":"#EBF0F8","zerolinewidth":2},"yaxis":{"automargin":true,"gridcolor":"#EBF0F8","linecolor":"#EBF0F8","ticks":"","title":{"standoff":15},"zerolinecolor":"#EBF0F8","zerolinewidth":2}}},"title":{"text":"ROC Curve (AUC=0.8994)"},"width":800,"xaxis":{"anchor":"y","constrain":"domain","domain":[0,1],"title":{"text":"False Positive Rate"}},"yaxis":{"anchor":"x","domain":[0,1],"scaleanchor":"x","scaleratio":1,"title":{"text":"True Positive Rate"}}}},"text/html":["<div>                            <div id=\"db9d5428-9f9e-4b1d-b1c5-080a85f86207\" class=\"plotly-graph-div\" style=\"height:600px; width:800px;\"></div>            <script type=\"text/javascript\">                require([\"plotly\"], function(Plotly) {                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"db9d5428-9f9e-4b1d-b1c5-080a85f86207\")) {                    Plotly.newPlot(                        \"db9d5428-9f9e-4b1d-b1c5-080a85f86207\",                        [{\"fillpattern\":{\"shape\":\"\"},\"hovertemplate\":\"False Positive Rate=%{x}\\u003cbr\\u003eTrue Positive Rate=%{y}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"\",\"line\":{\"color\":\"#636efa\"},\"marker\":{\"symbol\":\"circle\"},\"mode\":\"lines\",\"name\":\"\",\"orientation\":\"v\",\"showlegend\":false,\"stackgroup\":\"1\",\"x\":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.00267379679144385,0.00267379679144385,0.00267379679144385,0.00267379679144385,0.00267379679144385,0.00267379679144385,0.0053475935828877,0.0053475935828877,0.008021390374331552,0.008021390374331552,0.0106951871657754,0.0106951871657754,0.013368983957219251,0.013368983957219251,0.016042780748663103,0.016042780748663103,0.01871657754010695,0.01871657754010695,0.0213903743315508,0.0213903743315508,0.02406417112299465,0.02406417112299465,0.026737967914438502,0.026737967914438502,0.029411764705882353,0.029411764705882353,0.03208556149732621,0.03208556149732621,0.034759358288770054,0.034759358288770054,0.0374331550802139,0.0374331550802139,0.040106951871657755,0.040106951871657755,0.0427807486631016,0.0427807486631016,0.045454545454545456,0.045454545454545456,0.0481283422459893,0.0481283422459893,0.05080213903743316,0.05080213903743316,0.05614973262032086,0.05614973262032086,0.058823529411764705,0.058823529411764705,0.06149732620320856,0.06149732620320856,0.06417112299465241,0.06417112299465241,0.06684491978609626,0.06684491978609626,0.06951871657754011,0.06951871657754011,0.07219251336898395,0.07219251336898395,0.0748663101604278,0.0748663101604278,0.07754010695187166,0.07754010695187166,0.08021390374331551,0.08021390374331551,0.08288770053475936,0.08288770053475936,0.0855614973262032,0.0855614973262032,0.08823529411764706,0.08823529411764706,0.09090909090909091,0.09090909090909091,0.09358288770053476,0.09358288770053476,0.0962566844919786,0.0962566844919786,0.09893048128342247,0.09893048128342247,0.10160427807486631,0.10160427807486631,0.10427807486631016,0.10427807486631016,0.10695187165775401,0.10695187165775401,0.10962566844919786,0.10962566844919786,0.11229946524064172,0.11229946524064172,0.11497326203208556,0.11497326203208556,0.11764705882352941,0.11764705882352941,0.12032085561497326,0.12032085561497326,0.12299465240641712,0.12299465240641712,0.12566844919786097,0.12566844919786097,0.13101604278074866,0.13101604278074866,0.13368983957219252,0.13368983957219252,0.13636363636363635,0.13636363636363635,0.13903743315508021,0.13903743315508021,0.1443850267379679,0.1443850267379679,0.14705882352941177,0.14705882352941177,0.1497326203208556,0.1497326203208556,0.15240641711229946,0.15240641711229946,0.15508021390374332,0.15508021390374332,0.16310160427807488,0.16310160427807488,0.16844919786096257,0.16844919786096257,0.1711229946524064,0.1711229946524064,0.17379679144385027,0.17379679144385027,0.17914438502673796,0.17914438502673796,0.18181818181818182,0.18181818181818182,0.18449197860962566,0.18449197860962566,0.18716577540106952,0.18716577540106952,0.18983957219251338,0.18983957219251338,0.19518716577540107,0.19518716577540107,0.20320855614973263,0.20320855614973263,0.20855614973262032,0.20855614973262032,0.21122994652406418,0.21122994652406418,0.21390374331550802,0.21390374331550802,0.21657754010695188,0.21657754010695188,0.2192513368983957,0.2192513368983957,0.22192513368983957,0.22192513368983957,0.22459893048128343,0.22459893048128343,0.232620320855615,0.232620320855615,0.23529411764705882,0.23529411764705882,0.23796791443850268,0.23796791443850268,0.24064171122994651,0.24064171122994651,0.24331550802139038,0.24331550802139038,0.24598930481283424,0.24598930481283424,0.25133689839572193,0.25133689839572193,0.2540106951871658,0.2540106951871658,0.25668449197860965,0.25668449197860965,0.25935828877005346,0.25935828877005346,0.2620320855614973,0.2620320855614973,0.2647058823529412,0.2647058823529412,0.26737967914438504,0.26737967914438504,0.2700534759358289,0.2700534759358289,0.27807486631016043,0.27807486631016043,0.2914438502673797,0.2914438502673797,0.2967914438502674,0.2967914438502674,0.2994652406417112,0.2994652406417112,0.3074866310160428,0.3074866310160428,0.31283422459893045,0.31283422459893045,0.3155080213903743,0.3155080213903743,0.3181818181818182,0.3181818181818182,0.32887700534759357,0.32887700534759357,0.3315508021390374,0.3315508021390374,0.33689839572192515,0.33689839572192515,0.3422459893048128,0.3422459893048128,0.3449197860962567,0.3449197860962567,0.34759358288770054,0.34759358288770054,0.35294117647058826,0.35294117647058826,0.35561497326203206,0.35561497326203206,0.3689839572192513,0.3689839572192513,0.3716577540106952,0.3716577540106952,0.37433155080213903,0.37433155080213903,0.38235294117647056,0.38235294117647056,0.3850267379679144,0.3850267379679144,0.3877005347593583,0.3877005347593583,0.39037433155080214,0.39037433155080214,0.3983957219251337,0.3983957219251337,0.40106951871657753,0.40106951871657753,0.41711229946524064,0.41711229946524064,0.4385026737967914,0.4385026737967914,0.44385026737967914,0.44385026737967914,0.44919786096256686,0.44919786096256686,0.45989304812834225,0.45989304812834225,0.4625668449197861,0.4625668449197861,0.4679144385026738,0.4679144385026738,0.4786096256684492,0.4786096256684492,0.48128342245989303,0.48128342245989303,0.48663101604278075,0.48663101604278075,0.4919786096256685,0.4919786096256685,0.49732620320855614,0.49732620320855614,0.5,0.5,0.5106951871657754,0.5106951871657754,0.516042780748663,0.516042780748663,0.5213903743315508,0.5213903743315508,0.5427807486631016,0.5427807486631016,0.5454545454545454,0.5454545454545454,0.5481283422459893,0.5481283422459893,0.56951871657754,0.56951871657754,0.5802139037433155,0.5802139037433155,0.5855614973262032,0.5855614973262032,0.6176470588235294,0.6176470588235294,0.6443850267379679,0.6443850267379679,0.6550802139037433,0.6550802139037433,0.6711229946524064,0.6711229946524064,0.732620320855615,0.732620320855615,0.7352941176470589,0.7352941176470589,1.0],\"xaxis\":\"x\",\"y\":[0.0,0.010648596321393998,0.03872216844143272,0.04065827686350436,0.0484027105517909,0.05033881897386254,0.057115198451113264,0.057115198451113264,0.060019361084220714,0.061955469506292354,0.08131655372700872,0.08325266214908035,0.11713455953533398,0.11713455953533398,0.1713455953533398,0.1713455953533398,0.22555663117134558,0.22555663117134558,0.24007744433688286,0.24007744433688286,0.2468538238141336,0.2468538238141336,0.2942884801548887,0.2942884801548887,0.33204259438528555,0.33204259438528555,0.3552758954501452,0.3552758954501452,0.4085188770571152,0.4085188770571152,0.4762826718296225,0.4762826718296225,0.48693126815101645,0.48693126815101645,0.4995159728944821,0.4995159728944821,0.5014520813165537,0.5014520813165537,0.5062923523717329,0.5062923523717329,0.5072604065827686,0.5072604065827686,0.5188770571151985,0.5188770571151985,0.5227492739593417,0.5227492739593417,0.542110358180058,0.542110358180058,0.5440464666021297,0.5440464666021297,0.547918683446273,0.547918683446273,0.558567279767667,0.558567279767667,0.5614714424007744,0.5614714424007744,0.5624394966118103,0.5624394966118103,0.5643756050338818,0.5643756050338818,0.5953533397870281,0.5953533397870281,0.5963213939980639,0.5963213939980639,0.6021297192642788,0.6021297192642788,0.6050338818973863,0.6050338818973863,0.6176185866408519,0.6176185866408519,0.6195546950629235,0.6195546950629235,0.6234269119070668,0.6234269119070668,0.6282671829622459,0.6282671829622459,0.633107454017425,0.633107454017425,0.6340755082284608,0.6340755082284608,0.6360116166505324,0.6360116166505324,0.6602129719264279,0.6602129719264279,0.6611810261374637,0.6611810261374637,0.6689254598257502,0.6689254598257502,0.6698935140367861,0.6698935140367861,0.6727976766698935,0.6727976766698935,0.6882865440464666,0.6882865440464666,0.7028073572120038,0.7028073572120038,0.7289448209099709,0.7289448209099709,0.7308809293320426,0.7308809293320426,0.7328170377541142,0.7328170377541142,0.7366892545982575,0.7366892545982575,0.7415295256534365,0.7415295256534365,0.7434656340755083,0.7434656340755083,0.7618586640851888,0.7618586640851888,0.7647628267182962,0.7647628267182962,0.7657308809293321,0.7657308809293321,0.7666989351403679,0.7666989351403679,0.7686350435624395,0.7686350435624395,0.771539206195547,0.771539206195547,0.7773475314617618,0.7773475314617618,0.7802516940948693,0.7802516940948693,0.782187802516941,0.782187802516941,0.7879961277831559,0.7879961277831559,0.7909002904162633,0.7909002904162633,0.8054211035818006,0.8054211035818006,0.8063891577928364,0.8063891577928364,0.8121974830590513,0.8121974830590513,0.814133591481123,0.814133591481123,0.8170377541142304,0.8170377541142304,0.8209099709583737,0.8209099709583737,0.8276863504356244,0.8276863504356244,0.829622458857696,0.829622458857696,0.8305905130687319,0.8305905130687319,0.8354307841239109,0.8354307841239109,0.8393030009680542,0.8393030009680542,0.8412391093901258,0.8412391093901258,0.8489835430784124,0.8489835430784124,0.8499515972894482,0.8499515972894482,0.8518877057115198,0.8518877057115198,0.8538238141335914,0.8538238141335914,0.856727976766699,0.856727976766699,0.8606001936108422,0.8606001936108422,0.861568247821878,0.861568247821878,0.8664085188770572,0.8664085188770572,0.872216844143272,0.872216844143272,0.8760890609874153,0.8760890609874153,0.8780251694094869,0.8780251694094869,0.882865440464666,0.882865440464666,0.888673765730881,0.888673765730881,0.8906098741529526,0.8906098741529526,0.8925459825750242,0.8925459825750242,0.8973862536302033,0.8973862536302033,0.9022265246853823,0.9022265246853823,0.9070667957405615,0.9070667957405615,0.9099709583736689,0.9099709583736689,0.9119070667957405,0.9119070667957405,0.9167473378509197,0.9167473378509197,0.9177153920619555,0.9177153920619555,0.9186834462729913,0.9186834462729913,0.920619554695063,0.920619554695063,0.9225556631171346,0.9225556631171346,0.9235237173281704,0.9235237173281704,0.9244917715392061,0.9244917715392061,0.9273959341723137,0.9273959341723137,0.9341723136495643,0.9341723136495643,0.9351403678606002,0.9351403678606002,0.9370764762826719,0.9370764762826719,0.9390125847047435,0.9390125847047435,0.9419167473378509,0.9419167473378509,0.9438528557599225,0.9438528557599225,0.9448209099709584,0.9448209099709584,0.9457889641819942,0.9457889641819942,0.9477250726040658,0.9477250726040658,0.9486931268151017,0.9486931268151017,0.9496611810261375,0.9496611810261375,0.9603097773475314,0.9603097773475314,0.9651500484027106,0.9651500484027106,0.9661181026137464,0.9661181026137464,0.9670861568247822,0.9670861568247822,0.9690222652468539,0.9690222652468539,0.9699903194578896,0.9699903194578896,0.9719264278799613,0.9719264278799613,0.972894482090997,0.972894482090997,0.9738625363020329,0.9738625363020329,0.9748305905130688,0.9748305905130688,0.9757986447241046,0.9757986447241046,0.9777347531461762,0.9777347531461762,0.978702807357212,0.978702807357212,0.9796708615682478,0.9796708615682478,0.9816069699903195,0.9816069699903195,0.9835430784123911,0.9835430784123911,0.9845111326234269,0.9845111326234269,0.9854791868344628,0.9854791868344628,0.9864472410454985,0.9864472410454985,0.9874152952565344,0.9874152952565344,0.989351403678606,0.989351403678606,0.9903194578896418,0.9903194578896418,0.9932236205227493,0.9932236205227493,0.9941916747337851,0.9941916747337851,0.995159728944821,0.995159728944821,0.9961277831558567,0.9961277831558567,0.9970958373668926,0.9970958373668926,0.9980638915779284,0.9980638915779284,0.9990319457889641,0.9990319457889641,1.0,1.0],\"yaxis\":\"y\",\"type\":\"scatter\"}],                        {\"template\":{\"data\":{\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"white\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"white\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"#C8D4E3\",\"linecolor\":\"#C8D4E3\",\"minorgridcolor\":\"#C8D4E3\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"#C8D4E3\",\"linecolor\":\"#C8D4E3\",\"minorgridcolor\":\"#C8D4E3\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"choropleth\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"choropleth\"}],\"contourcarpet\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"contourcarpet\"}],\"contour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"contour\"}],\"heatmapgl\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmapgl\"}],\"heatmap\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmap\"}],\"histogram2dcontour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2dcontour\"}],\"histogram2d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2d\"}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"mesh3d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"mesh3d\"}],\"parcoords\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"parcoords\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}],\"scatter3d\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatter3d\"}],\"scattercarpet\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattercarpet\"}],\"scattergeo\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergeo\"}],\"scattergl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergl\"}],\"scattermapbox\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattermapbox\"}],\"scatterpolargl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolargl\"}],\"scatterpolar\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolar\"}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"scatterternary\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterternary\"}],\"surface\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"surface\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}]},\"layout\":{\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"autotypenumbers\":\"strict\",\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]],\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]},\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"geo\":{\"bgcolor\":\"white\",\"lakecolor\":\"white\",\"landcolor\":\"white\",\"showlakes\":true,\"showland\":true,\"subunitcolor\":\"#C8D4E3\"},\"hoverlabel\":{\"align\":\"left\"},\"hovermode\":\"closest\",\"mapbox\":{\"style\":\"light\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"white\",\"polar\":{\"angularaxis\":{\"gridcolor\":\"#EBF0F8\",\"linecolor\":\"#EBF0F8\",\"ticks\":\"\"},\"bgcolor\":\"white\",\"radialaxis\":{\"gridcolor\":\"#EBF0F8\",\"linecolor\":\"#EBF0F8\",\"ticks\":\"\"}},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"white\",\"gridcolor\":\"#DFE8F3\",\"gridwidth\":2,\"linecolor\":\"#EBF0F8\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"#EBF0F8\"},\"yaxis\":{\"backgroundcolor\":\"white\",\"gridcolor\":\"#DFE8F3\",\"gridwidth\":2,\"linecolor\":\"#EBF0F8\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"#EBF0F8\"},\"zaxis\":{\"backgroundcolor\":\"white\",\"gridcolor\":\"#DFE8F3\",\"gridwidth\":2,\"linecolor\":\"#EBF0F8\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"#EBF0F8\"}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"ternary\":{\"aaxis\":{\"gridcolor\":\"#DFE8F3\",\"linecolor\":\"#A2B1C6\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"#DFE8F3\",\"linecolor\":\"#A2B1C6\",\"ticks\":\"\"},\"bgcolor\":\"white\",\"caxis\":{\"gridcolor\":\"#DFE8F3\",\"linecolor\":\"#A2B1C6\",\"ticks\":\"\"}},\"title\":{\"x\":0.05},\"xaxis\":{\"automargin\":true,\"gridcolor\":\"#EBF0F8\",\"linecolor\":\"#EBF0F8\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"#EBF0F8\",\"zerolinewidth\":2},\"yaxis\":{\"automargin\":true,\"gridcolor\":\"#EBF0F8\",\"linecolor\":\"#EBF0F8\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"#EBF0F8\",\"zerolinewidth\":2}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"False Positive Rate\"},\"constrain\":\"domain\"},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"True Positive Rate\"},\"scaleanchor\":\"x\",\"scaleratio\":1},\"legend\":{\"tracegroupgap\":0},\"title\":{\"text\":\"ROC Curve (AUC=0.8994)\"},\"height\":600,\"width\":800,\"shapes\":[{\"line\":{\"dash\":\"dash\"},\"type\":\"line\",\"x0\":0,\"x1\":1,\"y0\":0,\"y1\":1}],\"showlegend\":false},                        {\"responsive\": true}                    ).then(function(){\n","                            \n","var gd = document.getElementById('db9d5428-9f9e-4b1d-b1c5-080a85f86207');\n","var x = new MutationObserver(function (mutations, observer) {{\n","        var display = window.getComputedStyle(gd).display;\n","        if (!display || display === 'none') {{\n","            console.log([gd, 'removed!']);\n","            Plotly.purge(gd);\n","            observer.disconnect();\n","        }}\n","}});\n","\n","// Listen for the removal of the full notebook cells\n","var notebookContainer = gd.closest('#notebook-container');\n","if (notebookContainer) {{\n","    x.observe(notebookContainer, {childList: true});\n","}}\n","\n","// Listen for the clearing of the current output cell\n","var outputEl = gd.closest('.output');\n","if (outputEl) {{\n","    x.observe(outputEl, {childList: true});\n","}}\n","\n","                        })                };                });            </script>        </div>"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.plotly.v1+json":{"config":{"plotlyServerURL":"https://plot.ly"},"data":[{"fillpattern":{"shape":""},"hovertemplate":"Recall=%{x}<br>Precision=%{y}<extra></extra>","legendgroup":"","line":{"color":"#636efa"},"marker":{"symbol":"circle"},"mode":"lines","name":"","orientation":"v","showlegend":false,"stackgroup":"1","type":"scatter","x":[1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,0.9990319457889641,0.9990319457889641,0.9980638915779284,0.9980638915779284,0.9980638915779284,0.9980638915779284,0.9980638915779284,0.9980638915779284,0.9980638915779284,0.9980638915779284,0.9980638915779284,0.9980638915779284,0.9980638915779284,0.9980638915779284,0.9980638915779284,0.9980638915779284,0.9980638915779284,0.9980638915779284,0.9980638915779284,0.9980638915779284,0.9980638915779284,0.9980638915779284,0.9980638915779284,0.9980638915779284,0.9980638915779284,0.9980638915779284,0.9970958373668926,0.9970958373668926,0.9970958373668926,0.9970958373668926,0.9970958373668926,0.9970958373668926,0.9970958373668926,0.9961277831558567,0.9961277831558567,0.9961277831558567,0.9961277831558567,0.9961277831558567,0.995159728944821,0.995159728944821,0.995159728944821,0.995159728944821,0.995159728944821,0.995159728944821,0.995159728944821,0.995159728944821,0.995159728944821,0.995159728944821,0.995159728944821,0.9941916747337851,0.9941916747337851,0.9941916747337851,0.9941916747337851,0.9941916747337851,0.9941916747337851,0.9941916747337851,0.9941916747337851,0.9941916747337851,0.9941916747337851,0.9941916747337851,0.9941916747337851,0.9941916747337851,0.9932236205227493,0.9932236205227493,0.9932236205227493,0.9922555663117134,0.9912875121006777,0.9903194578896418,0.9903194578896418,0.9903194578896418,0.9903194578896418,0.9903194578896418,0.989351403678606,0.989351403678606,0.989351403678606,0.989351403678606,0.989351403678606,0.989351403678606,0.989351403678606,0.989351403678606,0.989351403678606,0.9883833494675702,0.9874152952565344,0.9874152952565344,0.9864472410454985,0.9864472410454985,0.9854791868344628,0.9854791868344628,0.9854791868344628,0.9854791868344628,0.9854791868344628,0.9854791868344628,0.9854791868344628,0.9854791868344628,0.9854791868344628,0.9845111326234269,0.9845111326234269,0.9845111326234269,0.9835430784123911,0.9835430784123911,0.9835430784123911,0.9825750242013552,0.9816069699903195,0.9816069699903195,0.9816069699903195,0.9816069699903195,0.9816069699903195,0.9806389157792836,0.9796708615682478,0.9796708615682478,0.978702807357212,0.978702807357212,0.978702807357212,0.9777347531461762,0.9777347531461762,0.9777347531461762,0.9767666989351403,0.9757986447241046,0.9757986447241046,0.9757986447241046,0.9748305905130688,0.9748305905130688,0.9738625363020329,0.9738625363020329,0.9738625363020329,0.9738625363020329,0.9738625363020329,0.972894482090997,0.972894482090997,0.972894482090997,0.9719264278799613,0.9719264278799613,0.9709583736689255,0.9699903194578896,0.9699903194578896,0.9699903194578896,0.9699903194578896,0.9699903194578896,0.9690222652468539,0.9690222652468539,0.9690222652468539,0.968054211035818,0.9670861568247822,0.9670861568247822,0.9670861568247822,0.9661181026137464,0.9661181026137464,0.9661181026137464,0.9661181026137464,0.9661181026137464,0.9661181026137464,0.9661181026137464,0.9661181026137464,0.9661181026137464,0.9651500484027106,0.9651500484027106,0.9651500484027106,0.9651500484027106,0.9651500484027106,0.9651500484027106,0.9651500484027106,0.9641819941916747,0.9632139399806389,0.9622458857696031,0.9612778315585673,0.9603097773475314,0.9603097773475314,0.9593417231364957,0.9583736689254598,0.957405614714424,0.9564375605033882,0.9554695062923524,0.9545014520813165,0.9535333978702807,0.952565343659245,0.9515972894482091,0.9506292352371732,0.9496611810261375,0.9496611810261375,0.9496611810261375,0.9496611810261375,0.9486931268151017,0.9486931268151017,0.9477250726040658,0.9477250726040658,0.9467570183930301,0.9457889641819942,0.9457889641819942,0.9448209099709584,0.9448209099709584,0.9448209099709584,0.9448209099709584,0.9438528557599225,0.9438528557599225,0.9428848015488868,0.9419167473378509,0.9419167473378509,0.9409486931268151,0.9399806389157793,0.9390125847047435,0.9390125847047435,0.9390125847047435,0.9390125847047435,0.9390125847047435,0.9390125847047435,0.9380445304937076,0.9370764762826719,0.9370764762826719,0.936108422071636,0.9351403678606002,0.9351403678606002,0.9351403678606002,0.9341723136495643,0.9341723136495643,0.9332042594385286,0.9322362052274927,0.9312681510164569,0.9303000968054211,0.9293320425943853,0.9283639883833494,0.9273959341723137,0.9273959341723137,0.9264278799612778,0.925459825750242,0.9244917715392061,0.9244917715392061,0.9244917715392061,0.9235237173281704,0.9235237173281704,0.9235237173281704,0.9225556631171346,0.9225556631171346,0.9215876089060987,0.920619554695063,0.920619554695063,0.920619554695063,0.920619554695063,0.920619554695063,0.9196515004840271,0.9186834462729913,0.9186834462729913,0.9177153920619555,0.9177153920619555,0.9167473378509197,0.9167473378509197,0.9167473378509197,0.9157792836398838,0.914811229428848,0.9138431752178122,0.9128751210067764,0.9119070667957405,0.9119070667957405,0.9119070667957405,0.9119070667957405,0.9109390125847048,0.9099709583736689,0.9099709583736689,0.9090029041626331,0.9080348499515973,0.9070667957405615,0.9070667957405615,0.9070667957405615,0.9060987415295256,0.9051306873184899,0.904162633107454,0.9031945788964182,0.9022265246853823,0.9022265246853823,0.9022265246853823,0.9022265246853823,0.9022265246853823,0.9022265246853823,0.9012584704743466,0.9002904162633107,0.8993223620522749,0.8983543078412392,0.8973862536302033,0.8973862536302033,0.8973862536302033,0.8973862536302033,0.8964181994191674,0.8954501452081317,0.8944820909970959,0.89351403678606,0.8925459825750242,0.8925459825750242,0.8915779283639884,0.8906098741529526,0.8906098741529526,0.8896418199419167,0.888673765730881,0.888673765730881,0.8877057115198451,0.8867376573088093,0.8857696030977735,0.8848015488867377,0.8838334946757018,0.882865440464666,0.882865440464666,0.8818973862536302,0.8809293320425944,0.8799612778315585,0.8789932236205228,0.8780251694094869,0.8780251694094869,0.8770571151984511,0.8760890609874153,0.8760890609874153,0.8751210067763795,0.8741529525653436,0.8731848983543078,0.872216844143272,0.872216844143272,0.8712487899322362,0.8702807357212003,0.8693126815101646,0.8683446272991288,0.8673765730880929,0.8664085188770572,0.8664085188770572,0.8664085188770572,0.8654404646660213,0.8644724104549855,0.8635043562439496,0.8625363020329139,0.861568247821878,0.861568247821878,0.8606001936108422,0.8606001936108422,0.8596321393998064,0.8586640851887706,0.8576960309777347,0.856727976766699,0.856727976766699,0.8557599225556631,0.8547918683446273,0.8538238141335914,0.8538238141335914,0.8528557599225557,0.8518877057115198,0.8518877057115198,0.850919651500484,0.8499515972894482,0.8499515972894482,0.8499515972894482,0.8499515972894482,0.8489835430784124,0.8489835430784124,0.8480154888673765,0.8470474346563408,0.846079380445305,0.8451113262342691,0.8441432720232332,0.8431752178121975,0.8422071636011617,0.8412391093901258,0.8412391093901258,0.8402710551790901,0.8393030009680542,0.8393030009680542,0.8383349467570184,0.8373668925459826,0.8363988383349468,0.8354307841239109,0.8354307841239109,0.8344627299128751,0.8334946757018393,0.8325266214908035,0.8315585672797676,0.8305905130687319,0.8305905130687319,0.829622458857696,0.829622458857696,0.8286544046466602,0.8276863504356244,0.8276863504356244,0.8276863504356244,0.8267182962245886,0.8257502420135527,0.8247821878025169,0.8238141335914811,0.8228460793804453,0.8218780251694094,0.8209099709583737,0.8209099709583737,0.8209099709583737,0.8209099709583737,0.8199419167473379,0.818973862536302,0.8180058083252663,0.8170377541142304,0.8170377541142304,0.8170377541142304,0.8160696999031946,0.8151016456921588,0.814133591481123,0.814133591481123,0.8131655372700871,0.8121974830590513,0.8121974830590513,0.8112294288480155,0.8102613746369797,0.8092933204259438,0.8083252662149081,0.8073572120038722,0.8063891577928364,0.8063891577928364,0.8054211035818006,0.8054211035818006,0.8044530493707648,0.8034849951597289,0.8025169409486931,0.8015488867376573,0.8005808325266215,0.7996127783155856,0.7986447241045499,0.797676669893514,0.7967086156824782,0.7957405614714425,0.7947725072604066,0.7938044530493708,0.7928363988383349,0.7918683446272992,0.7909002904162633,0.7909002904162633,0.7909002904162633,0.7899322362052275,0.7889641819941917,0.7879961277831559,0.7879961277831559,0.78702807357212,0.7860600193610843,0.7850919651500484,0.7841239109390126,0.7831558567279767,0.782187802516941,0.782187802516941,0.7812197483059051,0.7802516940948693,0.7802516940948693,0.7802516940948693,0.7792836398838335,0.7783155856727977,0.7773475314617618,0.7773475314617618,0.7773475314617618,0.7773475314617618,0.7763794772507261,0.7754114230396902,0.7744433688286544,0.7734753146176185,0.7725072604065828,0.771539206195547,0.771539206195547,0.7705711519845111,0.7696030977734754,0.7686350435624395,0.7686350435624395,0.7676669893514037,0.7666989351403679,0.7666989351403679,0.7657308809293321,0.7657308809293321,0.7647628267182962,0.7647628267182962,0.7647628267182962,0.7637947725072604,0.7628267182962246,0.7618586640851888,0.7618586640851888,0.7608906098741529,0.7599225556631172,0.7589545014520813,0.7579864472410455,0.7570183930300097,0.7560503388189739,0.755082284607938,0.7541142303969022,0.7531461761858664,0.7521781219748306,0.7512100677637947,0.750242013552759,0.7492739593417231,0.7483059051306873,0.7473378509196515,0.7463697967086157,0.7454017424975798,0.744433688286544,0.7434656340755083,0.7434656340755083,0.7424975798644724,0.7415295256534365,0.7415295256534365,0.7405614714424008,0.739593417231365,0.7386253630203291,0.7376573088092934,0.7366892545982575,0.7366892545982575,0.7366892545982575,0.7357212003872217,0.7347531461761858,0.7337850919651501,0.7328170377541142,0.7328170377541142,0.7318489835430784,0.7308809293320426,0.7308809293320426,0.7299128751210068,0.7289448209099709,0.7289448209099709,0.7279767666989352,0.7270087124878993,0.7260406582768635,0.7250726040658277,0.7241045498547919,0.723136495643756,0.7221684414327202,0.7212003872216844,0.7202323330106486,0.7192642787996127,0.718296224588577,0.7173281703775412,0.7163601161665053,0.7153920619554696,0.7144240077444337,0.7134559535333979,0.712487899322362,0.7115198451113263,0.7105517909002904,0.7095837366892546,0.7086156824782188,0.707647628267183,0.7066795740561471,0.7057115198451114,0.7047434656340755,0.7037754114230397,0.7028073572120038,0.7028073572120038,0.7018393030009681,0.7008712487899322,0.6999031945788964,0.6989351403678606,0.6979670861568248,0.6969990319457889,0.6960309777347532,0.6950629235237173,0.6940948693126815,0.6931268151016456,0.6921587608906099,0.691190706679574,0.6902226524685382,0.6892545982575025,0.6882865440464666,0.6882865440464666,0.6873184898354308,0.686350435624395,0.6853823814133592,0.6844143272023233,0.6834462729912875,0.6824782187802517,0.6815101645692159,0.68054211035818,0.6795740561471443,0.6786060019361084,0.6776379477250726,0.6766698935140368,0.675701839303001,0.6747337850919651,0.6737657308809293,0.6727976766698935,0.6727976766698935,0.6718296224588577,0.6708615682478218,0.6698935140367861,0.6698935140367861,0.6689254598257502,0.6689254598257502,0.6679574056147144,0.6669893514036787,0.6660212971926428,0.665053242981607,0.6640851887705711,0.6631171345595354,0.6621490803484995,0.6611810261374637,0.6611810261374637,0.6602129719264279,0.6602129719264279,0.6592449177153921,0.6582768635043562,0.6573088092933205,0.6563407550822846,0.6553727008712488,0.6544046466602129,0.6534365924491772,0.6524685382381413,0.6515004840271055,0.6505324298160697,0.6495643756050339,0.648596321393998,0.6476282671829623,0.6466602129719264,0.6456921587608906,0.6447241045498547,0.643756050338819,0.6427879961277831,0.6418199419167473,0.6408518877057116,0.6398838334946757,0.6389157792836399,0.6379477250726041,0.6369796708615683,0.6360116166505324,0.6360116166505324,0.6350435624394967,0.6340755082284608,0.6340755082284608,0.633107454017425,0.633107454017425,0.6321393998063891,0.6311713455953534,0.6302032913843175,0.6292352371732817,0.6282671829622459,0.6282671829622459,0.6272991287512101,0.6263310745401742,0.6253630203291385,0.6243949661181026,0.6234269119070668,0.6234269119070668,0.6224588576960309,0.6214908034849952,0.6205227492739593,0.6195546950629235,0.6195546950629235,0.6185866408518877,0.6176185866408519,0.6176185866408519,0.616650532429816,0.6156824782187803,0.6147144240077445,0.6137463697967086,0.6127783155856728,0.611810261374637,0.6108422071636012,0.6098741529525653,0.6089060987415296,0.6079380445304937,0.6069699903194579,0.6060019361084221,0.6050338818973863,0.6050338818973863,0.6040658276863504,0.6030977734753146,0.6021297192642788,0.6021297192642788,0.601161665053243,0.6001936108422071,0.5992255566311714,0.5982575024201355,0.5972894482090997,0.5963213939980639,0.5963213939980639,0.5953533397870281,0.5953533397870281,0.5943852855759922,0.5934172313649564,0.5924491771539206,0.5914811229428848,0.590513068731849,0.5895450145208132,0.5885769603097774,0.5876089060987415,0.5866408518877058,0.5856727976766699,0.5847047434656341,0.5837366892545982,0.5827686350435625,0.5818005808325266,0.5808325266214908,0.579864472410455,0.5788964181994192,0.5779283639883833,0.5769603097773476,0.5759922555663117,0.5750242013552759,0.57405614714424,0.5730880929332043,0.5721200387221684,0.5711519845111326,0.5701839303000968,0.569215876089061,0.5682478218780251,0.5672797676669894,0.5663117134559535,0.5653436592449177,0.5643756050338818,0.5643756050338818,0.5634075508228461,0.5624394966118103,0.5624394966118103,0.5614714424007744,0.5614714424007744,0.5605033881897387,0.5595353339787028,0.558567279767667,0.558567279767667,0.5575992255566312,0.5566311713455954,0.5556631171345595,0.5546950629235237,0.5537270087124879,0.5527589545014521,0.5517909002904162,0.5508228460793805,0.5498547918683446,0.5488867376573088,0.547918683446273,0.547918683446273,0.5469506292352372,0.5459825750242013,0.5450145208131656,0.5440464666021297,0.5440464666021297,0.5440464666021297,0.5430784123910939,0.542110358180058,0.542110358180058,0.5411423039690223,0.5401742497579864,0.5392061955469506,0.5382381413359149,0.537270087124879,0.5363020329138432,0.5353339787028074,0.5343659244917716,0.5333978702807357,0.5324298160696999,0.5314617618586641,0.5304937076476283,0.5295256534365924,0.5285575992255567,0.5275895450145208,0.526621490803485,0.5256534365924492,0.5246853823814134,0.5237173281703775,0.5227492739593417,0.5227492739593417,0.5217812197483059,0.5208131655372701,0.5198451113262342,0.5188770571151985,0.5188770571151985,0.5179090029041626,0.5169409486931268,0.515972894482091,0.5150048402710552,0.5140367860600193,0.5130687318489835,0.5121006776379478,0.5111326234269119,0.510164569215876,0.5091965150048403,0.5082284607938045,0.5072604065827686,0.5072604065827686,0.5062923523717329,0.5062923523717329,0.505324298160697,0.5043562439496612,0.5033881897386253,0.5024201355275896,0.5014520813165537,0.5014520813165537,0.5004840271055179,0.4995159728944821,0.4995159728944821,0.4985479186834463,0.4975798644724105,0.49661181026137463,0.49564375605033884,0.494675701839303,0.4937076476282672,0.4927395934172314,0.49177153920619554,0.49080348499515974,0.4898354307841239,0.4888673765730881,0.4878993223620523,0.48693126815101645,0.48693126815101645,0.48596321393998065,0.4849951597289448,0.484027105517909,0.4830590513068732,0.48209099709583736,0.48112294288480156,0.4801548886737657,0.4791868344627299,0.4782187802516941,0.47725072604065827,0.4762826718296225,0.4762826718296225,0.4753146176185866,0.4743465634075508,0.47337850919651503,0.4724104549854792,0.4714424007744434,0.47047434656340753,0.46950629235237173,0.46853823814133594,0.4675701839303001,0.4666021297192643,0.46563407550822844,0.46466602129719264,0.46369796708615685,0.462729912875121,0.4617618586640852,0.46079380445304935,0.45982575024201355,0.45885769603097776,0.4578896418199419,0.4569215876089061,0.45595353339787026,0.45498547918683446,0.45401742497579867,0.4530493707647628,0.452081316553727,0.45111326234269117,0.45014520813165537,0.4491771539206196,0.4482090997095837,0.44724104549854793,0.4462729912875121,0.4453049370764763,0.4443368828654405,0.44336882865440463,0.44240077444336884,0.441432720232333,0.4404646660212972,0.4394966118102614,0.43852855759922554,0.43756050338818975,0.4365924491771539,0.4356243949661181,0.4346563407550823,0.43368828654404645,0.43272023233301066,0.4317521781219748,0.430784123910939,0.4298160696999032,0.42884801548886736,0.42787996127783157,0.4269119070667957,0.4259438528557599,0.4249757986447241,0.42400774443368827,0.4230396902226525,0.4220716360116166,0.42110358180058083,0.42013552758954503,0.4191674733785092,0.4181994191674734,0.41723136495643753,0.41626331074540174,0.41529525653436594,0.4143272023233301,0.4133591481122943,0.41239109390125844,0.41142303969022265,0.41045498547918685,0.409486931268151,0.4085188770571152,0.4085188770571152,0.4075508228460794,0.40658276863504356,0.40561471442400776,0.4046466602129719,0.4036786060019361,0.4027105517909003,0.40174249757986447,0.40077444336882867,0.3998063891577928,0.398838334946757,0.3978702807357212,0.3969022265246854,0.3959341723136496,0.39496611810261373,0.39399806389157793,0.39303000968054214,0.3920619554695063,0.3910939012584705,0.39012584704743464,0.38915779283639884,0.38818973862536305,0.3872216844143272,0.3862536302032914,0.38528557599225555,0.38431752178121975,0.38334946757018395,0.3823814133591481,0.3814133591481123,0.38044530493707646,0.37947725072604066,0.37850919651500486,0.377541142303969,0.3765730880929332,0.37560503388189737,0.37463697967086157,0.3736689254598258,0.3727008712487899,0.3717328170377541,0.3707647628267183,0.3697967086156825,0.3688286544046467,0.36786060019361083,0.36689254598257504,0.3659244917715392,0.3649564375605034,0.3639883833494676,0.36302032913843174,0.36205227492739595,0.3610842207163601,0.3601161665053243,0.3591481122942885,0.35818005808325265,0.35721200387221685,0.356243949661181,0.3552758954501452,0.3552758954501452,0.3543078412391094,0.35333978702807356,0.35237173281703776,0.3514036786060019,0.3504356243949661,0.3494675701839303,0.34849951597289447,0.3475314617618587,0.3465634075508228,0.345595353339787,0.34462729912875123,0.3436592449177154,0.3426911907066796,0.34172313649564373,0.34075508228460794,0.33978702807357214,0.3388189738625363,0.3378509196515005,0.33688286544046464,0.33591481122942884,0.33494675701839305,0.3339787028073572,0.3330106485963214,0.33204259438528555,0.33204259438528555,0.33107454017424975,0.33010648596321396,0.3291384317521781,0.3281703775411423,0.32720232333010646,0.32623426911907066,0.32526621490803487,0.324298160696999,0.3233301064859632,0.32236205227492737,0.3213939980638916,0.3204259438528558,0.3194578896418199,0.31848983543078413,0.31752178121974833,0.3165537270087125,0.3155856727976767,0.31461761858664083,0.31364956437560504,0.31268151016456924,0.3117134559535334,0.3107454017424976,0.30977734753146174,0.30880929332042595,0.30784123910939015,0.3068731848983543,0.3059051306873185,0.30493707647628265,0.30396902226524686,0.30300096805421106,0.3020329138431752,0.3010648596321394,0.30009680542110356,0.29912875121006777,0.29816069699903197,0.2971926427879961,0.2962245885769603,0.2952565343659245,0.2942884801548887,0.2942884801548887,0.2933204259438529,0.29235237173281703,0.29138431752178123,0.2904162633107454,0.2894482090997096,0.2884801548886738,0.28751210067763794,0.28654404646660214,0.2855759922555663,0.2846079380445305,0.2836398838334947,0.28267182962245885,0.28170377541142305,0.2807357212003872,0.2797676669893514,0.2787996127783156,0.27783155856727976,0.27686350435624396,0.2758954501452081,0.2749273959341723,0.2739593417231365,0.27299128751210067,0.27202323330106487,0.271055179090029,0.2700871248789932,0.2691190706679574,0.2681510164569216,0.2671829622458858,0.26621490803484993,0.26524685382381413,0.26427879961277834,0.2633107454017425,0.2623426911907067,0.26137463697967084,0.26040658276863504,0.25943852855759925,0.2584704743465634,0.2575024201355276,0.25653436592449175,0.25556631171345595,0.25459825750242016,0.2536302032913843,0.2526621490803485,0.25169409486931266,0.25072604065827686,0.24975798644724104,0.24878993223620524,0.24782187802516942,0.2468538238141336,0.2468538238141336,0.24588576960309777,0.24491771539206195,0.24394966118102615,0.24298160696999033,0.2420135527589545,0.24104549854791868,0.24007744433688286,0.24007744433688286,0.23910939012584706,0.23814133591481124,0.2371732817037754,0.2362052274927396,0.23523717328170377,0.23426911907066797,0.23330106485963215,0.23233301064859632,0.2313649564375605,0.23039690222652467,0.22942884801548888,0.22846079380445306,0.22749273959341723,0.2265246853823814,0.22555663117134558,0.22555663117134558,0.2245885769603098,0.22362052274927396,0.22265246853823814,0.22168441432720232,0.2207163601161665,0.2197483059051307,0.21878025169409487,0.21781219748305905,0.21684414327202323,0.2158760890609874,0.2149080348499516,0.21393998063891578,0.21297192642787996,0.21200387221684414,0.2110358180058083,0.21006776379477252,0.2090997095837367,0.20813165537270087,0.20716360116166505,0.20619554695062922,0.20522749273959343,0.2042594385285576,0.20329138431752178,0.20232333010648595,0.20135527589545016,0.20038722168441434,0.1994191674733785,0.1984511132623427,0.19748305905130686,0.19651500484027107,0.19554695062923524,0.19457889641819942,0.1936108422071636,0.19264278799612777,0.19167473378509198,0.19070667957405615,0.18973862536302033,0.1887705711519845,0.18780251694094868,0.1868344627299129,0.18586640851887706,0.18489835430784124,0.18393030009680542,0.1829622458857696,0.1819941916747338,0.18102613746369797,0.18005808325266215,0.17909002904162633,0.1781219748305905,0.1771539206195547,0.17618586640851888,0.17521781219748306,0.17424975798644723,0.1732817037754114,0.17231364956437561,0.1713455953533398,0.1713455953533398,0.17037754114230397,0.16940948693126814,0.16844143272023232,0.16747337850919652,0.1665053242981607,0.16553727008712488,0.16456921587608905,0.16360116166505323,0.16263310745401743,0.1616650532429816,0.1606969990319458,0.15972894482090996,0.15876089060987417,0.15779283639883834,0.15682478218780252,0.1558567279767667,0.15488867376573087,0.15392061955469508,0.15295256534365925,0.15198451113262343,0.1510164569215876,0.15004840271055178,0.14908034849951599,0.14811229428848016,0.14714424007744434,0.14617618586640851,0.1452081316553727,0.1442400774443369,0.14327202323330107,0.14230396902226525,0.14133591481122942,0.1403678606001936,0.1393998063891578,0.13843175217812198,0.13746369796708616,0.13649564375605033,0.1355275895450145,0.1345595353339787,0.1335914811229429,0.13262342691190707,0.13165537270087124,0.13068731848983542,0.12971926427879962,0.1287512100677638,0.12778315585672798,0.12681510164569215,0.12584704743465633,0.12487899322362052,0.12391093901258471,0.12294288480154889,0.12197483059051308,0.12100677637947725,0.12003872216844143,0.11907066795740562,0.1181026137463698,0.11713455953533398,0.11713455953533398,0.11616650532429816,0.11519845111326234,0.11423039690222653,0.1132623426911907,0.1122942884801549,0.11132623426911907,0.11035818005808325,0.10939012584704744,0.10842207163601161,0.1074540174249758,0.10648596321393998,0.10551790900290416,0.10454985479186835,0.10358180058083252,0.10261374636979671,0.10164569215876089,0.10067763794772508,0.09970958373668926,0.09874152952565343,0.09777347531461762,0.0968054211035818,0.09583736689254599,0.09486931268151017,0.09390125847047434,0.09293320425943853,0.09196515004840271,0.0909970958373669,0.09002904162633107,0.08906098741529525,0.08809293320425944,0.08712487899322362,0.08615682478218781,0.08518877057115198,0.08422071636011616,0.08325266214908035,0.08131655372700872,0.0803484995159729,0.07938044530493708,0.07841239109390126,0.07744433688286544,0.07647628267182963,0.0755082284607938,0.07454017424975799,0.07357212003872217,0.07260406582768635,0.07163601161665054,0.07066795740561471,0.0696999031945789,0.06873184898354308,0.06776379477250725,0.06679574056147145,0.06582768635043562,0.06485963213939981,0.06389157792836399,0.06292352371732816,0.061955469506292354,0.060019361084220714,0.0590513068731849,0.05808325266214908,0.057115198451113264,0.057115198451113264,0.05614714424007745,0.05517909002904162,0.05421103581800581,0.05324298160696999,0.05227492739593417,0.051306873184898356,0.05033881897386254,0.0484027105517909,0.04743465634075508,0.046466602129719266,0.04549854791868345,0.044530493707647625,0.04356243949661181,0.04259438528557599,0.041626331074540175,0.04065827686350436,0.03872216844143272,0.0377541142303969,0.036786060019361085,0.03581800580832527,0.03484995159728945,0.03388189738625363,0.03291384317521781,0.031945788964181994,0.030977734753146177,0.030009680542110357,0.02904162633107454,0.028073572120038724,0.027105517909002903,0.026137463697967087,0.02516940948693127,0.02420135527589545,0.023233301064859633,0.022265246853823813,0.021297192642787996,0.02032913843175218,0.01936108422071636,0.018393030009680542,0.017424975798644726,0.016456921587608905,0.015488867376573089,0.01452081316553727,0.013552758954501452,0.012584704743465635,0.011616650532429816,0.010648596321393998,0],"xaxis":"x","y":[0.7341862117981521,0.7347083926031295,0.7352313167259786,0.7357549857549858,0.7362794012829651,0.7368045649072753,0.7373304782298359,0.7378571428571429,0.7383845604002859,0.7389127324749643,0.7394416607015032,0.7399713467048711,0.7405017921146954,0.7410329985652798,0.741564967695621,0.7420977011494253,0.7426312005751258,0.7431654676258993,0.7437005039596832,0.7442363112391931,0.7447728911319395,0.7453102453102453,0.7458483754512636,0.7463872832369942,0.7469269703543022,0.7474674384949349,0.7480086893555394,0.7485507246376811,0.7490935460478607,0.7496371552975326,0.7501815541031227,0.7507267441860465,0.7512727272727273,0.7518195050946143,0.752367079388201,0.7529154518950437,0.7534646243617797,0.754014598540146,0.7545653761869978,0.7551169590643275,0.7556693489392831,0.7562225475841874,0.7567765567765568,0.7573313782991202,0.7578870139398386,0.7584434654919237,0.7590007347538574,0.7595588235294117,0.7601177336276674,0.7606774668630338,0.7612380250552689,0.7617994100294986,0.7623616236162362,0.7629246676514032,0.7634885439763488,0.7640532544378699,0.764618800888231,0.7651851851851852,0.765752409191994,0.766320474777448,0.7668893838158871,0.7674591381872214,0.7680297397769517,0.7686011904761905,0.7691734921816828,0.7697466467958272,0.7703206562266965,0.7708955223880597,0.7714712471994025,0.7720478325859492,0.7726252804786836,0.7732035928143712,0.7737827715355805,0.7743628185907047,0.7749437359339835,0.7755255255255256,0.7761081893313299,0.7766917293233083,0.7772761474793077,0.7778614457831325,0.7784476262245666,0.7790346907993967,0.779622641509434,0.7802114803625377,0.780801209372638,0.7813918305597579,0.7819833459500378,0.7825757575757576,0.7831690674753601,0.783763277693475,0.7843583902809416,0.7849544072948328,0.785551330798479,0.7861491628614916,0.7867479055597868,0.7873475609756098,0.7879481311975591,0.7885496183206107,0.7891520244461421,0.7897553516819572,0.7895944912012242,0.7901990811638591,0.7900383141762453,0.7906441717791411,0.7912509593246354,0.7918586789554531,0.792467332820907,0.7930769230769231,0.7936874518860662,0.7942989214175655,0.79491133384734,0.7955246913580247,0.7961389961389962,0.7967542503863988,0.7973704563031709,0.7979876160990712,0.7986057319907048,0.7992248062015503,0.799844840961986,0.8004658385093167,0.8010878010878011,0.8017107309486781,0.8023346303501946,0.8029595015576324,0.8035853468433359,0.8042121684867395,0.8040593286494926,0.8046875,0.8053166536356529,0.8059467918622848,0.8065779169929522,0.8072100313479624,0.807843137254902,0.8076923076923077,0.8083267871170463,0.8089622641509434,0.8095987411487018,0.8102362204724409,0.8100866824271079,0.8107255520504731,0.8113654301499605,0.8120063191153238,0.8126482213438735,0.8132911392405063,0.8139350752177356,0.8145800316957211,0.8152260111022998,0.8158730158730159,0.8165210484511517,0.8163751987281399,0.817024661893397,0.8176751592356688,0.8183266932270916,0.8189792663476874,0.819632881085395,0.8202875399361023,0.820943245403677,0.8216,0.822257806244996,0.8229166666666666,0.8235765838011226,0.8242375601926164,0.8240963855421687,0.8247588424437299,0.825422365245374,0.8252818035426731,0.8251410153102336,0.825,0.8256658595641646,0.8263327948303716,0.8270008084074374,0.8276699029126213,0.8275303643724696,0.8282009724473258,0.8288726682887266,0.8295454545454546,0.8302193338748984,0.8308943089430895,0.8315703824247356,0.8322475570032574,0.8329258353708231,0.83278955954323,0.8326530612244898,0.8333333333333334,0.8331970564186427,0.8338788870703764,0.8337428337428338,0.8344262295081967,0.8351107465135357,0.8357963875205254,0.8364831552999178,0.837171052631579,0.8378600823045268,0.8385502471169687,0.8392415498763397,0.8391089108910891,0.8398018166804294,0.8404958677685951,0.8403639371381307,0.8410596026490066,0.8417564208782105,0.8416252072968491,0.84149377593361,0.8421926910299004,0.8428927680798005,0.8435940099833611,0.8442964196502915,0.8441666666666666,0.8440366972477065,0.8447412353923205,0.8446115288220551,0.8453177257525084,0.8460251046025105,0.8458961474036851,0.8466051969823973,0.8473154362416108,0.8471872376154492,0.8470588235294118,0.847771236333053,0.8484848484848485,0.8483572030328559,0.8490725126475548,0.8489451476793249,0.8496621621621622,0.8503803888419273,0.8510998307952623,0.8518204911092294,0.8516949152542372,0.8524173027989822,0.8531409168081494,0.8530161427357689,0.8537414965986394,0.8536170212765958,0.8534923339011925,0.8542199488491049,0.8549488054607508,0.8556789069171649,0.8564102564102564,0.8562874251497006,0.8570205479452054,0.8577549271636675,0.8576329331046312,0.8575107296137339,0.8582474226804123,0.8589853826311264,0.8588640275387264,0.859603789836348,0.8603448275862069,0.8610871440897325,0.8618307426597582,0.8625756266205704,0.8633217993079585,0.8640692640692641,0.8648180242634316,0.8647007805724197,0.8654513888888888,0.8662033014769766,0.8669565217391304,0.8677110530896431,0.8684668989547039,0.8692240627724499,0.8691099476439791,0.868995633187773,0.8688811188811189,0.868766404199475,0.8686514886164624,0.8694127957931639,0.8692982456140351,0.8691834942932397,0.8690685413005272,0.8689533861037819,0.8688380281690141,0.8687224669603524,0.8686067019400353,0.8684907325684025,0.8683745583038869,0.8682581786030061,0.868141592920354,0.8689105403011514,0.8696808510638298,0.870452528837622,0.8703374777975134,0.8711111111111111,0.8709964412811388,0.8717720391807658,0.8716577540106952,0.8715432649420161,0.8723214285714286,0.872207327971403,0.8729874776386404,0.873769024171889,0.8745519713261649,0.874439461883408,0.8752244165170556,0.8751123090745733,0.875,0.8757875787578758,0.8756756756756757,0.8755635707844905,0.8754512635379061,0.8762420957542909,0.8770343580470162,0.8778280542986425,0.8786231884057971,0.8794197642792384,0.8793103448275862,0.8792007266121707,0.88,0.8798908098271155,0.8797814207650273,0.8805834092980857,0.8813868613138686,0.8812785388127854,0.8820840950639853,0.8819762122598354,0.8818681318681318,0.8817598533455545,0.881651376146789,0.8815426997245179,0.8814338235294118,0.8813247470101196,0.8821362799263351,0.8820276497695853,0.8819188191881919,0.8818097876269622,0.8826247689463955,0.883441258094357,0.8833333333333333,0.8841519925857275,0.8849721706864564,0.8848653667595172,0.8856877323420075,0.8855813953488372,0.8854748603351955,0.8863000931966449,0.8871268656716418,0.8879551820728291,0.888785046728972,0.8886810102899907,0.8885767790262172,0.8894095595126523,0.8893058161350844,0.8901408450704226,0.8900375939849624,0.8908748824082785,0.891713747645951,0.8916116870876531,0.8915094339622641,0.8914069877242682,0.8913043478260869,0.89120151371807,0.8920454545454546,0.8928909952606635,0.8937381404174574,0.8936372269705603,0.8935361216730038,0.8943862987630827,0.8942857142857142,0.894184938036225,0.8940839694656488,0.894937917860554,0.8957934990439771,0.8956937799043062,0.8955938697318008,0.8954937679769894,0.8953934740882917,0.8952929875120077,0.8961538461538462,0.8970163618864293,0.8978805394990366,0.8987463837994214,0.8996138996138996,0.8995169082125604,0.8994197292069632,0.8993223620522749,0.8992248062015504,0.8991270611057226,0.9,0.9008746355685131,0.9017509727626459,0.9016553067185978,0.9015594541910331,0.9014634146341464,0.9013671875,0.9012707722385142,0.9021526418786693,0.9020568070519099,0.9019607843137255,0.9028459273797841,0.9027504911591355,0.9026548672566371,0.9035433070866141,0.903448275862069,0.903353057199211,0.9032576505429417,0.9031620553359684,0.9030662710187932,0.902970297029703,0.9038652130822596,0.9037698412698413,0.9036742800397219,0.9035785288270378,0.9034825870646767,0.9033864541832669,0.9042871385842473,0.9041916167664671,0.9040959040959041,0.905,0.9049049049049049,0.9048096192384769,0.9047141424272819,0.9046184738955824,0.9055276381909547,0.9054325955734407,0.905337361530715,0.905241935483871,0.905146316851665,0.9050505050505051,0.9049544994944388,0.9058704453441295,0.9067882472137792,0.9066937119675457,0.9065989847715736,0.9065040650406504,0.9064089521871821,0.9063136456211812,0.9072375127420998,0.9071428571428571,0.9080694586312564,0.9079754601226994,0.9078812691914022,0.9077868852459017,0.9076923076923077,0.9086242299794661,0.9085303186022611,0.9084362139917695,0.9083419155509783,0.9092783505154639,0.9091847265221878,0.9090909090909091,0.9100310237849017,0.9099378881987578,0.9098445595854923,0.9107883817427386,0.9117341640706127,0.9126819126819127,0.9125910509885536,0.9135416666666667,0.913451511991658,0.9133611691022965,0.9132706374085684,0.9131799163179917,0.9130890052356021,0.9129979035639413,0.912906610703043,0.9128151260504201,0.9137749737118822,0.9136842105263158,0.9135932560590094,0.9145569620253164,0.914466737064414,0.9143763213530656,0.9142857142857143,0.9141949152542372,0.9151643690349947,0.9150743099787686,0.9149840595111584,0.9148936170212766,0.9148029818956337,0.9147121535181236,0.9156883671291356,0.9155982905982906,0.9165775401069519,0.9164882226980728,0.9163987138263665,0.9173819742489271,0.9183673469387755,0.9182795698924732,0.9181916038751345,0.9181034482758621,0.918015102481122,0.91792656587473,0.9178378378378378,0.9177489177489178,0.9187432286023836,0.9197396963123644,0.9207383279044516,0.9206521739130434,0.9205658324265505,0.920479302832244,0.920392584514722,0.9213973799126638,0.9224043715846995,0.9223194748358862,0.9222343921139102,0.9221491228070176,0.9231613611416026,0.9230769230769231,0.922992299229923,0.9240088105726872,0.9239250275633958,0.9238410596026491,0.9237569060773481,0.9236725663716814,0.9235880398671097,0.9235033259423503,0.9245283018867925,0.9244444444444444,0.9254727474972191,0.9253897550111359,0.9253065774804905,0.9252232142857143,0.9251396648044693,0.9250559284116331,0.9249720044792833,0.9248878923766816,0.9248035914702581,0.9247191011235955,0.9246344206974129,0.9245495495495496,0.9244644870349493,0.9243792325056434,0.9242937853107345,0.9242081447963801,0.9252548131370328,0.9263038548752834,0.9262202043132803,0.9261363636363636,0.926052332195677,0.9271070615034168,0.927023945267959,0.9269406392694064,0.9268571428571428,0.9267734553775744,0.9266895761741123,0.926605504587156,0.9276693455797933,0.9275862068965517,0.9275028768699655,0.9285714285714286,0.9296424452133795,0.9295612009237876,0.9294797687861271,0.9293981481481481,0.9304750869061413,0.931554524361949,0.9326364692218351,0.9325581395348838,0.9324796274738067,0.9324009324009324,0.9323220536756126,0.9322429906542056,0.9321637426900585,0.9332552693208431,0.9331770222743259,0.9330985915492958,0.9330199764982373,0.9341176470588235,0.934040047114252,0.9339622641509434,0.935064935064935,0.9349881796690307,0.936094674556213,0.9360189573459715,0.9371293001186239,0.9382422802850356,0.9381688466111772,0.9380952380952381,0.9380214541120382,0.9391408114558473,0.9390681003584229,0.9389952153110048,0.9389221556886228,0.9388489208633094,0.9387755102040817,0.9387019230769231,0.9386281588447654,0.9385542168674699,0.9384800965018094,0.9384057971014492,0.9383313180169287,0.9382566585956417,0.9381818181818182,0.9381067961165048,0.9380315917375456,0.9379562043795621,0.9378806333739342,0.9378048780487804,0.9377289377289377,0.9388753056234719,0.9388004895960832,0.9387254901960784,0.939877300613497,0.9398034398034398,0.939729397293973,0.9396551724137931,0.9395807644882861,0.9395061728395062,0.9406674907292955,0.9418316831683168,0.9417596034696406,0.9416873449131513,0.9416149068322981,0.9415422885572139,0.9427148194271482,0.942643391521197,0.9425717852684145,0.94375,0.9436795994993742,0.943609022556391,0.944792973651192,0.9447236180904522,0.9446540880503145,0.9445843828715366,0.9445145018915511,0.9444444444444444,0.9443742098609356,0.9443037974683545,0.944233206590621,0.9441624365482234,0.9440914866581956,0.9440203562340967,0.9439490445859873,0.9438775510204082,0.9438058748403576,0.9437340153452686,0.9436619718309859,0.9435897435897436,0.9435173299101413,0.9434447300771208,0.9433719433719434,0.9432989690721649,0.9432258064516129,0.9431524547803618,0.943078913324709,0.9430051813471503,0.9429312581063554,0.9428571428571428,0.9440832249674902,0.9440104166666666,0.9439374185136897,0.943864229765013,0.9437908496732026,0.943717277486911,0.9436435124508519,0.9435695538057742,0.9434954007884363,0.9434210526315789,0.9433465085638999,0.9432717678100264,0.9431968295904888,0.9431216931216931,0.9430463576158941,0.9429708222811671,0.9442231075697212,0.9441489361702128,0.9440745672436751,0.944,0.9439252336448598,0.9438502673796791,0.9437751004016064,0.9436997319034852,0.9436241610738255,0.9435483870967742,0.9434724091520862,0.9433962264150944,0.9433198380566802,0.9432432432432433,0.9431664411366711,0.943089430894309,0.9430122116689281,0.9442934782608695,0.9442176870748299,0.944141689373297,0.9440654843110505,0.9453551912568307,0.945280437756498,0.9465753424657535,0.9465020576131687,0.9464285714285714,0.9463548830811555,0.9462809917355371,0.9462068965517242,0.9461325966850829,0.946058091286307,0.945983379501385,0.9472954230235784,0.9472222222222222,0.9485396383866481,0.9484679665738162,0.9483960948396095,0.9483240223463687,0.9482517482517483,0.9481792717086834,0.9481065918653576,0.9480337078651685,0.9479606188466948,0.9478873239436619,0.9478138222849083,0.9477401129943502,0.9476661951909476,0.9475920679886686,0.9475177304964539,0.9474431818181818,0.9473684210526315,0.9472934472934473,0.9472182596291013,0.9471428571428572,0.9470672389127325,0.9469914040114613,0.9469153515064562,0.9468390804597702,0.9467625899280575,0.946685878962536,0.948051948051948,0.9479768786127167,0.9479015918958031,0.9492753623188406,0.9492017416545718,0.9505813953488372,0.950509461426492,0.9504373177842566,0.9503649635036496,0.9502923976608187,0.9502196193265008,0.9516129032258065,0.9515418502202643,0.9514705882352941,0.9513991163475699,0.9513274336283186,0.9512555391432792,0.9526627218934911,0.9525925925925925,0.9525222551928784,0.9524517087667161,0.9523809523809523,0.9538002980625931,0.9537313432835821,0.953662182361734,0.9550898203592815,0.9550224887556222,0.954954954954955,0.9548872180451128,0.9548192771084337,0.9547511312217195,0.9546827794561934,0.9546142208774584,0.9545454545454546,0.9544764795144158,0.9544072948328267,0.954337899543379,0.9542682926829268,0.9541984732824428,0.9556574923547401,0.9555895865237366,0.9555214723926381,0.9554531490015361,0.9569230769230769,0.9568567026194145,0.9567901234567902,0.9567233384853169,0.9566563467492261,0.9565891472868217,0.9565217391304348,0.9580093312597201,0.9579439252336449,0.9594383775351014,0.959375,0.9593114241001565,0.9592476489028213,0.9591836734693877,0.9591194968553459,0.9590551181102362,0.9589905362776026,0.9589257503949447,0.9588607594936709,0.9587955625990491,0.9587301587301588,0.958664546899841,0.9585987261146497,0.9585326953748007,0.9584664536741214,0.9584,0.9583333333333334,0.9582664526484751,0.9581993569131833,0.9581320450885669,0.9580645161290322,0.9579967689822294,0.9579288025889967,0.9578606158833063,0.9577922077922078,0.9577235772357724,0.9576547231270358,0.9575856443719413,0.9575163398692811,0.9574468085106383,0.9573770491803278,0.9573070607553367,0.9588815789473685,0.9588138385502472,0.9587458745874587,0.9603305785123967,0.9602649006622517,0.9618573797678275,0.9617940199335548,0.961730449251248,0.9616666666666667,0.9632721202003339,0.9632107023411371,0.9631490787269682,0.9630872483221476,0.9630252100840336,0.9629629629629629,0.9629005059021922,0.9628378378378378,0.9627749576988156,0.9627118644067797,0.9626485568760611,0.9625850340136054,0.9642248722316865,0.9641638225255973,0.9641025641025641,0.964041095890411,0.9639794168096055,0.9656357388316151,0.9672977624784854,0.9672413793103448,0.9671848013816926,0.9688581314878892,0.9688041594454073,0.96875,0.9686956521739131,0.9686411149825784,0.9685863874345549,0.9685314685314685,0.968476357267951,0.968421052631579,0.968365553602812,0.9683098591549296,0.9682539682539683,0.9681978798586572,0.968141592920354,0.9680851063829787,0.9680284191829485,0.9679715302491103,0.9679144385026738,0.9678571428571429,0.9677996422182469,0.967741935483871,0.9694793536804309,0.9694244604316546,0.9693693693693693,0.9693140794223827,0.969258589511754,0.9710144927536232,0.9709618874773139,0.9709090909090909,0.970856102003643,0.9708029197080292,0.9707495429616088,0.9706959706959707,0.9706422018348624,0.9705882352941176,0.9705340699815838,0.9704797047970479,0.9704251386321626,0.9703703703703703,0.9721706864564007,0.9721189591078067,0.9739292364990689,0.9738805970149254,0.9738317757009346,0.9737827715355806,0.9737335834896811,0.9736842105263158,0.975517890772128,0.9754716981132076,0.9754253308128544,0.9772727272727273,0.9772296015180265,0.9771863117870723,0.9771428571428571,0.9770992366412213,0.9770554493307839,0.9770114942528736,0.9769673704414588,0.9769230769230769,0.976878612716763,0.9768339768339769,0.97678916827853,0.9767441860465116,0.9766990291262136,0.9785992217898832,0.9785575048732943,0.978515625,0.9784735812133072,0.9784313725490196,0.9783889980353635,0.9783464566929134,0.9783037475345168,0.9782608695652174,0.9782178217821782,0.9781746031746031,0.9781312127236581,0.9800796812749004,0.9800399201596807,0.98,0.9799599198396793,0.9799196787148594,0.9798792756539235,0.9798387096774194,0.9797979797979798,0.979757085020243,0.9797160243407708,0.9796747967479674,0.9796334012219959,0.9795918367346939,0.9795501022494888,0.9795081967213115,0.9794661190965093,0.9794238683127572,0.979381443298969,0.9793388429752066,0.979296066252588,0.979253112033195,0.9792099792099792,0.9791666666666666,0.9791231732776617,0.9790794979079498,0.9790356394129979,0.9789915966386554,0.9789473684210527,0.9789029535864979,0.9788583509513742,0.9788135593220338,0.9787685774946921,0.9787234042553191,0.9786780383795309,0.9786324786324786,0.9785867237687366,0.9785407725321889,0.978494623655914,0.978448275862069,0.978401727861771,0.9783549783549783,0.9783080260303688,0.9782608695652174,0.9782135076252724,0.9781659388646288,0.9781181619256017,0.9780701754385965,0.978021978021978,0.9779735682819384,0.977924944812362,0.9778761061946902,0.9778270509977827,0.9777777777777777,0.977728285077951,0.9776785714285714,0.9776286353467561,0.9775784753363229,0.9775280898876404,0.9774774774774775,0.9774266365688488,0.9773755656108597,0.9773242630385488,0.9772727272727273,0.9772209567198178,0.9771689497716894,0.977116704805492,0.9770642201834863,0.9770114942528736,0.9769585253456221,0.976905311778291,0.9768518518518519,0.9791183294663574,0.9790697674418605,0.9790209790209791,0.9789719626168224,0.9789227166276346,0.9788732394366197,0.9788235294117648,0.9787735849056604,0.9787234042553191,0.9786729857819905,0.9786223277909739,0.9785714285714285,0.9785202863961814,0.9784688995215312,0.9784172661870504,0.9783653846153846,0.9783132530120482,0.9782608695652174,0.9782082324455206,0.9781553398058253,0.9781021897810219,0.9780487804878049,0.9779951100244498,0.9779411764705882,0.9778869778869779,0.9778325123152709,0.9777777777777777,0.9777227722772277,0.9776674937965261,0.9776119402985075,0.9775561097256857,0.9775,0.9774436090225563,0.9773869346733668,0.9773299748110831,0.9772727272727273,0.9772151898734177,0.9771573604060914,0.9770992366412213,0.9770408163265306,0.9769820971867008,0.9769230769230769,0.9768637532133676,0.9768041237113402,0.9767441860465116,0.9766839378238342,0.9766233766233766,0.9765625,0.9765013054830287,0.9764397905759162,0.9763779527559056,0.9763157894736842,0.9762532981530343,0.9761904761904762,0.9761273209549072,0.976063829787234,0.9786666666666667,0.9786096256684492,0.9785522788203753,0.978494623655914,0.9784366576819407,0.9783783783783784,0.978319783197832,0.9782608695652174,0.9782016348773842,0.9781420765027322,0.9780821917808219,0.978021978021978,0.977961432506887,0.9779005524861878,0.9778393351800554,0.9777777777777777,0.9777158774373259,0.9776536312849162,0.9775910364145658,0.9775280898876404,0.9774647887323944,0.9774011299435028,0.9773371104815864,0.9772727272727273,0.9772079772079773,0.98,0.9799426934097422,0.9798850574712644,0.9798270893371758,0.9797687861271677,0.9797101449275363,0.9796511627906976,0.9795918367346939,0.97953216374269,0.9794721407624634,0.9794117647058823,0.9793510324483776,0.9792899408284024,0.9792284866468842,0.9791666666666666,0.9791044776119403,0.9790419161676647,0.978978978978979,0.9789156626506024,0.9788519637462235,0.9787878787878788,0.9787234042553191,0.9786585365853658,0.9785932721712538,0.9785276073619632,0.9784615384615385,0.9783950617283951,0.978328173374613,0.9782608695652174,0.9781931464174455,0.978125,0.9780564263322884,0.9779874213836478,0.9779179810725552,0.9778481012658228,0.9777777777777777,0.9777070063694268,0.9776357827476039,0.9775641025641025,0.977491961414791,0.9806451612903225,0.9805825242718447,0.9805194805194806,0.9804560260586319,0.9803921568627451,0.980327868852459,0.9802631578947368,0.9801980198019802,0.9801324503311258,0.9800664451827242,0.98,0.979933110367893,0.9798657718120806,0.9797979797979798,0.9797297297297297,0.9796610169491525,0.9795918367346939,0.9795221843003413,0.9794520547945206,0.979381443298969,0.9793103448275862,0.9792387543252595,0.9791666666666666,0.9790940766550522,0.9790209790209791,0.9789473684210527,0.9788732394366197,0.9787985865724381,0.9787234042553191,0.9786476868327402,0.9785714285714285,0.978494623655914,0.9784172661870504,0.9783393501805054,0.9782608695652174,0.9781818181818182,0.9781021897810219,0.978021978021978,0.9779411764705882,0.977859778597786,0.9777777777777777,0.9776951672862454,0.9776119402985075,0.9775280898876404,0.9774436090225563,0.9773584905660377,0.9772727272727273,0.9771863117870723,0.9770992366412213,0.9770114942528736,0.9807692307692307,0.9806949806949807,0.9806201550387597,0.980544747081712,0.98046875,0.9803921568627451,0.9803149606299213,0.9802371541501976,0.9841269841269841,0.9840637450199203,0.984,0.9839357429718876,0.9838709677419355,0.9838056680161943,0.983739837398374,0.9836734693877551,0.9836065573770492,0.9835390946502057,0.9834710743801653,0.983402489626556,0.9833333333333333,0.9832635983263598,0.9831932773109243,0.9831223628691983,0.9872881355932204,0.9872340425531915,0.9871794871794872,0.9871244635193133,0.9870689655172413,0.987012987012987,0.9869565217391304,0.9868995633187773,0.9868421052631579,0.986784140969163,0.9867256637168141,0.9866666666666667,0.9866071428571429,0.9865470852017937,0.9864864864864865,0.9864253393665159,0.9863636363636363,0.9863013698630136,0.9862385321100917,0.9861751152073732,0.9861111111111112,0.986046511627907,0.985981308411215,0.9859154929577465,0.9858490566037735,0.985781990521327,0.9857142857142858,0.9856459330143541,0.9855769230769231,0.9855072463768116,0.9854368932038835,0.9853658536585366,0.9852941176470589,0.9852216748768473,0.9851485148514851,0.9850746268656716,0.985,0.9849246231155779,0.9848484848484849,0.9847715736040609,0.9846938775510204,0.9846153846153847,0.9845360824742269,0.9844559585492227,0.984375,0.9842931937172775,0.9842105263157894,0.9841269841269841,0.9840425531914894,0.983957219251337,0.9838709677419355,0.9837837837837838,0.9836956521739131,0.9836065573770492,0.9835164835164835,0.9834254143646409,0.9833333333333333,0.9888268156424581,0.9887640449438202,0.9887005649717514,0.9886363636363636,0.9885714285714285,0.9885057471264368,0.9884393063583815,0.9883720930232558,0.9883040935672515,0.9882352941176471,0.9881656804733728,0.9880952380952381,0.9880239520958084,0.9879518072289156,0.9878787878787879,0.9878048780487805,0.9877300613496932,0.9876543209876543,0.9875776397515528,0.9875,0.9874213836477987,0.9873417721518988,0.9872611464968153,0.9871794871794872,0.9870967741935484,0.987012987012987,0.9869281045751634,0.9868421052631579,0.9867549668874173,0.9866666666666667,0.9865771812080537,0.9864864864864865,0.9863945578231292,0.9863013698630136,0.9862068965517241,0.9861111111111112,0.986013986013986,0.9859154929577465,0.9858156028368794,0.9857142857142858,0.9856115107913669,0.9855072463768116,0.9854014598540146,0.9852941176470589,0.9851851851851852,0.9850746268656716,0.9849624060150376,0.9848484848484849,0.9847328244274809,0.9846153846153847,0.9844961240310077,0.984375,0.984251968503937,0.9841269841269841,0.984,0.9838709677419355,0.983739837398374,0.9918032786885246,0.9917355371900827,0.9916666666666667,0.9915966386554622,0.9915254237288136,0.9914529914529915,0.9913793103448276,0.991304347826087,0.9912280701754386,0.9911504424778761,0.9910714285714286,0.990990990990991,0.990909090909091,0.9908256880733946,0.9907407407407407,0.9906542056074766,0.9905660377358491,0.9904761904761905,0.9903846153846154,0.9902912621359223,0.9901960784313726,0.9900990099009901,0.99,0.98989898989899,0.9897959183673469,0.9896907216494846,0.9895833333333334,0.9894736842105263,0.9893617021276596,0.989247311827957,0.9891304347826086,0.989010989010989,0.9888888888888889,0.9887640449438202,0.9886363636363636,0.9885057471264368,0.9882352941176471,0.9880952380952381,0.9879518072289156,0.9878048780487805,0.9876543209876543,0.9875,0.9873417721518988,0.9871794871794872,0.987012987012987,0.9868421052631579,0.9866666666666667,0.9864864864864865,0.9863013698630136,0.9861111111111112,0.9859154929577465,0.9857142857142858,0.9855072463768116,0.9852941176470589,0.9850746268656716,0.9848484848484849,0.9846153846153847,0.9841269841269841,0.9838709677419355,0.9836065573770492,0.9833333333333333,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1],"yaxis":"y"}],"layout":{"height":600,"legend":{"tracegroupgap":0},"shapes":[{"line":{"dash":"dash"},"type":"line","x0":0,"x1":1,"y0":1,"y1":0}],"showlegend":false,"template":{"data":{"bar":[{"error_x":{"color":"#2a3f5f"},"error_y":{"color":"#2a3f5f"},"marker":{"line":{"color":"white","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"barpolar":[{"marker":{"line":{"color":"white","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"carpet":[{"aaxis":{"endlinecolor":"#2a3f5f","gridcolor":"#C8D4E3","linecolor":"#C8D4E3","minorgridcolor":"#C8D4E3","startlinecolor":"#2a3f5f"},"baxis":{"endlinecolor":"#2a3f5f","gridcolor":"#C8D4E3","linecolor":"#C8D4E3","minorgridcolor":"#C8D4E3","startlinecolor":"#2a3f5f"},"type":"carpet"}],"choropleth":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"choropleth"}],"contour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"contour"}],"contourcarpet":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"contourcarpet"}],"heatmap":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmap"}],"heatmapgl":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmapgl"}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"histogram2d":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2d"}],"histogram2dcontour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2dcontour"}],"mesh3d":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"mesh3d"}],"parcoords":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"parcoords"}],"pie":[{"automargin":true,"type":"pie"}],"scatter":[{"fillpattern":{"fillmode":"overlay","size":10,"solidity":0.2},"type":"scatter"}],"scatter3d":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter3d"}],"scattercarpet":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattercarpet"}],"scattergeo":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergeo"}],"scattergl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergl"}],"scattermapbox":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattermapbox"}],"scatterpolar":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolar"}],"scatterpolargl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolargl"}],"scatterternary":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterternary"}],"surface":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"surface"}],"table":[{"cells":{"fill":{"color":"#EBF0F8"},"line":{"color":"white"}},"header":{"fill":{"color":"#C8D4E3"},"line":{"color":"white"}},"type":"table"}]},"layout":{"annotationdefaults":{"arrowcolor":"#2a3f5f","arrowhead":0,"arrowwidth":1},"autotypenumbers":"strict","coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]],"sequential":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"sequentialminus":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]]},"colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#2a3f5f"},"geo":{"bgcolor":"white","lakecolor":"white","landcolor":"white","showlakes":true,"showland":true,"subunitcolor":"#C8D4E3"},"hoverlabel":{"align":"left"},"hovermode":"closest","mapbox":{"style":"light"},"paper_bgcolor":"white","plot_bgcolor":"white","polar":{"angularaxis":{"gridcolor":"#EBF0F8","linecolor":"#EBF0F8","ticks":""},"bgcolor":"white","radialaxis":{"gridcolor":"#EBF0F8","linecolor":"#EBF0F8","ticks":""}},"scene":{"xaxis":{"backgroundcolor":"white","gridcolor":"#DFE8F3","gridwidth":2,"linecolor":"#EBF0F8","showbackground":true,"ticks":"","zerolinecolor":"#EBF0F8"},"yaxis":{"backgroundcolor":"white","gridcolor":"#DFE8F3","gridwidth":2,"linecolor":"#EBF0F8","showbackground":true,"ticks":"","zerolinecolor":"#EBF0F8"},"zaxis":{"backgroundcolor":"white","gridcolor":"#DFE8F3","gridwidth":2,"linecolor":"#EBF0F8","showbackground":true,"ticks":"","zerolinecolor":"#EBF0F8"}},"shapedefaults":{"line":{"color":"#2a3f5f"}},"ternary":{"aaxis":{"gridcolor":"#DFE8F3","linecolor":"#A2B1C6","ticks":""},"baxis":{"gridcolor":"#DFE8F3","linecolor":"#A2B1C6","ticks":""},"bgcolor":"white","caxis":{"gridcolor":"#DFE8F3","linecolor":"#A2B1C6","ticks":""}},"title":{"x":0.05},"xaxis":{"automargin":true,"gridcolor":"#EBF0F8","linecolor":"#EBF0F8","ticks":"","title":{"standoff":15},"zerolinecolor":"#EBF0F8","zerolinewidth":2},"yaxis":{"automargin":true,"gridcolor":"#EBF0F8","linecolor":"#EBF0F8","ticks":"","title":{"standoff":15},"zerolinecolor":"#EBF0F8","zerolinewidth":2}}},"title":{"text":"Precision-Recall Curve (AUC=0.8994)"},"width":800,"xaxis":{"anchor":"y","constrain":"domain","domain":[0,1],"title":{"text":"Recall"}},"yaxis":{"anchor":"x","domain":[0,1],"scaleanchor":"x","scaleratio":1,"title":{"text":"Precision"}}}},"text/html":["<div>                            <div id=\"f45e23c0-a81d-4219-b8cb-a2404c696d26\" class=\"plotly-graph-div\" style=\"height:600px; width:800px;\"></div>            <script type=\"text/javascript\">                require([\"plotly\"], function(Plotly) {                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"f45e23c0-a81d-4219-b8cb-a2404c696d26\")) {                    Plotly.newPlot(                        \"f45e23c0-a81d-4219-b8cb-a2404c696d26\",                        [{\"fillpattern\":{\"shape\":\"\"},\"hovertemplate\":\"Recall=%{x}\\u003cbr\\u003ePrecision=%{y}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"\",\"line\":{\"color\":\"#636efa\"},\"marker\":{\"symbol\":\"circle\"},\"mode\":\"lines\",\"name\":\"\",\"orientation\":\"v\",\"showlegend\":false,\"stackgroup\":\"1\",\"x\":[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,0.9990319457889641,0.9990319457889641,0.9980638915779284,0.9980638915779284,0.9980638915779284,0.9980638915779284,0.9980638915779284,0.9980638915779284,0.9980638915779284,0.9980638915779284,0.9980638915779284,0.9980638915779284,0.9980638915779284,0.9980638915779284,0.9980638915779284,0.9980638915779284,0.9980638915779284,0.9980638915779284,0.9980638915779284,0.9980638915779284,0.9980638915779284,0.9980638915779284,0.9980638915779284,0.9980638915779284,0.9980638915779284,0.9980638915779284,0.9970958373668926,0.9970958373668926,0.9970958373668926,0.9970958373668926,0.9970958373668926,0.9970958373668926,0.9970958373668926,0.9961277831558567,0.9961277831558567,0.9961277831558567,0.9961277831558567,0.9961277831558567,0.995159728944821,0.995159728944821,0.995159728944821,0.995159728944821,0.995159728944821,0.995159728944821,0.995159728944821,0.995159728944821,0.995159728944821,0.995159728944821,0.995159728944821,0.9941916747337851,0.9941916747337851,0.9941916747337851,0.9941916747337851,0.9941916747337851,0.9941916747337851,0.9941916747337851,0.9941916747337851,0.9941916747337851,0.9941916747337851,0.9941916747337851,0.9941916747337851,0.9941916747337851,0.9932236205227493,0.9932236205227493,0.9932236205227493,0.9922555663117134,0.9912875121006777,0.9903194578896418,0.9903194578896418,0.9903194578896418,0.9903194578896418,0.9903194578896418,0.989351403678606,0.989351403678606,0.989351403678606,0.989351403678606,0.989351403678606,0.989351403678606,0.989351403678606,0.989351403678606,0.989351403678606,0.9883833494675702,0.9874152952565344,0.9874152952565344,0.9864472410454985,0.9864472410454985,0.9854791868344628,0.9854791868344628,0.9854791868344628,0.9854791868344628,0.9854791868344628,0.9854791868344628,0.9854791868344628,0.9854791868344628,0.9854791868344628,0.9845111326234269,0.9845111326234269,0.9845111326234269,0.9835430784123911,0.9835430784123911,0.9835430784123911,0.9825750242013552,0.9816069699903195,0.9816069699903195,0.9816069699903195,0.9816069699903195,0.9816069699903195,0.9806389157792836,0.9796708615682478,0.9796708615682478,0.978702807357212,0.978702807357212,0.978702807357212,0.9777347531461762,0.9777347531461762,0.9777347531461762,0.9767666989351403,0.9757986447241046,0.9757986447241046,0.9757986447241046,0.9748305905130688,0.9748305905130688,0.9738625363020329,0.9738625363020329,0.9738625363020329,0.9738625363020329,0.9738625363020329,0.972894482090997,0.972894482090997,0.972894482090997,0.9719264278799613,0.9719264278799613,0.9709583736689255,0.9699903194578896,0.9699903194578896,0.9699903194578896,0.9699903194578896,0.9699903194578896,0.9690222652468539,0.9690222652468539,0.9690222652468539,0.968054211035818,0.9670861568247822,0.9670861568247822,0.9670861568247822,0.9661181026137464,0.9661181026137464,0.9661181026137464,0.9661181026137464,0.9661181026137464,0.9661181026137464,0.9661181026137464,0.9661181026137464,0.9661181026137464,0.9651500484027106,0.9651500484027106,0.9651500484027106,0.9651500484027106,0.9651500484027106,0.9651500484027106,0.9651500484027106,0.9641819941916747,0.9632139399806389,0.9622458857696031,0.9612778315585673,0.9603097773475314,0.9603097773475314,0.9593417231364957,0.9583736689254598,0.957405614714424,0.9564375605033882,0.9554695062923524,0.9545014520813165,0.9535333978702807,0.952565343659245,0.9515972894482091,0.9506292352371732,0.9496611810261375,0.9496611810261375,0.9496611810261375,0.9496611810261375,0.9486931268151017,0.9486931268151017,0.9477250726040658,0.9477250726040658,0.9467570183930301,0.9457889641819942,0.9457889641819942,0.9448209099709584,0.9448209099709584,0.9448209099709584,0.9448209099709584,0.9438528557599225,0.9438528557599225,0.9428848015488868,0.9419167473378509,0.9419167473378509,0.9409486931268151,0.9399806389157793,0.9390125847047435,0.9390125847047435,0.9390125847047435,0.9390125847047435,0.9390125847047435,0.9390125847047435,0.9380445304937076,0.9370764762826719,0.9370764762826719,0.936108422071636,0.9351403678606002,0.9351403678606002,0.9351403678606002,0.9341723136495643,0.9341723136495643,0.9332042594385286,0.9322362052274927,0.9312681510164569,0.9303000968054211,0.9293320425943853,0.9283639883833494,0.9273959341723137,0.9273959341723137,0.9264278799612778,0.925459825750242,0.9244917715392061,0.9244917715392061,0.9244917715392061,0.9235237173281704,0.9235237173281704,0.9235237173281704,0.9225556631171346,0.9225556631171346,0.9215876089060987,0.920619554695063,0.920619554695063,0.920619554695063,0.920619554695063,0.920619554695063,0.9196515004840271,0.9186834462729913,0.9186834462729913,0.9177153920619555,0.9177153920619555,0.9167473378509197,0.9167473378509197,0.9167473378509197,0.9157792836398838,0.914811229428848,0.9138431752178122,0.9128751210067764,0.9119070667957405,0.9119070667957405,0.9119070667957405,0.9119070667957405,0.9109390125847048,0.9099709583736689,0.9099709583736689,0.9090029041626331,0.9080348499515973,0.9070667957405615,0.9070667957405615,0.9070667957405615,0.9060987415295256,0.9051306873184899,0.904162633107454,0.9031945788964182,0.9022265246853823,0.9022265246853823,0.9022265246853823,0.9022265246853823,0.9022265246853823,0.9022265246853823,0.9012584704743466,0.9002904162633107,0.8993223620522749,0.8983543078412392,0.8973862536302033,0.8973862536302033,0.8973862536302033,0.8973862536302033,0.8964181994191674,0.8954501452081317,0.8944820909970959,0.89351403678606,0.8925459825750242,0.8925459825750242,0.8915779283639884,0.8906098741529526,0.8906098741529526,0.8896418199419167,0.888673765730881,0.888673765730881,0.8877057115198451,0.8867376573088093,0.8857696030977735,0.8848015488867377,0.8838334946757018,0.882865440464666,0.882865440464666,0.8818973862536302,0.8809293320425944,0.8799612778315585,0.8789932236205228,0.8780251694094869,0.8780251694094869,0.8770571151984511,0.8760890609874153,0.8760890609874153,0.8751210067763795,0.8741529525653436,0.8731848983543078,0.872216844143272,0.872216844143272,0.8712487899322362,0.8702807357212003,0.8693126815101646,0.8683446272991288,0.8673765730880929,0.8664085188770572,0.8664085188770572,0.8664085188770572,0.8654404646660213,0.8644724104549855,0.8635043562439496,0.8625363020329139,0.861568247821878,0.861568247821878,0.8606001936108422,0.8606001936108422,0.8596321393998064,0.8586640851887706,0.8576960309777347,0.856727976766699,0.856727976766699,0.8557599225556631,0.8547918683446273,0.8538238141335914,0.8538238141335914,0.8528557599225557,0.8518877057115198,0.8518877057115198,0.850919651500484,0.8499515972894482,0.8499515972894482,0.8499515972894482,0.8499515972894482,0.8489835430784124,0.8489835430784124,0.8480154888673765,0.8470474346563408,0.846079380445305,0.8451113262342691,0.8441432720232332,0.8431752178121975,0.8422071636011617,0.8412391093901258,0.8412391093901258,0.8402710551790901,0.8393030009680542,0.8393030009680542,0.8383349467570184,0.8373668925459826,0.8363988383349468,0.8354307841239109,0.8354307841239109,0.8344627299128751,0.8334946757018393,0.8325266214908035,0.8315585672797676,0.8305905130687319,0.8305905130687319,0.829622458857696,0.829622458857696,0.8286544046466602,0.8276863504356244,0.8276863504356244,0.8276863504356244,0.8267182962245886,0.8257502420135527,0.8247821878025169,0.8238141335914811,0.8228460793804453,0.8218780251694094,0.8209099709583737,0.8209099709583737,0.8209099709583737,0.8209099709583737,0.8199419167473379,0.818973862536302,0.8180058083252663,0.8170377541142304,0.8170377541142304,0.8170377541142304,0.8160696999031946,0.8151016456921588,0.814133591481123,0.814133591481123,0.8131655372700871,0.8121974830590513,0.8121974830590513,0.8112294288480155,0.8102613746369797,0.8092933204259438,0.8083252662149081,0.8073572120038722,0.8063891577928364,0.8063891577928364,0.8054211035818006,0.8054211035818006,0.8044530493707648,0.8034849951597289,0.8025169409486931,0.8015488867376573,0.8005808325266215,0.7996127783155856,0.7986447241045499,0.797676669893514,0.7967086156824782,0.7957405614714425,0.7947725072604066,0.7938044530493708,0.7928363988383349,0.7918683446272992,0.7909002904162633,0.7909002904162633,0.7909002904162633,0.7899322362052275,0.7889641819941917,0.7879961277831559,0.7879961277831559,0.78702807357212,0.7860600193610843,0.7850919651500484,0.7841239109390126,0.7831558567279767,0.782187802516941,0.782187802516941,0.7812197483059051,0.7802516940948693,0.7802516940948693,0.7802516940948693,0.7792836398838335,0.7783155856727977,0.7773475314617618,0.7773475314617618,0.7773475314617618,0.7773475314617618,0.7763794772507261,0.7754114230396902,0.7744433688286544,0.7734753146176185,0.7725072604065828,0.771539206195547,0.771539206195547,0.7705711519845111,0.7696030977734754,0.7686350435624395,0.7686350435624395,0.7676669893514037,0.7666989351403679,0.7666989351403679,0.7657308809293321,0.7657308809293321,0.7647628267182962,0.7647628267182962,0.7647628267182962,0.7637947725072604,0.7628267182962246,0.7618586640851888,0.7618586640851888,0.7608906098741529,0.7599225556631172,0.7589545014520813,0.7579864472410455,0.7570183930300097,0.7560503388189739,0.755082284607938,0.7541142303969022,0.7531461761858664,0.7521781219748306,0.7512100677637947,0.750242013552759,0.7492739593417231,0.7483059051306873,0.7473378509196515,0.7463697967086157,0.7454017424975798,0.744433688286544,0.7434656340755083,0.7434656340755083,0.7424975798644724,0.7415295256534365,0.7415295256534365,0.7405614714424008,0.739593417231365,0.7386253630203291,0.7376573088092934,0.7366892545982575,0.7366892545982575,0.7366892545982575,0.7357212003872217,0.7347531461761858,0.7337850919651501,0.7328170377541142,0.7328170377541142,0.7318489835430784,0.7308809293320426,0.7308809293320426,0.7299128751210068,0.7289448209099709,0.7289448209099709,0.7279767666989352,0.7270087124878993,0.7260406582768635,0.7250726040658277,0.7241045498547919,0.723136495643756,0.7221684414327202,0.7212003872216844,0.7202323330106486,0.7192642787996127,0.718296224588577,0.7173281703775412,0.7163601161665053,0.7153920619554696,0.7144240077444337,0.7134559535333979,0.712487899322362,0.7115198451113263,0.7105517909002904,0.7095837366892546,0.7086156824782188,0.707647628267183,0.7066795740561471,0.7057115198451114,0.7047434656340755,0.7037754114230397,0.7028073572120038,0.7028073572120038,0.7018393030009681,0.7008712487899322,0.6999031945788964,0.6989351403678606,0.6979670861568248,0.6969990319457889,0.6960309777347532,0.6950629235237173,0.6940948693126815,0.6931268151016456,0.6921587608906099,0.691190706679574,0.6902226524685382,0.6892545982575025,0.6882865440464666,0.6882865440464666,0.6873184898354308,0.686350435624395,0.6853823814133592,0.6844143272023233,0.6834462729912875,0.6824782187802517,0.6815101645692159,0.68054211035818,0.6795740561471443,0.6786060019361084,0.6776379477250726,0.6766698935140368,0.675701839303001,0.6747337850919651,0.6737657308809293,0.6727976766698935,0.6727976766698935,0.6718296224588577,0.6708615682478218,0.6698935140367861,0.6698935140367861,0.6689254598257502,0.6689254598257502,0.6679574056147144,0.6669893514036787,0.6660212971926428,0.665053242981607,0.6640851887705711,0.6631171345595354,0.6621490803484995,0.6611810261374637,0.6611810261374637,0.6602129719264279,0.6602129719264279,0.6592449177153921,0.6582768635043562,0.6573088092933205,0.6563407550822846,0.6553727008712488,0.6544046466602129,0.6534365924491772,0.6524685382381413,0.6515004840271055,0.6505324298160697,0.6495643756050339,0.648596321393998,0.6476282671829623,0.6466602129719264,0.6456921587608906,0.6447241045498547,0.643756050338819,0.6427879961277831,0.6418199419167473,0.6408518877057116,0.6398838334946757,0.6389157792836399,0.6379477250726041,0.6369796708615683,0.6360116166505324,0.6360116166505324,0.6350435624394967,0.6340755082284608,0.6340755082284608,0.633107454017425,0.633107454017425,0.6321393998063891,0.6311713455953534,0.6302032913843175,0.6292352371732817,0.6282671829622459,0.6282671829622459,0.6272991287512101,0.6263310745401742,0.6253630203291385,0.6243949661181026,0.6234269119070668,0.6234269119070668,0.6224588576960309,0.6214908034849952,0.6205227492739593,0.6195546950629235,0.6195546950629235,0.6185866408518877,0.6176185866408519,0.6176185866408519,0.616650532429816,0.6156824782187803,0.6147144240077445,0.6137463697967086,0.6127783155856728,0.611810261374637,0.6108422071636012,0.6098741529525653,0.6089060987415296,0.6079380445304937,0.6069699903194579,0.6060019361084221,0.6050338818973863,0.6050338818973863,0.6040658276863504,0.6030977734753146,0.6021297192642788,0.6021297192642788,0.601161665053243,0.6001936108422071,0.5992255566311714,0.5982575024201355,0.5972894482090997,0.5963213939980639,0.5963213939980639,0.5953533397870281,0.5953533397870281,0.5943852855759922,0.5934172313649564,0.5924491771539206,0.5914811229428848,0.590513068731849,0.5895450145208132,0.5885769603097774,0.5876089060987415,0.5866408518877058,0.5856727976766699,0.5847047434656341,0.5837366892545982,0.5827686350435625,0.5818005808325266,0.5808325266214908,0.579864472410455,0.5788964181994192,0.5779283639883833,0.5769603097773476,0.5759922555663117,0.5750242013552759,0.57405614714424,0.5730880929332043,0.5721200387221684,0.5711519845111326,0.5701839303000968,0.569215876089061,0.5682478218780251,0.5672797676669894,0.5663117134559535,0.5653436592449177,0.5643756050338818,0.5643756050338818,0.5634075508228461,0.5624394966118103,0.5624394966118103,0.5614714424007744,0.5614714424007744,0.5605033881897387,0.5595353339787028,0.558567279767667,0.558567279767667,0.5575992255566312,0.5566311713455954,0.5556631171345595,0.5546950629235237,0.5537270087124879,0.5527589545014521,0.5517909002904162,0.5508228460793805,0.5498547918683446,0.5488867376573088,0.547918683446273,0.547918683446273,0.5469506292352372,0.5459825750242013,0.5450145208131656,0.5440464666021297,0.5440464666021297,0.5440464666021297,0.5430784123910939,0.542110358180058,0.542110358180058,0.5411423039690223,0.5401742497579864,0.5392061955469506,0.5382381413359149,0.537270087124879,0.5363020329138432,0.5353339787028074,0.5343659244917716,0.5333978702807357,0.5324298160696999,0.5314617618586641,0.5304937076476283,0.5295256534365924,0.5285575992255567,0.5275895450145208,0.526621490803485,0.5256534365924492,0.5246853823814134,0.5237173281703775,0.5227492739593417,0.5227492739593417,0.5217812197483059,0.5208131655372701,0.5198451113262342,0.5188770571151985,0.5188770571151985,0.5179090029041626,0.5169409486931268,0.515972894482091,0.5150048402710552,0.5140367860600193,0.5130687318489835,0.5121006776379478,0.5111326234269119,0.510164569215876,0.5091965150048403,0.5082284607938045,0.5072604065827686,0.5072604065827686,0.5062923523717329,0.5062923523717329,0.505324298160697,0.5043562439496612,0.5033881897386253,0.5024201355275896,0.5014520813165537,0.5014520813165537,0.5004840271055179,0.4995159728944821,0.4995159728944821,0.4985479186834463,0.4975798644724105,0.49661181026137463,0.49564375605033884,0.494675701839303,0.4937076476282672,0.4927395934172314,0.49177153920619554,0.49080348499515974,0.4898354307841239,0.4888673765730881,0.4878993223620523,0.48693126815101645,0.48693126815101645,0.48596321393998065,0.4849951597289448,0.484027105517909,0.4830590513068732,0.48209099709583736,0.48112294288480156,0.4801548886737657,0.4791868344627299,0.4782187802516941,0.47725072604065827,0.4762826718296225,0.4762826718296225,0.4753146176185866,0.4743465634075508,0.47337850919651503,0.4724104549854792,0.4714424007744434,0.47047434656340753,0.46950629235237173,0.46853823814133594,0.4675701839303001,0.4666021297192643,0.46563407550822844,0.46466602129719264,0.46369796708615685,0.462729912875121,0.4617618586640852,0.46079380445304935,0.45982575024201355,0.45885769603097776,0.4578896418199419,0.4569215876089061,0.45595353339787026,0.45498547918683446,0.45401742497579867,0.4530493707647628,0.452081316553727,0.45111326234269117,0.45014520813165537,0.4491771539206196,0.4482090997095837,0.44724104549854793,0.4462729912875121,0.4453049370764763,0.4443368828654405,0.44336882865440463,0.44240077444336884,0.441432720232333,0.4404646660212972,0.4394966118102614,0.43852855759922554,0.43756050338818975,0.4365924491771539,0.4356243949661181,0.4346563407550823,0.43368828654404645,0.43272023233301066,0.4317521781219748,0.430784123910939,0.4298160696999032,0.42884801548886736,0.42787996127783157,0.4269119070667957,0.4259438528557599,0.4249757986447241,0.42400774443368827,0.4230396902226525,0.4220716360116166,0.42110358180058083,0.42013552758954503,0.4191674733785092,0.4181994191674734,0.41723136495643753,0.41626331074540174,0.41529525653436594,0.4143272023233301,0.4133591481122943,0.41239109390125844,0.41142303969022265,0.41045498547918685,0.409486931268151,0.4085188770571152,0.4085188770571152,0.4075508228460794,0.40658276863504356,0.40561471442400776,0.4046466602129719,0.4036786060019361,0.4027105517909003,0.40174249757986447,0.40077444336882867,0.3998063891577928,0.398838334946757,0.3978702807357212,0.3969022265246854,0.3959341723136496,0.39496611810261373,0.39399806389157793,0.39303000968054214,0.3920619554695063,0.3910939012584705,0.39012584704743464,0.38915779283639884,0.38818973862536305,0.3872216844143272,0.3862536302032914,0.38528557599225555,0.38431752178121975,0.38334946757018395,0.3823814133591481,0.3814133591481123,0.38044530493707646,0.37947725072604066,0.37850919651500486,0.377541142303969,0.3765730880929332,0.37560503388189737,0.37463697967086157,0.3736689254598258,0.3727008712487899,0.3717328170377541,0.3707647628267183,0.3697967086156825,0.3688286544046467,0.36786060019361083,0.36689254598257504,0.3659244917715392,0.3649564375605034,0.3639883833494676,0.36302032913843174,0.36205227492739595,0.3610842207163601,0.3601161665053243,0.3591481122942885,0.35818005808325265,0.35721200387221685,0.356243949661181,0.3552758954501452,0.3552758954501452,0.3543078412391094,0.35333978702807356,0.35237173281703776,0.3514036786060019,0.3504356243949661,0.3494675701839303,0.34849951597289447,0.3475314617618587,0.3465634075508228,0.345595353339787,0.34462729912875123,0.3436592449177154,0.3426911907066796,0.34172313649564373,0.34075508228460794,0.33978702807357214,0.3388189738625363,0.3378509196515005,0.33688286544046464,0.33591481122942884,0.33494675701839305,0.3339787028073572,0.3330106485963214,0.33204259438528555,0.33204259438528555,0.33107454017424975,0.33010648596321396,0.3291384317521781,0.3281703775411423,0.32720232333010646,0.32623426911907066,0.32526621490803487,0.324298160696999,0.3233301064859632,0.32236205227492737,0.3213939980638916,0.3204259438528558,0.3194578896418199,0.31848983543078413,0.31752178121974833,0.3165537270087125,0.3155856727976767,0.31461761858664083,0.31364956437560504,0.31268151016456924,0.3117134559535334,0.3107454017424976,0.30977734753146174,0.30880929332042595,0.30784123910939015,0.3068731848983543,0.3059051306873185,0.30493707647628265,0.30396902226524686,0.30300096805421106,0.3020329138431752,0.3010648596321394,0.30009680542110356,0.29912875121006777,0.29816069699903197,0.2971926427879961,0.2962245885769603,0.2952565343659245,0.2942884801548887,0.2942884801548887,0.2933204259438529,0.29235237173281703,0.29138431752178123,0.2904162633107454,0.2894482090997096,0.2884801548886738,0.28751210067763794,0.28654404646660214,0.2855759922555663,0.2846079380445305,0.2836398838334947,0.28267182962245885,0.28170377541142305,0.2807357212003872,0.2797676669893514,0.2787996127783156,0.27783155856727976,0.27686350435624396,0.2758954501452081,0.2749273959341723,0.2739593417231365,0.27299128751210067,0.27202323330106487,0.271055179090029,0.2700871248789932,0.2691190706679574,0.2681510164569216,0.2671829622458858,0.26621490803484993,0.26524685382381413,0.26427879961277834,0.2633107454017425,0.2623426911907067,0.26137463697967084,0.26040658276863504,0.25943852855759925,0.2584704743465634,0.2575024201355276,0.25653436592449175,0.25556631171345595,0.25459825750242016,0.2536302032913843,0.2526621490803485,0.25169409486931266,0.25072604065827686,0.24975798644724104,0.24878993223620524,0.24782187802516942,0.2468538238141336,0.2468538238141336,0.24588576960309777,0.24491771539206195,0.24394966118102615,0.24298160696999033,0.2420135527589545,0.24104549854791868,0.24007744433688286,0.24007744433688286,0.23910939012584706,0.23814133591481124,0.2371732817037754,0.2362052274927396,0.23523717328170377,0.23426911907066797,0.23330106485963215,0.23233301064859632,0.2313649564375605,0.23039690222652467,0.22942884801548888,0.22846079380445306,0.22749273959341723,0.2265246853823814,0.22555663117134558,0.22555663117134558,0.2245885769603098,0.22362052274927396,0.22265246853823814,0.22168441432720232,0.2207163601161665,0.2197483059051307,0.21878025169409487,0.21781219748305905,0.21684414327202323,0.2158760890609874,0.2149080348499516,0.21393998063891578,0.21297192642787996,0.21200387221684414,0.2110358180058083,0.21006776379477252,0.2090997095837367,0.20813165537270087,0.20716360116166505,0.20619554695062922,0.20522749273959343,0.2042594385285576,0.20329138431752178,0.20232333010648595,0.20135527589545016,0.20038722168441434,0.1994191674733785,0.1984511132623427,0.19748305905130686,0.19651500484027107,0.19554695062923524,0.19457889641819942,0.1936108422071636,0.19264278799612777,0.19167473378509198,0.19070667957405615,0.18973862536302033,0.1887705711519845,0.18780251694094868,0.1868344627299129,0.18586640851887706,0.18489835430784124,0.18393030009680542,0.1829622458857696,0.1819941916747338,0.18102613746369797,0.18005808325266215,0.17909002904162633,0.1781219748305905,0.1771539206195547,0.17618586640851888,0.17521781219748306,0.17424975798644723,0.1732817037754114,0.17231364956437561,0.1713455953533398,0.1713455953533398,0.17037754114230397,0.16940948693126814,0.16844143272023232,0.16747337850919652,0.1665053242981607,0.16553727008712488,0.16456921587608905,0.16360116166505323,0.16263310745401743,0.1616650532429816,0.1606969990319458,0.15972894482090996,0.15876089060987417,0.15779283639883834,0.15682478218780252,0.1558567279767667,0.15488867376573087,0.15392061955469508,0.15295256534365925,0.15198451113262343,0.1510164569215876,0.15004840271055178,0.14908034849951599,0.14811229428848016,0.14714424007744434,0.14617618586640851,0.1452081316553727,0.1442400774443369,0.14327202323330107,0.14230396902226525,0.14133591481122942,0.1403678606001936,0.1393998063891578,0.13843175217812198,0.13746369796708616,0.13649564375605033,0.1355275895450145,0.1345595353339787,0.1335914811229429,0.13262342691190707,0.13165537270087124,0.13068731848983542,0.12971926427879962,0.1287512100677638,0.12778315585672798,0.12681510164569215,0.12584704743465633,0.12487899322362052,0.12391093901258471,0.12294288480154889,0.12197483059051308,0.12100677637947725,0.12003872216844143,0.11907066795740562,0.1181026137463698,0.11713455953533398,0.11713455953533398,0.11616650532429816,0.11519845111326234,0.11423039690222653,0.1132623426911907,0.1122942884801549,0.11132623426911907,0.11035818005808325,0.10939012584704744,0.10842207163601161,0.1074540174249758,0.10648596321393998,0.10551790900290416,0.10454985479186835,0.10358180058083252,0.10261374636979671,0.10164569215876089,0.10067763794772508,0.09970958373668926,0.09874152952565343,0.09777347531461762,0.0968054211035818,0.09583736689254599,0.09486931268151017,0.09390125847047434,0.09293320425943853,0.09196515004840271,0.0909970958373669,0.09002904162633107,0.08906098741529525,0.08809293320425944,0.08712487899322362,0.08615682478218781,0.08518877057115198,0.08422071636011616,0.08325266214908035,0.08131655372700872,0.0803484995159729,0.07938044530493708,0.07841239109390126,0.07744433688286544,0.07647628267182963,0.0755082284607938,0.07454017424975799,0.07357212003872217,0.07260406582768635,0.07163601161665054,0.07066795740561471,0.0696999031945789,0.06873184898354308,0.06776379477250725,0.06679574056147145,0.06582768635043562,0.06485963213939981,0.06389157792836399,0.06292352371732816,0.061955469506292354,0.060019361084220714,0.0590513068731849,0.05808325266214908,0.057115198451113264,0.057115198451113264,0.05614714424007745,0.05517909002904162,0.05421103581800581,0.05324298160696999,0.05227492739593417,0.051306873184898356,0.05033881897386254,0.0484027105517909,0.04743465634075508,0.046466602129719266,0.04549854791868345,0.044530493707647625,0.04356243949661181,0.04259438528557599,0.041626331074540175,0.04065827686350436,0.03872216844143272,0.0377541142303969,0.036786060019361085,0.03581800580832527,0.03484995159728945,0.03388189738625363,0.03291384317521781,0.031945788964181994,0.030977734753146177,0.030009680542110357,0.02904162633107454,0.028073572120038724,0.027105517909002903,0.026137463697967087,0.02516940948693127,0.02420135527589545,0.023233301064859633,0.022265246853823813,0.021297192642787996,0.02032913843175218,0.01936108422071636,0.018393030009680542,0.017424975798644726,0.016456921587608905,0.015488867376573089,0.01452081316553727,0.013552758954501452,0.012584704743465635,0.011616650532429816,0.010648596321393998,0.0],\"xaxis\":\"x\",\"y\":[0.7341862117981521,0.7347083926031295,0.7352313167259786,0.7357549857549858,0.7362794012829651,0.7368045649072753,0.7373304782298359,0.7378571428571429,0.7383845604002859,0.7389127324749643,0.7394416607015032,0.7399713467048711,0.7405017921146954,0.7410329985652798,0.741564967695621,0.7420977011494253,0.7426312005751258,0.7431654676258993,0.7437005039596832,0.7442363112391931,0.7447728911319395,0.7453102453102453,0.7458483754512636,0.7463872832369942,0.7469269703543022,0.7474674384949349,0.7480086893555394,0.7485507246376811,0.7490935460478607,0.7496371552975326,0.7501815541031227,0.7507267441860465,0.7512727272727273,0.7518195050946143,0.752367079388201,0.7529154518950437,0.7534646243617797,0.754014598540146,0.7545653761869978,0.7551169590643275,0.7556693489392831,0.7562225475841874,0.7567765567765568,0.7573313782991202,0.7578870139398386,0.7584434654919237,0.7590007347538574,0.7595588235294117,0.7601177336276674,0.7606774668630338,0.7612380250552689,0.7617994100294986,0.7623616236162362,0.7629246676514032,0.7634885439763488,0.7640532544378699,0.764618800888231,0.7651851851851852,0.765752409191994,0.766320474777448,0.7668893838158871,0.7674591381872214,0.7680297397769517,0.7686011904761905,0.7691734921816828,0.7697466467958272,0.7703206562266965,0.7708955223880597,0.7714712471994025,0.7720478325859492,0.7726252804786836,0.7732035928143712,0.7737827715355805,0.7743628185907047,0.7749437359339835,0.7755255255255256,0.7761081893313299,0.7766917293233083,0.7772761474793077,0.7778614457831325,0.7784476262245666,0.7790346907993967,0.779622641509434,0.7802114803625377,0.780801209372638,0.7813918305597579,0.7819833459500378,0.7825757575757576,0.7831690674753601,0.783763277693475,0.7843583902809416,0.7849544072948328,0.785551330798479,0.7861491628614916,0.7867479055597868,0.7873475609756098,0.7879481311975591,0.7885496183206107,0.7891520244461421,0.7897553516819572,0.7895944912012242,0.7901990811638591,0.7900383141762453,0.7906441717791411,0.7912509593246354,0.7918586789554531,0.792467332820907,0.7930769230769231,0.7936874518860662,0.7942989214175655,0.79491133384734,0.7955246913580247,0.7961389961389962,0.7967542503863988,0.7973704563031709,0.7979876160990712,0.7986057319907048,0.7992248062015503,0.799844840961986,0.8004658385093167,0.8010878010878011,0.8017107309486781,0.8023346303501946,0.8029595015576324,0.8035853468433359,0.8042121684867395,0.8040593286494926,0.8046875,0.8053166536356529,0.8059467918622848,0.8065779169929522,0.8072100313479624,0.807843137254902,0.8076923076923077,0.8083267871170463,0.8089622641509434,0.8095987411487018,0.8102362204724409,0.8100866824271079,0.8107255520504731,0.8113654301499605,0.8120063191153238,0.8126482213438735,0.8132911392405063,0.8139350752177356,0.8145800316957211,0.8152260111022998,0.8158730158730159,0.8165210484511517,0.8163751987281399,0.817024661893397,0.8176751592356688,0.8183266932270916,0.8189792663476874,0.819632881085395,0.8202875399361023,0.820943245403677,0.8216,0.822257806244996,0.8229166666666666,0.8235765838011226,0.8242375601926164,0.8240963855421687,0.8247588424437299,0.825422365245374,0.8252818035426731,0.8251410153102336,0.825,0.8256658595641646,0.8263327948303716,0.8270008084074374,0.8276699029126213,0.8275303643724696,0.8282009724473258,0.8288726682887266,0.8295454545454546,0.8302193338748984,0.8308943089430895,0.8315703824247356,0.8322475570032574,0.8329258353708231,0.83278955954323,0.8326530612244898,0.8333333333333334,0.8331970564186427,0.8338788870703764,0.8337428337428338,0.8344262295081967,0.8351107465135357,0.8357963875205254,0.8364831552999178,0.837171052631579,0.8378600823045268,0.8385502471169687,0.8392415498763397,0.8391089108910891,0.8398018166804294,0.8404958677685951,0.8403639371381307,0.8410596026490066,0.8417564208782105,0.8416252072968491,0.84149377593361,0.8421926910299004,0.8428927680798005,0.8435940099833611,0.8442964196502915,0.8441666666666666,0.8440366972477065,0.8447412353923205,0.8446115288220551,0.8453177257525084,0.8460251046025105,0.8458961474036851,0.8466051969823973,0.8473154362416108,0.8471872376154492,0.8470588235294118,0.847771236333053,0.8484848484848485,0.8483572030328559,0.8490725126475548,0.8489451476793249,0.8496621621621622,0.8503803888419273,0.8510998307952623,0.8518204911092294,0.8516949152542372,0.8524173027989822,0.8531409168081494,0.8530161427357689,0.8537414965986394,0.8536170212765958,0.8534923339011925,0.8542199488491049,0.8549488054607508,0.8556789069171649,0.8564102564102564,0.8562874251497006,0.8570205479452054,0.8577549271636675,0.8576329331046312,0.8575107296137339,0.8582474226804123,0.8589853826311264,0.8588640275387264,0.859603789836348,0.8603448275862069,0.8610871440897325,0.8618307426597582,0.8625756266205704,0.8633217993079585,0.8640692640692641,0.8648180242634316,0.8647007805724197,0.8654513888888888,0.8662033014769766,0.8669565217391304,0.8677110530896431,0.8684668989547039,0.8692240627724499,0.8691099476439791,0.868995633187773,0.8688811188811189,0.868766404199475,0.8686514886164624,0.8694127957931639,0.8692982456140351,0.8691834942932397,0.8690685413005272,0.8689533861037819,0.8688380281690141,0.8687224669603524,0.8686067019400353,0.8684907325684025,0.8683745583038869,0.8682581786030061,0.868141592920354,0.8689105403011514,0.8696808510638298,0.870452528837622,0.8703374777975134,0.8711111111111111,0.8709964412811388,0.8717720391807658,0.8716577540106952,0.8715432649420161,0.8723214285714286,0.872207327971403,0.8729874776386404,0.873769024171889,0.8745519713261649,0.874439461883408,0.8752244165170556,0.8751123090745733,0.875,0.8757875787578758,0.8756756756756757,0.8755635707844905,0.8754512635379061,0.8762420957542909,0.8770343580470162,0.8778280542986425,0.8786231884057971,0.8794197642792384,0.8793103448275862,0.8792007266121707,0.88,0.8798908098271155,0.8797814207650273,0.8805834092980857,0.8813868613138686,0.8812785388127854,0.8820840950639853,0.8819762122598354,0.8818681318681318,0.8817598533455545,0.881651376146789,0.8815426997245179,0.8814338235294118,0.8813247470101196,0.8821362799263351,0.8820276497695853,0.8819188191881919,0.8818097876269622,0.8826247689463955,0.883441258094357,0.8833333333333333,0.8841519925857275,0.8849721706864564,0.8848653667595172,0.8856877323420075,0.8855813953488372,0.8854748603351955,0.8863000931966449,0.8871268656716418,0.8879551820728291,0.888785046728972,0.8886810102899907,0.8885767790262172,0.8894095595126523,0.8893058161350844,0.8901408450704226,0.8900375939849624,0.8908748824082785,0.891713747645951,0.8916116870876531,0.8915094339622641,0.8914069877242682,0.8913043478260869,0.89120151371807,0.8920454545454546,0.8928909952606635,0.8937381404174574,0.8936372269705603,0.8935361216730038,0.8943862987630827,0.8942857142857142,0.894184938036225,0.8940839694656488,0.894937917860554,0.8957934990439771,0.8956937799043062,0.8955938697318008,0.8954937679769894,0.8953934740882917,0.8952929875120077,0.8961538461538462,0.8970163618864293,0.8978805394990366,0.8987463837994214,0.8996138996138996,0.8995169082125604,0.8994197292069632,0.8993223620522749,0.8992248062015504,0.8991270611057226,0.9,0.9008746355685131,0.9017509727626459,0.9016553067185978,0.9015594541910331,0.9014634146341464,0.9013671875,0.9012707722385142,0.9021526418786693,0.9020568070519099,0.9019607843137255,0.9028459273797841,0.9027504911591355,0.9026548672566371,0.9035433070866141,0.903448275862069,0.903353057199211,0.9032576505429417,0.9031620553359684,0.9030662710187932,0.902970297029703,0.9038652130822596,0.9037698412698413,0.9036742800397219,0.9035785288270378,0.9034825870646767,0.9033864541832669,0.9042871385842473,0.9041916167664671,0.9040959040959041,0.905,0.9049049049049049,0.9048096192384769,0.9047141424272819,0.9046184738955824,0.9055276381909547,0.9054325955734407,0.905337361530715,0.905241935483871,0.905146316851665,0.9050505050505051,0.9049544994944388,0.9058704453441295,0.9067882472137792,0.9066937119675457,0.9065989847715736,0.9065040650406504,0.9064089521871821,0.9063136456211812,0.9072375127420998,0.9071428571428571,0.9080694586312564,0.9079754601226994,0.9078812691914022,0.9077868852459017,0.9076923076923077,0.9086242299794661,0.9085303186022611,0.9084362139917695,0.9083419155509783,0.9092783505154639,0.9091847265221878,0.9090909090909091,0.9100310237849017,0.9099378881987578,0.9098445595854923,0.9107883817427386,0.9117341640706127,0.9126819126819127,0.9125910509885536,0.9135416666666667,0.913451511991658,0.9133611691022965,0.9132706374085684,0.9131799163179917,0.9130890052356021,0.9129979035639413,0.912906610703043,0.9128151260504201,0.9137749737118822,0.9136842105263158,0.9135932560590094,0.9145569620253164,0.914466737064414,0.9143763213530656,0.9142857142857143,0.9141949152542372,0.9151643690349947,0.9150743099787686,0.9149840595111584,0.9148936170212766,0.9148029818956337,0.9147121535181236,0.9156883671291356,0.9155982905982906,0.9165775401069519,0.9164882226980728,0.9163987138263665,0.9173819742489271,0.9183673469387755,0.9182795698924732,0.9181916038751345,0.9181034482758621,0.918015102481122,0.91792656587473,0.9178378378378378,0.9177489177489178,0.9187432286023836,0.9197396963123644,0.9207383279044516,0.9206521739130434,0.9205658324265505,0.920479302832244,0.920392584514722,0.9213973799126638,0.9224043715846995,0.9223194748358862,0.9222343921139102,0.9221491228070176,0.9231613611416026,0.9230769230769231,0.922992299229923,0.9240088105726872,0.9239250275633958,0.9238410596026491,0.9237569060773481,0.9236725663716814,0.9235880398671097,0.9235033259423503,0.9245283018867925,0.9244444444444444,0.9254727474972191,0.9253897550111359,0.9253065774804905,0.9252232142857143,0.9251396648044693,0.9250559284116331,0.9249720044792833,0.9248878923766816,0.9248035914702581,0.9247191011235955,0.9246344206974129,0.9245495495495496,0.9244644870349493,0.9243792325056434,0.9242937853107345,0.9242081447963801,0.9252548131370328,0.9263038548752834,0.9262202043132803,0.9261363636363636,0.926052332195677,0.9271070615034168,0.927023945267959,0.9269406392694064,0.9268571428571428,0.9267734553775744,0.9266895761741123,0.926605504587156,0.9276693455797933,0.9275862068965517,0.9275028768699655,0.9285714285714286,0.9296424452133795,0.9295612009237876,0.9294797687861271,0.9293981481481481,0.9304750869061413,0.931554524361949,0.9326364692218351,0.9325581395348838,0.9324796274738067,0.9324009324009324,0.9323220536756126,0.9322429906542056,0.9321637426900585,0.9332552693208431,0.9331770222743259,0.9330985915492958,0.9330199764982373,0.9341176470588235,0.934040047114252,0.9339622641509434,0.935064935064935,0.9349881796690307,0.936094674556213,0.9360189573459715,0.9371293001186239,0.9382422802850356,0.9381688466111772,0.9380952380952381,0.9380214541120382,0.9391408114558473,0.9390681003584229,0.9389952153110048,0.9389221556886228,0.9388489208633094,0.9387755102040817,0.9387019230769231,0.9386281588447654,0.9385542168674699,0.9384800965018094,0.9384057971014492,0.9383313180169287,0.9382566585956417,0.9381818181818182,0.9381067961165048,0.9380315917375456,0.9379562043795621,0.9378806333739342,0.9378048780487804,0.9377289377289377,0.9388753056234719,0.9388004895960832,0.9387254901960784,0.939877300613497,0.9398034398034398,0.939729397293973,0.9396551724137931,0.9395807644882861,0.9395061728395062,0.9406674907292955,0.9418316831683168,0.9417596034696406,0.9416873449131513,0.9416149068322981,0.9415422885572139,0.9427148194271482,0.942643391521197,0.9425717852684145,0.94375,0.9436795994993742,0.943609022556391,0.944792973651192,0.9447236180904522,0.9446540880503145,0.9445843828715366,0.9445145018915511,0.9444444444444444,0.9443742098609356,0.9443037974683545,0.944233206590621,0.9441624365482234,0.9440914866581956,0.9440203562340967,0.9439490445859873,0.9438775510204082,0.9438058748403576,0.9437340153452686,0.9436619718309859,0.9435897435897436,0.9435173299101413,0.9434447300771208,0.9433719433719434,0.9432989690721649,0.9432258064516129,0.9431524547803618,0.943078913324709,0.9430051813471503,0.9429312581063554,0.9428571428571428,0.9440832249674902,0.9440104166666666,0.9439374185136897,0.943864229765013,0.9437908496732026,0.943717277486911,0.9436435124508519,0.9435695538057742,0.9434954007884363,0.9434210526315789,0.9433465085638999,0.9432717678100264,0.9431968295904888,0.9431216931216931,0.9430463576158941,0.9429708222811671,0.9442231075697212,0.9441489361702128,0.9440745672436751,0.944,0.9439252336448598,0.9438502673796791,0.9437751004016064,0.9436997319034852,0.9436241610738255,0.9435483870967742,0.9434724091520862,0.9433962264150944,0.9433198380566802,0.9432432432432433,0.9431664411366711,0.943089430894309,0.9430122116689281,0.9442934782608695,0.9442176870748299,0.944141689373297,0.9440654843110505,0.9453551912568307,0.945280437756498,0.9465753424657535,0.9465020576131687,0.9464285714285714,0.9463548830811555,0.9462809917355371,0.9462068965517242,0.9461325966850829,0.946058091286307,0.945983379501385,0.9472954230235784,0.9472222222222222,0.9485396383866481,0.9484679665738162,0.9483960948396095,0.9483240223463687,0.9482517482517483,0.9481792717086834,0.9481065918653576,0.9480337078651685,0.9479606188466948,0.9478873239436619,0.9478138222849083,0.9477401129943502,0.9476661951909476,0.9475920679886686,0.9475177304964539,0.9474431818181818,0.9473684210526315,0.9472934472934473,0.9472182596291013,0.9471428571428572,0.9470672389127325,0.9469914040114613,0.9469153515064562,0.9468390804597702,0.9467625899280575,0.946685878962536,0.948051948051948,0.9479768786127167,0.9479015918958031,0.9492753623188406,0.9492017416545718,0.9505813953488372,0.950509461426492,0.9504373177842566,0.9503649635036496,0.9502923976608187,0.9502196193265008,0.9516129032258065,0.9515418502202643,0.9514705882352941,0.9513991163475699,0.9513274336283186,0.9512555391432792,0.9526627218934911,0.9525925925925925,0.9525222551928784,0.9524517087667161,0.9523809523809523,0.9538002980625931,0.9537313432835821,0.953662182361734,0.9550898203592815,0.9550224887556222,0.954954954954955,0.9548872180451128,0.9548192771084337,0.9547511312217195,0.9546827794561934,0.9546142208774584,0.9545454545454546,0.9544764795144158,0.9544072948328267,0.954337899543379,0.9542682926829268,0.9541984732824428,0.9556574923547401,0.9555895865237366,0.9555214723926381,0.9554531490015361,0.9569230769230769,0.9568567026194145,0.9567901234567902,0.9567233384853169,0.9566563467492261,0.9565891472868217,0.9565217391304348,0.9580093312597201,0.9579439252336449,0.9594383775351014,0.959375,0.9593114241001565,0.9592476489028213,0.9591836734693877,0.9591194968553459,0.9590551181102362,0.9589905362776026,0.9589257503949447,0.9588607594936709,0.9587955625990491,0.9587301587301588,0.958664546899841,0.9585987261146497,0.9585326953748007,0.9584664536741214,0.9584,0.9583333333333334,0.9582664526484751,0.9581993569131833,0.9581320450885669,0.9580645161290322,0.9579967689822294,0.9579288025889967,0.9578606158833063,0.9577922077922078,0.9577235772357724,0.9576547231270358,0.9575856443719413,0.9575163398692811,0.9574468085106383,0.9573770491803278,0.9573070607553367,0.9588815789473685,0.9588138385502472,0.9587458745874587,0.9603305785123967,0.9602649006622517,0.9618573797678275,0.9617940199335548,0.961730449251248,0.9616666666666667,0.9632721202003339,0.9632107023411371,0.9631490787269682,0.9630872483221476,0.9630252100840336,0.9629629629629629,0.9629005059021922,0.9628378378378378,0.9627749576988156,0.9627118644067797,0.9626485568760611,0.9625850340136054,0.9642248722316865,0.9641638225255973,0.9641025641025641,0.964041095890411,0.9639794168096055,0.9656357388316151,0.9672977624784854,0.9672413793103448,0.9671848013816926,0.9688581314878892,0.9688041594454073,0.96875,0.9686956521739131,0.9686411149825784,0.9685863874345549,0.9685314685314685,0.968476357267951,0.968421052631579,0.968365553602812,0.9683098591549296,0.9682539682539683,0.9681978798586572,0.968141592920354,0.9680851063829787,0.9680284191829485,0.9679715302491103,0.9679144385026738,0.9678571428571429,0.9677996422182469,0.967741935483871,0.9694793536804309,0.9694244604316546,0.9693693693693693,0.9693140794223827,0.969258589511754,0.9710144927536232,0.9709618874773139,0.9709090909090909,0.970856102003643,0.9708029197080292,0.9707495429616088,0.9706959706959707,0.9706422018348624,0.9705882352941176,0.9705340699815838,0.9704797047970479,0.9704251386321626,0.9703703703703703,0.9721706864564007,0.9721189591078067,0.9739292364990689,0.9738805970149254,0.9738317757009346,0.9737827715355806,0.9737335834896811,0.9736842105263158,0.975517890772128,0.9754716981132076,0.9754253308128544,0.9772727272727273,0.9772296015180265,0.9771863117870723,0.9771428571428571,0.9770992366412213,0.9770554493307839,0.9770114942528736,0.9769673704414588,0.9769230769230769,0.976878612716763,0.9768339768339769,0.97678916827853,0.9767441860465116,0.9766990291262136,0.9785992217898832,0.9785575048732943,0.978515625,0.9784735812133072,0.9784313725490196,0.9783889980353635,0.9783464566929134,0.9783037475345168,0.9782608695652174,0.9782178217821782,0.9781746031746031,0.9781312127236581,0.9800796812749004,0.9800399201596807,0.98,0.9799599198396793,0.9799196787148594,0.9798792756539235,0.9798387096774194,0.9797979797979798,0.979757085020243,0.9797160243407708,0.9796747967479674,0.9796334012219959,0.9795918367346939,0.9795501022494888,0.9795081967213115,0.9794661190965093,0.9794238683127572,0.979381443298969,0.9793388429752066,0.979296066252588,0.979253112033195,0.9792099792099792,0.9791666666666666,0.9791231732776617,0.9790794979079498,0.9790356394129979,0.9789915966386554,0.9789473684210527,0.9789029535864979,0.9788583509513742,0.9788135593220338,0.9787685774946921,0.9787234042553191,0.9786780383795309,0.9786324786324786,0.9785867237687366,0.9785407725321889,0.978494623655914,0.978448275862069,0.978401727861771,0.9783549783549783,0.9783080260303688,0.9782608695652174,0.9782135076252724,0.9781659388646288,0.9781181619256017,0.9780701754385965,0.978021978021978,0.9779735682819384,0.977924944812362,0.9778761061946902,0.9778270509977827,0.9777777777777777,0.977728285077951,0.9776785714285714,0.9776286353467561,0.9775784753363229,0.9775280898876404,0.9774774774774775,0.9774266365688488,0.9773755656108597,0.9773242630385488,0.9772727272727273,0.9772209567198178,0.9771689497716894,0.977116704805492,0.9770642201834863,0.9770114942528736,0.9769585253456221,0.976905311778291,0.9768518518518519,0.9791183294663574,0.9790697674418605,0.9790209790209791,0.9789719626168224,0.9789227166276346,0.9788732394366197,0.9788235294117648,0.9787735849056604,0.9787234042553191,0.9786729857819905,0.9786223277909739,0.9785714285714285,0.9785202863961814,0.9784688995215312,0.9784172661870504,0.9783653846153846,0.9783132530120482,0.9782608695652174,0.9782082324455206,0.9781553398058253,0.9781021897810219,0.9780487804878049,0.9779951100244498,0.9779411764705882,0.9778869778869779,0.9778325123152709,0.9777777777777777,0.9777227722772277,0.9776674937965261,0.9776119402985075,0.9775561097256857,0.9775,0.9774436090225563,0.9773869346733668,0.9773299748110831,0.9772727272727273,0.9772151898734177,0.9771573604060914,0.9770992366412213,0.9770408163265306,0.9769820971867008,0.9769230769230769,0.9768637532133676,0.9768041237113402,0.9767441860465116,0.9766839378238342,0.9766233766233766,0.9765625,0.9765013054830287,0.9764397905759162,0.9763779527559056,0.9763157894736842,0.9762532981530343,0.9761904761904762,0.9761273209549072,0.976063829787234,0.9786666666666667,0.9786096256684492,0.9785522788203753,0.978494623655914,0.9784366576819407,0.9783783783783784,0.978319783197832,0.9782608695652174,0.9782016348773842,0.9781420765027322,0.9780821917808219,0.978021978021978,0.977961432506887,0.9779005524861878,0.9778393351800554,0.9777777777777777,0.9777158774373259,0.9776536312849162,0.9775910364145658,0.9775280898876404,0.9774647887323944,0.9774011299435028,0.9773371104815864,0.9772727272727273,0.9772079772079773,0.98,0.9799426934097422,0.9798850574712644,0.9798270893371758,0.9797687861271677,0.9797101449275363,0.9796511627906976,0.9795918367346939,0.97953216374269,0.9794721407624634,0.9794117647058823,0.9793510324483776,0.9792899408284024,0.9792284866468842,0.9791666666666666,0.9791044776119403,0.9790419161676647,0.978978978978979,0.9789156626506024,0.9788519637462235,0.9787878787878788,0.9787234042553191,0.9786585365853658,0.9785932721712538,0.9785276073619632,0.9784615384615385,0.9783950617283951,0.978328173374613,0.9782608695652174,0.9781931464174455,0.978125,0.9780564263322884,0.9779874213836478,0.9779179810725552,0.9778481012658228,0.9777777777777777,0.9777070063694268,0.9776357827476039,0.9775641025641025,0.977491961414791,0.9806451612903225,0.9805825242718447,0.9805194805194806,0.9804560260586319,0.9803921568627451,0.980327868852459,0.9802631578947368,0.9801980198019802,0.9801324503311258,0.9800664451827242,0.98,0.979933110367893,0.9798657718120806,0.9797979797979798,0.9797297297297297,0.9796610169491525,0.9795918367346939,0.9795221843003413,0.9794520547945206,0.979381443298969,0.9793103448275862,0.9792387543252595,0.9791666666666666,0.9790940766550522,0.9790209790209791,0.9789473684210527,0.9788732394366197,0.9787985865724381,0.9787234042553191,0.9786476868327402,0.9785714285714285,0.978494623655914,0.9784172661870504,0.9783393501805054,0.9782608695652174,0.9781818181818182,0.9781021897810219,0.978021978021978,0.9779411764705882,0.977859778597786,0.9777777777777777,0.9776951672862454,0.9776119402985075,0.9775280898876404,0.9774436090225563,0.9773584905660377,0.9772727272727273,0.9771863117870723,0.9770992366412213,0.9770114942528736,0.9807692307692307,0.9806949806949807,0.9806201550387597,0.980544747081712,0.98046875,0.9803921568627451,0.9803149606299213,0.9802371541501976,0.9841269841269841,0.9840637450199203,0.984,0.9839357429718876,0.9838709677419355,0.9838056680161943,0.983739837398374,0.9836734693877551,0.9836065573770492,0.9835390946502057,0.9834710743801653,0.983402489626556,0.9833333333333333,0.9832635983263598,0.9831932773109243,0.9831223628691983,0.9872881355932204,0.9872340425531915,0.9871794871794872,0.9871244635193133,0.9870689655172413,0.987012987012987,0.9869565217391304,0.9868995633187773,0.9868421052631579,0.986784140969163,0.9867256637168141,0.9866666666666667,0.9866071428571429,0.9865470852017937,0.9864864864864865,0.9864253393665159,0.9863636363636363,0.9863013698630136,0.9862385321100917,0.9861751152073732,0.9861111111111112,0.986046511627907,0.985981308411215,0.9859154929577465,0.9858490566037735,0.985781990521327,0.9857142857142858,0.9856459330143541,0.9855769230769231,0.9855072463768116,0.9854368932038835,0.9853658536585366,0.9852941176470589,0.9852216748768473,0.9851485148514851,0.9850746268656716,0.985,0.9849246231155779,0.9848484848484849,0.9847715736040609,0.9846938775510204,0.9846153846153847,0.9845360824742269,0.9844559585492227,0.984375,0.9842931937172775,0.9842105263157894,0.9841269841269841,0.9840425531914894,0.983957219251337,0.9838709677419355,0.9837837837837838,0.9836956521739131,0.9836065573770492,0.9835164835164835,0.9834254143646409,0.9833333333333333,0.9888268156424581,0.9887640449438202,0.9887005649717514,0.9886363636363636,0.9885714285714285,0.9885057471264368,0.9884393063583815,0.9883720930232558,0.9883040935672515,0.9882352941176471,0.9881656804733728,0.9880952380952381,0.9880239520958084,0.9879518072289156,0.9878787878787879,0.9878048780487805,0.9877300613496932,0.9876543209876543,0.9875776397515528,0.9875,0.9874213836477987,0.9873417721518988,0.9872611464968153,0.9871794871794872,0.9870967741935484,0.987012987012987,0.9869281045751634,0.9868421052631579,0.9867549668874173,0.9866666666666667,0.9865771812080537,0.9864864864864865,0.9863945578231292,0.9863013698630136,0.9862068965517241,0.9861111111111112,0.986013986013986,0.9859154929577465,0.9858156028368794,0.9857142857142858,0.9856115107913669,0.9855072463768116,0.9854014598540146,0.9852941176470589,0.9851851851851852,0.9850746268656716,0.9849624060150376,0.9848484848484849,0.9847328244274809,0.9846153846153847,0.9844961240310077,0.984375,0.984251968503937,0.9841269841269841,0.984,0.9838709677419355,0.983739837398374,0.9918032786885246,0.9917355371900827,0.9916666666666667,0.9915966386554622,0.9915254237288136,0.9914529914529915,0.9913793103448276,0.991304347826087,0.9912280701754386,0.9911504424778761,0.9910714285714286,0.990990990990991,0.990909090909091,0.9908256880733946,0.9907407407407407,0.9906542056074766,0.9905660377358491,0.9904761904761905,0.9903846153846154,0.9902912621359223,0.9901960784313726,0.9900990099009901,0.99,0.98989898989899,0.9897959183673469,0.9896907216494846,0.9895833333333334,0.9894736842105263,0.9893617021276596,0.989247311827957,0.9891304347826086,0.989010989010989,0.9888888888888889,0.9887640449438202,0.9886363636363636,0.9885057471264368,0.9882352941176471,0.9880952380952381,0.9879518072289156,0.9878048780487805,0.9876543209876543,0.9875,0.9873417721518988,0.9871794871794872,0.987012987012987,0.9868421052631579,0.9866666666666667,0.9864864864864865,0.9863013698630136,0.9861111111111112,0.9859154929577465,0.9857142857142858,0.9855072463768116,0.9852941176470589,0.9850746268656716,0.9848484848484849,0.9846153846153847,0.9841269841269841,0.9838709677419355,0.9836065573770492,0.9833333333333333,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0],\"yaxis\":\"y\",\"type\":\"scatter\"}],                        {\"template\":{\"data\":{\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"white\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"white\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"#C8D4E3\",\"linecolor\":\"#C8D4E3\",\"minorgridcolor\":\"#C8D4E3\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"#C8D4E3\",\"linecolor\":\"#C8D4E3\",\"minorgridcolor\":\"#C8D4E3\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"choropleth\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"choropleth\"}],\"contourcarpet\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"contourcarpet\"}],\"contour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"contour\"}],\"heatmapgl\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmapgl\"}],\"heatmap\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmap\"}],\"histogram2dcontour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2dcontour\"}],\"histogram2d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2d\"}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"mesh3d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"mesh3d\"}],\"parcoords\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"parcoords\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}],\"scatter3d\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatter3d\"}],\"scattercarpet\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattercarpet\"}],\"scattergeo\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergeo\"}],\"scattergl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergl\"}],\"scattermapbox\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattermapbox\"}],\"scatterpolargl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolargl\"}],\"scatterpolar\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolar\"}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"scatterternary\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterternary\"}],\"surface\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"surface\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}]},\"layout\":{\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"autotypenumbers\":\"strict\",\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]],\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]},\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"geo\":{\"bgcolor\":\"white\",\"lakecolor\":\"white\",\"landcolor\":\"white\",\"showlakes\":true,\"showland\":true,\"subunitcolor\":\"#C8D4E3\"},\"hoverlabel\":{\"align\":\"left\"},\"hovermode\":\"closest\",\"mapbox\":{\"style\":\"light\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"white\",\"polar\":{\"angularaxis\":{\"gridcolor\":\"#EBF0F8\",\"linecolor\":\"#EBF0F8\",\"ticks\":\"\"},\"bgcolor\":\"white\",\"radialaxis\":{\"gridcolor\":\"#EBF0F8\",\"linecolor\":\"#EBF0F8\",\"ticks\":\"\"}},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"white\",\"gridcolor\":\"#DFE8F3\",\"gridwidth\":2,\"linecolor\":\"#EBF0F8\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"#EBF0F8\"},\"yaxis\":{\"backgroundcolor\":\"white\",\"gridcolor\":\"#DFE8F3\",\"gridwidth\":2,\"linecolor\":\"#EBF0F8\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"#EBF0F8\"},\"zaxis\":{\"backgroundcolor\":\"white\",\"gridcolor\":\"#DFE8F3\",\"gridwidth\":2,\"linecolor\":\"#EBF0F8\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"#EBF0F8\"}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"ternary\":{\"aaxis\":{\"gridcolor\":\"#DFE8F3\",\"linecolor\":\"#A2B1C6\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"#DFE8F3\",\"linecolor\":\"#A2B1C6\",\"ticks\":\"\"},\"bgcolor\":\"white\",\"caxis\":{\"gridcolor\":\"#DFE8F3\",\"linecolor\":\"#A2B1C6\",\"ticks\":\"\"}},\"title\":{\"x\":0.05},\"xaxis\":{\"automargin\":true,\"gridcolor\":\"#EBF0F8\",\"linecolor\":\"#EBF0F8\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"#EBF0F8\",\"zerolinewidth\":2},\"yaxis\":{\"automargin\":true,\"gridcolor\":\"#EBF0F8\",\"linecolor\":\"#EBF0F8\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"#EBF0F8\",\"zerolinewidth\":2}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"Recall\"},\"constrain\":\"domain\"},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"Precision\"},\"scaleanchor\":\"x\",\"scaleratio\":1},\"legend\":{\"tracegroupgap\":0},\"title\":{\"text\":\"Precision-Recall Curve (AUC=0.8994)\"},\"height\":600,\"width\":800,\"shapes\":[{\"line\":{\"dash\":\"dash\"},\"type\":\"line\",\"x0\":0,\"x1\":1,\"y0\":1,\"y1\":0}],\"showlegend\":false},                        {\"responsive\": true}                    ).then(function(){\n","                            \n","var gd = document.getElementById('f45e23c0-a81d-4219-b8cb-a2404c696d26');\n","var x = new MutationObserver(function (mutations, observer) {{\n","        var display = window.getComputedStyle(gd).display;\n","        if (!display || display === 'none') {{\n","            console.log([gd, 'removed!']);\n","            Plotly.purge(gd);\n","            observer.disconnect();\n","        }}\n","}});\n","\n","// Listen for the removal of the full notebook cells\n","var notebookContainer = gd.closest('#notebook-container');\n","if (notebookContainer) {{\n","    x.observe(notebookContainer, {childList: true});\n","}}\n","\n","// Listen for the clearing of the current output cell\n","var outputEl = gd.closest('.output');\n","if (outputEl) {{\n","    x.observe(outputEl, {childList: true});\n","}}\n","\n","                        })                };                });            </script>        </div>"]},"metadata":{},"output_type":"display_data"}],"source":["target_score = forest_clf.predict_proba(features_valid)[:, 1]\n","\n","fpr, tpr, thresholds = roc_curve(target_valid, target_score)\n","\n","fig = px.area(\n","    x=fpr, y=tpr,\n","    title=f'ROC Curve (AUC={auc(fpr, tpr):.4f})',\n","    labels=dict(x='False Positive Rate', y='True Positive Rate'),\n","    width=800, height=600\n",")\n","fig.add_shape(\n","    type='line', line=dict(dash='dash'),\n","    x0=0, x1=1, y0=0, y1=1\n",")\n","\n","fig.update_yaxes(scaleanchor=\"x\", scaleratio=1)\n","fig.update_xaxes(constrain='domain')\n","fig.update_layout(showlegend=False)\n","fig.show()\n","\n","precision, recall, thresholds = precision_recall_curve(target_valid, target_score)\n","\n","fig = px.area(\n","    x=recall, y=precision,\n","    title=f'Precision-Recall Curve (AUC={auc(fpr, tpr):.4f})',\n","    labels=dict(x='Recall', y='Precision'),\n","    width=800, height=600\n",")\n","fig.add_shape(\n","    type='line', line=dict(dash='dash'),\n","    x0=0, x1=1, y0=1, y1=0\n",")\n","fig.update_yaxes(scaleanchor=\"x\", scaleratio=1)\n","fig.update_xaxes(constrain='domain')\n","fig.update_layout(showlegend=False)\n","fig.show()"]},{"cell_type":"markdown","metadata":{},"source":["-----------"]},{"cell_type":"markdown","metadata":{},"source":["# Decision Tree"]},{"cell_type":"code","execution_count":114,"metadata":{"execution":{"iopub.execute_input":"2023-11-30T16:51:29.110557Z","iopub.status.busy":"2023-11-30T16:51:29.110290Z","iopub.status.idle":"2023-11-30T16:51:33.787645Z","shell.execute_reply":"2023-11-30T16:51:33.786705Z","shell.execute_reply.started":"2023-11-30T16:51:29.110532Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Runtime:\n","CPU times: user 158 ms, sys: 64.8 ms, total: 223 ms\n","Wall time: 849 ms\n"]}],"source":["%%time\n","tree_model = DecisionTreeClassifier(random_state=random_state)\n","tree_parameters = [{'max_depth': [2,6,12,18,22,35],\n","                     'min_samples_split': [2,6,12],\n","                     \"criterion\": ['gini', 'entropy', 'log_loss'],\n","                     \"splitter\": ['best', 'random'],\n","                   }]\n","\n","tree_clf = RandomizedSearchCV(tree_model, tree_parameters, scoring='roc_auc', n_jobs=-1, cv=cv)\n","tree_clf.fit(features_train, target_train)\n","# create a variable for the best model\n","best_tree = tree_clf.best_estimator_\n","tree_pred = best_tree.predict(features_valid)\n","print('Runtime:')"]},{"cell_type":"code","execution_count":115,"metadata":{"execution":{"iopub.execute_input":"2023-11-30T16:51:33.789028Z","iopub.status.busy":"2023-11-30T16:51:33.788754Z","iopub.status.idle":"2023-11-30T16:51:35.084919Z","shell.execute_reply":"2023-11-30T16:51:35.083993Z","shell.execute_reply.started":"2023-11-30T16:51:33.789006Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Cross Validation Scores: [0.76152018 0.74971198 0.77724654 0.72453917 0.77750576 0.78245968\n"," 0.74115783 0.74190668 0.79236015 0.72746186 0.74220294 0.7484735\n"," 0.76621544 0.69415323 0.75256336 0.74637097 0.73545507 0.74104263\n"," 0.8058108  0.75062124 0.73454191 0.78245968 0.76794355 0.73459101\n"," 0.75207373 0.79585253 0.73185484 0.78493664 0.76518435 0.74007455\n"," 0.7264513  0.74104263 0.75368664 0.72404954 0.79014977 0.75889977\n"," 0.78332373 0.74003456 0.72344545 0.74576687 0.73530086 0.76422811\n"," 0.72690092 0.70259217 0.71362327 0.78741359 0.76782834 0.79634217\n"," 0.75307732 0.77009651]\n"]}],"source":["tree_scores = cross_val_score(tree_model, features_train, target_train, cv=cv, scoring='roc_auc')\n","print('Cross Validation Scores: {}'.format(tree_scores))"]},{"cell_type":"code","execution_count":116,"metadata":{"execution":{"iopub.execute_input":"2023-11-30T16:51:35.086379Z","iopub.status.busy":"2023-11-30T16:51:35.086103Z","iopub.status.idle":"2023-11-30T16:51:35.097642Z","shell.execute_reply":"2023-11-30T16:51:35.096722Z","shell.execute_reply.started":"2023-11-30T16:51:35.086355Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Best hyperparameters: {'splitter': 'random', 'min_samples_split': 6, 'max_depth': 6, 'criterion': 'gini'}\n","\n","Best score: 0.8238422675633421\n","\n","Average Cross Validation Score: 0.7530509062867848\n","\n","ROC AUC Score - Validation Dataset: 0.8430742709827044\n"]}],"source":["# summary\n","print('Best hyperparameters:',  tree_clf.best_params_)\n","print()\n","print('Best score:',  tree_clf.best_score_)\n","print()\n","print('Average Cross Validation Score: {}'.format(tree_scores.mean()))\n","print()\n","print('ROC AUC Score - Validation Dataset:',  roc_auc_score(target_valid, tree_clf.predict_proba(features_valid)[:, 1]))"]},{"cell_type":"markdown","metadata":{},"source":["# ROC AUC Curve - DecisionTreeClassifier"]},{"cell_type":"code","execution_count":117,"metadata":{"execution":{"iopub.execute_input":"2023-11-30T16:51:35.099923Z","iopub.status.busy":"2023-11-30T16:51:35.099527Z","iopub.status.idle":"2023-11-30T16:51:35.224449Z","shell.execute_reply":"2023-11-30T16:51:35.223687Z","shell.execute_reply.started":"2023-11-30T16:51:35.099892Z"},"trusted":true},"outputs":[{"data":{"application/vnd.plotly.v1+json":{"config":{"plotlyServerURL":"https://plot.ly"},"data":[{"fillpattern":{"shape":""},"hovertemplate":"False Positive Rate=%{x}<br>True Positive Rate=%{y}<extra></extra>","legendgroup":"","line":{"color":"#636efa"},"marker":{"symbol":"circle"},"mode":"lines","name":"","orientation":"v","showlegend":false,"stackgroup":"1","type":"scatter","x":[0,0.034759358288770054,0.0374331550802139,0.0374331550802139,0.0427807486631016,0.0427807486631016,0.0427807486631016,0.0427807486631016,0.045454545454545456,0.045454545454545456,0.0481283422459893,0.0748663101604278,0.07754010695187166,0.08021390374331551,0.08288770053475936,0.08288770053475936,0.08288770053475936,0.09358288770053476,0.0962566844919786,0.10160427807486631,0.15775401069518716,0.16042780748663102,0.20320855614973263,0.24598930481283424,0.24598930481283424,0.2700534759358289,0.2727272727272727,0.2727272727272727,0.2807486631016043,0.44385026737967914,0.44385026737967914,0.6363636363636364,0.6390374331550802,0.6711229946524064,1],"xaxis":"x","y":[0,0.24104549854791868,0.2846079380445305,0.32526621490803487,0.35237173281703776,0.3862536302032914,0.40658276863504356,0.409486931268151,0.4259438528557599,0.43756050338818975,0.4530493707647628,0.5333978702807357,0.5450145208131656,0.5701839303000968,0.5779283639883833,0.5914811229428848,0.5924491771539206,0.6050338818973863,0.6089060987415296,0.6176185866408519,0.6824782187802517,0.6989351403678606,0.7666989351403679,0.7899322362052275,0.7909002904162633,0.8063891577928364,0.8151016456921588,0.8170377541142304,0.8238141335914811,0.8877057115198451,0.8906098741529526,0.9564375605033882,0.9603097773475314,0.9661181026137464,1],"yaxis":"y"}],"layout":{"height":600,"legend":{"tracegroupgap":0},"shapes":[{"line":{"dash":"dash"},"type":"line","x0":0,"x1":1,"y0":0,"y1":1}],"showlegend":false,"template":{"data":{"bar":[{"error_x":{"color":"#2a3f5f"},"error_y":{"color":"#2a3f5f"},"marker":{"line":{"color":"white","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"barpolar":[{"marker":{"line":{"color":"white","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"carpet":[{"aaxis":{"endlinecolor":"#2a3f5f","gridcolor":"#C8D4E3","linecolor":"#C8D4E3","minorgridcolor":"#C8D4E3","startlinecolor":"#2a3f5f"},"baxis":{"endlinecolor":"#2a3f5f","gridcolor":"#C8D4E3","linecolor":"#C8D4E3","minorgridcolor":"#C8D4E3","startlinecolor":"#2a3f5f"},"type":"carpet"}],"choropleth":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"choropleth"}],"contour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"contour"}],"contourcarpet":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"contourcarpet"}],"heatmap":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmap"}],"heatmapgl":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmapgl"}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"histogram2d":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2d"}],"histogram2dcontour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2dcontour"}],"mesh3d":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"mesh3d"}],"parcoords":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"parcoords"}],"pie":[{"automargin":true,"type":"pie"}],"scatter":[{"fillpattern":{"fillmode":"overlay","size":10,"solidity":0.2},"type":"scatter"}],"scatter3d":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter3d"}],"scattercarpet":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattercarpet"}],"scattergeo":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergeo"}],"scattergl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergl"}],"scattermapbox":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattermapbox"}],"scatterpolar":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolar"}],"scatterpolargl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolargl"}],"scatterternary":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterternary"}],"surface":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"surface"}],"table":[{"cells":{"fill":{"color":"#EBF0F8"},"line":{"color":"white"}},"header":{"fill":{"color":"#C8D4E3"},"line":{"color":"white"}},"type":"table"}]},"layout":{"annotationdefaults":{"arrowcolor":"#2a3f5f","arrowhead":0,"arrowwidth":1},"autotypenumbers":"strict","coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]],"sequential":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"sequentialminus":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]]},"colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#2a3f5f"},"geo":{"bgcolor":"white","lakecolor":"white","landcolor":"white","showlakes":true,"showland":true,"subunitcolor":"#C8D4E3"},"hoverlabel":{"align":"left"},"hovermode":"closest","mapbox":{"style":"light"},"paper_bgcolor":"white","plot_bgcolor":"white","polar":{"angularaxis":{"gridcolor":"#EBF0F8","linecolor":"#EBF0F8","ticks":""},"bgcolor":"white","radialaxis":{"gridcolor":"#EBF0F8","linecolor":"#EBF0F8","ticks":""}},"scene":{"xaxis":{"backgroundcolor":"white","gridcolor":"#DFE8F3","gridwidth":2,"linecolor":"#EBF0F8","showbackground":true,"ticks":"","zerolinecolor":"#EBF0F8"},"yaxis":{"backgroundcolor":"white","gridcolor":"#DFE8F3","gridwidth":2,"linecolor":"#EBF0F8","showbackground":true,"ticks":"","zerolinecolor":"#EBF0F8"},"zaxis":{"backgroundcolor":"white","gridcolor":"#DFE8F3","gridwidth":2,"linecolor":"#EBF0F8","showbackground":true,"ticks":"","zerolinecolor":"#EBF0F8"}},"shapedefaults":{"line":{"color":"#2a3f5f"}},"ternary":{"aaxis":{"gridcolor":"#DFE8F3","linecolor":"#A2B1C6","ticks":""},"baxis":{"gridcolor":"#DFE8F3","linecolor":"#A2B1C6","ticks":""},"bgcolor":"white","caxis":{"gridcolor":"#DFE8F3","linecolor":"#A2B1C6","ticks":""}},"title":{"x":0.05},"xaxis":{"automargin":true,"gridcolor":"#EBF0F8","linecolor":"#EBF0F8","ticks":"","title":{"standoff":15},"zerolinecolor":"#EBF0F8","zerolinewidth":2},"yaxis":{"automargin":true,"gridcolor":"#EBF0F8","linecolor":"#EBF0F8","ticks":"","title":{"standoff":15},"zerolinecolor":"#EBF0F8","zerolinewidth":2}}},"title":{"text":"ROC Curve (AUC=0.8431)"},"width":800,"xaxis":{"anchor":"y","constrain":"domain","domain":[0,1],"title":{"text":"False Positive Rate"}},"yaxis":{"anchor":"x","domain":[0,1],"scaleanchor":"x","scaleratio":1,"title":{"text":"True Positive Rate"}}}},"text/html":["<div>                            <div id=\"61f011ca-de54-468a-969c-18c84a6ca818\" class=\"plotly-graph-div\" style=\"height:600px; width:800px;\"></div>            <script type=\"text/javascript\">                require([\"plotly\"], function(Plotly) {                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"61f011ca-de54-468a-969c-18c84a6ca818\")) {                    Plotly.newPlot(                        \"61f011ca-de54-468a-969c-18c84a6ca818\",                        [{\"fillpattern\":{\"shape\":\"\"},\"hovertemplate\":\"False Positive Rate=%{x}\\u003cbr\\u003eTrue Positive Rate=%{y}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"\",\"line\":{\"color\":\"#636efa\"},\"marker\":{\"symbol\":\"circle\"},\"mode\":\"lines\",\"name\":\"\",\"orientation\":\"v\",\"showlegend\":false,\"stackgroup\":\"1\",\"x\":[0.0,0.034759358288770054,0.0374331550802139,0.0374331550802139,0.0427807486631016,0.0427807486631016,0.0427807486631016,0.0427807486631016,0.045454545454545456,0.045454545454545456,0.0481283422459893,0.0748663101604278,0.07754010695187166,0.08021390374331551,0.08288770053475936,0.08288770053475936,0.08288770053475936,0.09358288770053476,0.0962566844919786,0.10160427807486631,0.15775401069518716,0.16042780748663102,0.20320855614973263,0.24598930481283424,0.24598930481283424,0.2700534759358289,0.2727272727272727,0.2727272727272727,0.2807486631016043,0.44385026737967914,0.44385026737967914,0.6363636363636364,0.6390374331550802,0.6711229946524064,1.0],\"xaxis\":\"x\",\"y\":[0.0,0.24104549854791868,0.2846079380445305,0.32526621490803487,0.35237173281703776,0.3862536302032914,0.40658276863504356,0.409486931268151,0.4259438528557599,0.43756050338818975,0.4530493707647628,0.5333978702807357,0.5450145208131656,0.5701839303000968,0.5779283639883833,0.5914811229428848,0.5924491771539206,0.6050338818973863,0.6089060987415296,0.6176185866408519,0.6824782187802517,0.6989351403678606,0.7666989351403679,0.7899322362052275,0.7909002904162633,0.8063891577928364,0.8151016456921588,0.8170377541142304,0.8238141335914811,0.8877057115198451,0.8906098741529526,0.9564375605033882,0.9603097773475314,0.9661181026137464,1.0],\"yaxis\":\"y\",\"type\":\"scatter\"}],                        {\"template\":{\"data\":{\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"white\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"white\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"#C8D4E3\",\"linecolor\":\"#C8D4E3\",\"minorgridcolor\":\"#C8D4E3\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"#C8D4E3\",\"linecolor\":\"#C8D4E3\",\"minorgridcolor\":\"#C8D4E3\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"choropleth\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"choropleth\"}],\"contourcarpet\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"contourcarpet\"}],\"contour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"contour\"}],\"heatmapgl\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmapgl\"}],\"heatmap\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmap\"}],\"histogram2dcontour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2dcontour\"}],\"histogram2d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2d\"}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"mesh3d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"mesh3d\"}],\"parcoords\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"parcoords\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}],\"scatter3d\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatter3d\"}],\"scattercarpet\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattercarpet\"}],\"scattergeo\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergeo\"}],\"scattergl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergl\"}],\"scattermapbox\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattermapbox\"}],\"scatterpolargl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolargl\"}],\"scatterpolar\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolar\"}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"scatterternary\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterternary\"}],\"surface\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"surface\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}]},\"layout\":{\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"autotypenumbers\":\"strict\",\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]],\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]},\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"geo\":{\"bgcolor\":\"white\",\"lakecolor\":\"white\",\"landcolor\":\"white\",\"showlakes\":true,\"showland\":true,\"subunitcolor\":\"#C8D4E3\"},\"hoverlabel\":{\"align\":\"left\"},\"hovermode\":\"closest\",\"mapbox\":{\"style\":\"light\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"white\",\"polar\":{\"angularaxis\":{\"gridcolor\":\"#EBF0F8\",\"linecolor\":\"#EBF0F8\",\"ticks\":\"\"},\"bgcolor\":\"white\",\"radialaxis\":{\"gridcolor\":\"#EBF0F8\",\"linecolor\":\"#EBF0F8\",\"ticks\":\"\"}},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"white\",\"gridcolor\":\"#DFE8F3\",\"gridwidth\":2,\"linecolor\":\"#EBF0F8\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"#EBF0F8\"},\"yaxis\":{\"backgroundcolor\":\"white\",\"gridcolor\":\"#DFE8F3\",\"gridwidth\":2,\"linecolor\":\"#EBF0F8\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"#EBF0F8\"},\"zaxis\":{\"backgroundcolor\":\"white\",\"gridcolor\":\"#DFE8F3\",\"gridwidth\":2,\"linecolor\":\"#EBF0F8\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"#EBF0F8\"}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"ternary\":{\"aaxis\":{\"gridcolor\":\"#DFE8F3\",\"linecolor\":\"#A2B1C6\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"#DFE8F3\",\"linecolor\":\"#A2B1C6\",\"ticks\":\"\"},\"bgcolor\":\"white\",\"caxis\":{\"gridcolor\":\"#DFE8F3\",\"linecolor\":\"#A2B1C6\",\"ticks\":\"\"}},\"title\":{\"x\":0.05},\"xaxis\":{\"automargin\":true,\"gridcolor\":\"#EBF0F8\",\"linecolor\":\"#EBF0F8\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"#EBF0F8\",\"zerolinewidth\":2},\"yaxis\":{\"automargin\":true,\"gridcolor\":\"#EBF0F8\",\"linecolor\":\"#EBF0F8\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"#EBF0F8\",\"zerolinewidth\":2}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"False Positive Rate\"},\"constrain\":\"domain\"},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"True Positive Rate\"},\"scaleanchor\":\"x\",\"scaleratio\":1},\"legend\":{\"tracegroupgap\":0},\"title\":{\"text\":\"ROC Curve (AUC=0.8431)\"},\"height\":600,\"width\":800,\"shapes\":[{\"line\":{\"dash\":\"dash\"},\"type\":\"line\",\"x0\":0,\"x1\":1,\"y0\":0,\"y1\":1}],\"showlegend\":false},                        {\"responsive\": true}                    ).then(function(){\n","                            \n","var gd = document.getElementById('61f011ca-de54-468a-969c-18c84a6ca818');\n","var x = new MutationObserver(function (mutations, observer) {{\n","        var display = window.getComputedStyle(gd).display;\n","        if (!display || display === 'none') {{\n","            console.log([gd, 'removed!']);\n","            Plotly.purge(gd);\n","            observer.disconnect();\n","        }}\n","}});\n","\n","// Listen for the removal of the full notebook cells\n","var notebookContainer = gd.closest('#notebook-container');\n","if (notebookContainer) {{\n","    x.observe(notebookContainer, {childList: true});\n","}}\n","\n","// Listen for the clearing of the current output cell\n","var outputEl = gd.closest('.output');\n","if (outputEl) {{\n","    x.observe(outputEl, {childList: true});\n","}}\n","\n","                        })                };                });            </script>        </div>"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.plotly.v1+json":{"config":{"plotlyServerURL":"https://plot.ly"},"data":[{"fillpattern":{"shape":""},"hovertemplate":"Recall=%{x}<br>Precision=%{y}<extra></extra>","legendgroup":"","line":{"color":"#636efa"},"marker":{"symbol":"circle"},"mode":"lines","name":"","orientation":"v","showlegend":false,"stackgroup":"1","type":"scatter","x":[1,0.9661181026137464,0.9603097773475314,0.9564375605033882,0.8906098741529526,0.8877057115198451,0.8238141335914811,0.8170377541142304,0.8151016456921588,0.8063891577928364,0.7909002904162633,0.7899322362052275,0.7666989351403679,0.6989351403678606,0.6824782187802517,0.6176185866408519,0.6089060987415296,0.6050338818973863,0.5924491771539206,0.5914811229428848,0.5779283639883833,0.5701839303000968,0.5450145208131656,0.5333978702807357,0.4530493707647628,0.43756050338818975,0.4259438528557599,0.409486931268151,0.40658276863504356,0.3862536302032914,0.35237173281703776,0.32526621490803487,0.2846079380445305,0.24104549854791868,0],"xaxis":"x","y":[0.7341862117981521,0.7990392313851081,0.8058489033306255,0.8058727569331158,0.8471454880294659,0.8467220683287165,0.8901673640167364,0.8921775898520085,0.8919491525423728,0.8918629550321199,0.8987898789878987,0.8986784140969163,0.9124423963133641,0.9232736572890026,0.9227748691099477,0.9437869822485208,0.9458646616541353,0.946969696969697,0.9517884914463453,0.9517133956386293,0.9506369426751592,0.9515347334410339,0.9510135135135135,0.9516407599309153,0.9629629629629629,0.9637526652452025,0.962800875273523,0.9635535307517085,0.963302752293578,0.9614457831325302,0.9578947368421052,0.96,0.9545454545454546,0.950381679389313,1],"yaxis":"y"}],"layout":{"height":600,"legend":{"tracegroupgap":0},"shapes":[{"line":{"dash":"dash"},"type":"line","x0":0,"x1":1,"y0":1,"y1":0}],"showlegend":false,"template":{"data":{"bar":[{"error_x":{"color":"#2a3f5f"},"error_y":{"color":"#2a3f5f"},"marker":{"line":{"color":"white","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"barpolar":[{"marker":{"line":{"color":"white","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"carpet":[{"aaxis":{"endlinecolor":"#2a3f5f","gridcolor":"#C8D4E3","linecolor":"#C8D4E3","minorgridcolor":"#C8D4E3","startlinecolor":"#2a3f5f"},"baxis":{"endlinecolor":"#2a3f5f","gridcolor":"#C8D4E3","linecolor":"#C8D4E3","minorgridcolor":"#C8D4E3","startlinecolor":"#2a3f5f"},"type":"carpet"}],"choropleth":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"choropleth"}],"contour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"contour"}],"contourcarpet":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"contourcarpet"}],"heatmap":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmap"}],"heatmapgl":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmapgl"}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"histogram2d":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2d"}],"histogram2dcontour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2dcontour"}],"mesh3d":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"mesh3d"}],"parcoords":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"parcoords"}],"pie":[{"automargin":true,"type":"pie"}],"scatter":[{"fillpattern":{"fillmode":"overlay","size":10,"solidity":0.2},"type":"scatter"}],"scatter3d":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter3d"}],"scattercarpet":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattercarpet"}],"scattergeo":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergeo"}],"scattergl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergl"}],"scattermapbox":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattermapbox"}],"scatterpolar":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolar"}],"scatterpolargl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolargl"}],"scatterternary":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterternary"}],"surface":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"surface"}],"table":[{"cells":{"fill":{"color":"#EBF0F8"},"line":{"color":"white"}},"header":{"fill":{"color":"#C8D4E3"},"line":{"color":"white"}},"type":"table"}]},"layout":{"annotationdefaults":{"arrowcolor":"#2a3f5f","arrowhead":0,"arrowwidth":1},"autotypenumbers":"strict","coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]],"sequential":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"sequentialminus":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]]},"colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#2a3f5f"},"geo":{"bgcolor":"white","lakecolor":"white","landcolor":"white","showlakes":true,"showland":true,"subunitcolor":"#C8D4E3"},"hoverlabel":{"align":"left"},"hovermode":"closest","mapbox":{"style":"light"},"paper_bgcolor":"white","plot_bgcolor":"white","polar":{"angularaxis":{"gridcolor":"#EBF0F8","linecolor":"#EBF0F8","ticks":""},"bgcolor":"white","radialaxis":{"gridcolor":"#EBF0F8","linecolor":"#EBF0F8","ticks":""}},"scene":{"xaxis":{"backgroundcolor":"white","gridcolor":"#DFE8F3","gridwidth":2,"linecolor":"#EBF0F8","showbackground":true,"ticks":"","zerolinecolor":"#EBF0F8"},"yaxis":{"backgroundcolor":"white","gridcolor":"#DFE8F3","gridwidth":2,"linecolor":"#EBF0F8","showbackground":true,"ticks":"","zerolinecolor":"#EBF0F8"},"zaxis":{"backgroundcolor":"white","gridcolor":"#DFE8F3","gridwidth":2,"linecolor":"#EBF0F8","showbackground":true,"ticks":"","zerolinecolor":"#EBF0F8"}},"shapedefaults":{"line":{"color":"#2a3f5f"}},"ternary":{"aaxis":{"gridcolor":"#DFE8F3","linecolor":"#A2B1C6","ticks":""},"baxis":{"gridcolor":"#DFE8F3","linecolor":"#A2B1C6","ticks":""},"bgcolor":"white","caxis":{"gridcolor":"#DFE8F3","linecolor":"#A2B1C6","ticks":""}},"title":{"x":0.05},"xaxis":{"automargin":true,"gridcolor":"#EBF0F8","linecolor":"#EBF0F8","ticks":"","title":{"standoff":15},"zerolinecolor":"#EBF0F8","zerolinewidth":2},"yaxis":{"automargin":true,"gridcolor":"#EBF0F8","linecolor":"#EBF0F8","ticks":"","title":{"standoff":15},"zerolinecolor":"#EBF0F8","zerolinewidth":2}}},"title":{"text":"Precision-Recall Curve (AUC=0.8431)"},"width":800,"xaxis":{"anchor":"y","constrain":"domain","domain":[0,1],"title":{"text":"Recall"}},"yaxis":{"anchor":"x","domain":[0,1],"scaleanchor":"x","scaleratio":1,"title":{"text":"Precision"}}}},"text/html":["<div>                            <div id=\"2c2b0d2e-df18-4609-aa33-5c5ab4caff80\" class=\"plotly-graph-div\" style=\"height:600px; width:800px;\"></div>            <script type=\"text/javascript\">                require([\"plotly\"], function(Plotly) {                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"2c2b0d2e-df18-4609-aa33-5c5ab4caff80\")) {                    Plotly.newPlot(                        \"2c2b0d2e-df18-4609-aa33-5c5ab4caff80\",                        [{\"fillpattern\":{\"shape\":\"\"},\"hovertemplate\":\"Recall=%{x}\\u003cbr\\u003ePrecision=%{y}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"\",\"line\":{\"color\":\"#636efa\"},\"marker\":{\"symbol\":\"circle\"},\"mode\":\"lines\",\"name\":\"\",\"orientation\":\"v\",\"showlegend\":false,\"stackgroup\":\"1\",\"x\":[1.0,0.9661181026137464,0.9603097773475314,0.9564375605033882,0.8906098741529526,0.8877057115198451,0.8238141335914811,0.8170377541142304,0.8151016456921588,0.8063891577928364,0.7909002904162633,0.7899322362052275,0.7666989351403679,0.6989351403678606,0.6824782187802517,0.6176185866408519,0.6089060987415296,0.6050338818973863,0.5924491771539206,0.5914811229428848,0.5779283639883833,0.5701839303000968,0.5450145208131656,0.5333978702807357,0.4530493707647628,0.43756050338818975,0.4259438528557599,0.409486931268151,0.40658276863504356,0.3862536302032914,0.35237173281703776,0.32526621490803487,0.2846079380445305,0.24104549854791868,0.0],\"xaxis\":\"x\",\"y\":[0.7341862117981521,0.7990392313851081,0.8058489033306255,0.8058727569331158,0.8471454880294659,0.8467220683287165,0.8901673640167364,0.8921775898520085,0.8919491525423728,0.8918629550321199,0.8987898789878987,0.8986784140969163,0.9124423963133641,0.9232736572890026,0.9227748691099477,0.9437869822485208,0.9458646616541353,0.946969696969697,0.9517884914463453,0.9517133956386293,0.9506369426751592,0.9515347334410339,0.9510135135135135,0.9516407599309153,0.9629629629629629,0.9637526652452025,0.962800875273523,0.9635535307517085,0.963302752293578,0.9614457831325302,0.9578947368421052,0.96,0.9545454545454546,0.950381679389313,1.0],\"yaxis\":\"y\",\"type\":\"scatter\"}],                        {\"template\":{\"data\":{\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"white\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"white\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"#C8D4E3\",\"linecolor\":\"#C8D4E3\",\"minorgridcolor\":\"#C8D4E3\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"#C8D4E3\",\"linecolor\":\"#C8D4E3\",\"minorgridcolor\":\"#C8D4E3\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"choropleth\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"choropleth\"}],\"contourcarpet\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"contourcarpet\"}],\"contour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"contour\"}],\"heatmapgl\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmapgl\"}],\"heatmap\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmap\"}],\"histogram2dcontour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2dcontour\"}],\"histogram2d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2d\"}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"mesh3d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"mesh3d\"}],\"parcoords\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"parcoords\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}],\"scatter3d\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatter3d\"}],\"scattercarpet\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattercarpet\"}],\"scattergeo\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergeo\"}],\"scattergl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergl\"}],\"scattermapbox\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattermapbox\"}],\"scatterpolargl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolargl\"}],\"scatterpolar\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolar\"}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"scatterternary\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterternary\"}],\"surface\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"surface\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}]},\"layout\":{\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"autotypenumbers\":\"strict\",\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]],\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]},\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"geo\":{\"bgcolor\":\"white\",\"lakecolor\":\"white\",\"landcolor\":\"white\",\"showlakes\":true,\"showland\":true,\"subunitcolor\":\"#C8D4E3\"},\"hoverlabel\":{\"align\":\"left\"},\"hovermode\":\"closest\",\"mapbox\":{\"style\":\"light\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"white\",\"polar\":{\"angularaxis\":{\"gridcolor\":\"#EBF0F8\",\"linecolor\":\"#EBF0F8\",\"ticks\":\"\"},\"bgcolor\":\"white\",\"radialaxis\":{\"gridcolor\":\"#EBF0F8\",\"linecolor\":\"#EBF0F8\",\"ticks\":\"\"}},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"white\",\"gridcolor\":\"#DFE8F3\",\"gridwidth\":2,\"linecolor\":\"#EBF0F8\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"#EBF0F8\"},\"yaxis\":{\"backgroundcolor\":\"white\",\"gridcolor\":\"#DFE8F3\",\"gridwidth\":2,\"linecolor\":\"#EBF0F8\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"#EBF0F8\"},\"zaxis\":{\"backgroundcolor\":\"white\",\"gridcolor\":\"#DFE8F3\",\"gridwidth\":2,\"linecolor\":\"#EBF0F8\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"#EBF0F8\"}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"ternary\":{\"aaxis\":{\"gridcolor\":\"#DFE8F3\",\"linecolor\":\"#A2B1C6\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"#DFE8F3\",\"linecolor\":\"#A2B1C6\",\"ticks\":\"\"},\"bgcolor\":\"white\",\"caxis\":{\"gridcolor\":\"#DFE8F3\",\"linecolor\":\"#A2B1C6\",\"ticks\":\"\"}},\"title\":{\"x\":0.05},\"xaxis\":{\"automargin\":true,\"gridcolor\":\"#EBF0F8\",\"linecolor\":\"#EBF0F8\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"#EBF0F8\",\"zerolinewidth\":2},\"yaxis\":{\"automargin\":true,\"gridcolor\":\"#EBF0F8\",\"linecolor\":\"#EBF0F8\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"#EBF0F8\",\"zerolinewidth\":2}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"Recall\"},\"constrain\":\"domain\"},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"Precision\"},\"scaleanchor\":\"x\",\"scaleratio\":1},\"legend\":{\"tracegroupgap\":0},\"title\":{\"text\":\"Precision-Recall Curve (AUC=0.8431)\"},\"height\":600,\"width\":800,\"shapes\":[{\"line\":{\"dash\":\"dash\"},\"type\":\"line\",\"x0\":0,\"x1\":1,\"y0\":1,\"y1\":0}],\"showlegend\":false},                        {\"responsive\": true}                    ).then(function(){\n","                            \n","var gd = document.getElementById('2c2b0d2e-df18-4609-aa33-5c5ab4caff80');\n","var x = new MutationObserver(function (mutations, observer) {{\n","        var display = window.getComputedStyle(gd).display;\n","        if (!display || display === 'none') {{\n","            console.log([gd, 'removed!']);\n","            Plotly.purge(gd);\n","            observer.disconnect();\n","        }}\n","}});\n","\n","// Listen for the removal of the full notebook cells\n","var notebookContainer = gd.closest('#notebook-container');\n","if (notebookContainer) {{\n","    x.observe(notebookContainer, {childList: true});\n","}}\n","\n","// Listen for the clearing of the current output cell\n","var outputEl = gd.closest('.output');\n","if (outputEl) {{\n","    x.observe(outputEl, {childList: true});\n","}}\n","\n","                        })                };                });            </script>        </div>"]},"metadata":{},"output_type":"display_data"}],"source":["target_score = tree_clf.predict_proba(features_valid)[:, 1]\n","\n","fpr, tpr, thresholds = roc_curve(target_valid, target_score)\n","\n","fig = px.area(\n","    x=fpr, y=tpr,\n","    title=f'ROC Curve (AUC={auc(fpr, tpr):.4f})',\n","    labels=dict(x='False Positive Rate', y='True Positive Rate'),\n","    width=800, height=600\n",")\n","fig.add_shape(\n","    type='line', line=dict(dash='dash'),\n","    x0=0, x1=1, y0=0, y1=1\n",")\n","\n","fig.update_yaxes(scaleanchor=\"x\", scaleratio=1)\n","fig.update_xaxes(constrain='domain')\n","fig.update_layout(showlegend=False)\n","fig.show()\n","\n","precision, recall, thresholds = precision_recall_curve(target_valid, target_score)\n","\n","fig = px.area(\n","    x=recall, y=precision,\n","    title=f'Precision-Recall Curve (AUC={auc(fpr, tpr):.4f})',\n","    labels=dict(x='Recall', y='Precision'),\n","    width=800, height=600\n",")\n","fig.add_shape(\n","    type='line', line=dict(dash='dash'),\n","    x0=0, x1=1, y0=1, y1=0\n",")\n","fig.update_yaxes(scaleanchor=\"x\", scaleratio=1)\n","fig.update_xaxes(constrain='domain')\n","fig.update_layout(showlegend=False)\n","fig.show()"]},{"cell_type":"markdown","metadata":{},"source":["-----------"]},{"cell_type":"markdown","metadata":{},"source":["# Extra Trees"]},{"cell_type":"code","execution_count":118,"metadata":{"execution":{"iopub.execute_input":"2023-11-30T16:51:35.225674Z","iopub.status.busy":"2023-11-30T16:51:35.225423Z","iopub.status.idle":"2023-11-30T16:52:58.400487Z","shell.execute_reply":"2023-11-30T16:52:58.399533Z","shell.execute_reply.started":"2023-11-30T16:51:35.225641Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Runtime:\n","CPU times: user 929 ms, sys: 403 ms, total: 1.33 s\n","Wall time: 21.5 s\n"]}],"source":["%%time\n","extra_trees_model = ExtraTreesClassifier(random_state=random_state)\n","extra_trees_parameters = [{'max_depth': [2,6,8,12,18,30],\n","                     'min_samples_split': [2,6,8,12],\n","                     \"criterion\": ['gini', 'entropy', 'log_loss'],\n","                     \"warm_start\": [True, False],\n","                     'n_estimators': [50,100,200]}]\n","\n","extra_trees_clf = RandomizedSearchCV(extra_trees_model, extra_trees_parameters, scoring='roc_auc', n_jobs=-1, cv=cv)\n","extra_trees_clf.fit(features_train, target_train)\n","# create a variable for the best model\n","best_ext = extra_trees_clf.best_estimator_\n","ext_pred = best_ext.predict(features_valid)\n","print('Runtime:')"]},{"cell_type":"code","execution_count":119,"metadata":{"execution":{"iopub.execute_input":"2023-11-30T16:52:58.402048Z","iopub.status.busy":"2023-11-30T16:52:58.401764Z","iopub.status.idle":"2023-11-30T16:53:23.273446Z","shell.execute_reply":"2023-11-30T16:53:23.272464Z","shell.execute_reply.started":"2023-11-30T16:52:58.402022Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Cross Validation Scores: [0.86500845 0.83686636 0.85711406 0.82880184 0.84677419 0.84474366\n"," 0.8422235  0.8139977  0.87829404 0.86738615 0.8049947  0.83702477\n"," 0.86673387 0.8265553  0.86496256 0.83313652 0.86828917 0.83366935\n"," 0.8825994  0.83418574 0.83532377 0.86350806 0.86647465 0.85596198\n"," 0.83706797 0.87138537 0.86238479 0.8671803  0.83862113 0.79510518\n"," 0.84864106 0.82436636 0.8405962  0.86353687 0.86965726 0.85709965\n"," 0.84448445 0.85020161 0.85273636 0.85234628 0.85839276 0.85734447\n"," 0.82521601 0.82070853 0.83175403 0.84179147 0.85817972 0.86784274\n"," 0.8469718  0.86162159]\n"]}],"source":["extra_trees_scores = cross_val_score(extra_trees_model, features_train, target_train, cv=cv, scoring='roc_auc')\n","print('Cross Validation Scores: {}'.format(extra_trees_scores))"]},{"cell_type":"code","execution_count":120,"metadata":{"execution":{"iopub.execute_input":"2023-11-30T16:53:23.274951Z","iopub.status.busy":"2023-11-30T16:53:23.274646Z","iopub.status.idle":"2023-11-30T16:53:23.354259Z","shell.execute_reply":"2023-11-30T16:53:23.353326Z","shell.execute_reply.started":"2023-11-30T16:53:23.274927Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Best hyperparameters: {'warm_start': True, 'n_estimators': 200, 'min_samples_split': 12, 'max_depth': 30, 'criterion': 'entropy'}\n","\n","Best score: 0.856481014457062\n","\n","Average Cross Validation Score: 0.8479972754101266\n","\n","ROC AUC Score - Validation Dataset: 0.8840146812927405\n"]}],"source":["# summary\n","print('Best hyperparameters:',  extra_trees_clf.best_params_)\n","print()\n","print('Best score:',  extra_trees_clf.best_score_)\n","print()\n","print('Average Cross Validation Score: {}'.format(extra_trees_scores.mean()))\n","print()\n","print('ROC AUC Score - Validation Dataset:',  roc_auc_score(target_valid, extra_trees_clf.predict_proba(features_valid)[:, 1]))"]},{"cell_type":"markdown","metadata":{},"source":["# ROC AUC Curve - ExtraTreesClassifier"]},{"cell_type":"code","execution_count":121,"metadata":{"execution":{"iopub.execute_input":"2023-11-30T16:53:23.356396Z","iopub.status.busy":"2023-11-30T16:53:23.356005Z","iopub.status.idle":"2023-11-30T16:53:23.560598Z","shell.execute_reply":"2023-11-30T16:53:23.559701Z","shell.execute_reply.started":"2023-11-30T16:53:23.356359Z"},"trusted":true},"outputs":[{"data":{"application/vnd.plotly.v1+json":{"config":{"plotlyServerURL":"https://plot.ly"},"data":[{"fillpattern":{"shape":""},"hovertemplate":"False Positive Rate=%{x}<br>True Positive Rate=%{y}<extra></extra>","legendgroup":"","line":{"color":"#636efa"},"marker":{"symbol":"circle"},"mode":"lines","name":"","orientation":"v","showlegend":false,"stackgroup":"1","type":"scatter","x":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.00267379679144385,0.00267379679144385,0.0053475935828877,0.0053475935828877,0.008021390374331552,0.008021390374331552,0.0106951871657754,0.0106951871657754,0.013368983957219251,0.013368983957219251,0.016042780748663103,0.016042780748663103,0.01871657754010695,0.01871657754010695,0.0213903743315508,0.0213903743315508,0.02406417112299465,0.02406417112299465,0.026737967914438502,0.026737967914438502,0.029411764705882353,0.029411764705882353,0.03208556149732621,0.03208556149732621,0.034759358288770054,0.034759358288770054,0.0374331550802139,0.0374331550802139,0.040106951871657755,0.040106951871657755,0.0427807486631016,0.0427807486631016,0.045454545454545456,0.045454545454545456,0.0481283422459893,0.0481283422459893,0.05080213903743316,0.05080213903743316,0.053475935828877004,0.053475935828877004,0.058823529411764705,0.058823529411764705,0.06149732620320856,0.06149732620320856,0.06684491978609626,0.06684491978609626,0.06951871657754011,0.06951871657754011,0.07219251336898395,0.07219251336898395,0.0748663101604278,0.0748663101604278,0.07754010695187166,0.07754010695187166,0.0855614973262032,0.0855614973262032,0.08823529411764706,0.08823529411764706,0.0962566844919786,0.0962566844919786,0.09893048128342247,0.09893048128342247,0.10160427807486631,0.10160427807486631,0.10427807486631016,0.10427807486631016,0.10695187165775401,0.10695187165775401,0.10962566844919786,0.10962566844919786,0.11229946524064172,0.11229946524064172,0.11497326203208556,0.11497326203208556,0.11764705882352941,0.11764705882352941,0.12032085561497326,0.12032085561497326,0.12299465240641712,0.12299465240641712,0.12566844919786097,0.12566844919786097,0.12834224598930483,0.12834224598930483,0.13101604278074866,0.13101604278074866,0.13368983957219252,0.13368983957219252,0.13636363636363635,0.13636363636363635,0.13903743315508021,0.13903743315508021,0.14171122994652408,0.14171122994652408,0.1443850267379679,0.1443850267379679,0.14705882352941177,0.14705882352941177,0.1497326203208556,0.1497326203208556,0.15240641711229946,0.15240641711229946,0.15508021390374332,0.15508021390374332,0.15775401069518716,0.15775401069518716,0.16042780748663102,0.16042780748663102,0.1657754010695187,0.1657754010695187,0.16844919786096257,0.16844919786096257,0.17379679144385027,0.17379679144385027,0.17647058823529413,0.17647058823529413,0.17914438502673796,0.17914438502673796,0.18449197860962566,0.18449197860962566,0.18716577540106952,0.18716577540106952,0.18983957219251338,0.18983957219251338,0.1925133689839572,0.1925133689839572,0.19518716577540107,0.19518716577540107,0.19786096256684493,0.19786096256684493,0.20053475935828877,0.20053475935828877,0.20320855614973263,0.20320855614973263,0.20855614973262032,0.20855614973262032,0.21122994652406418,0.21122994652406418,0.21657754010695188,0.21657754010695188,0.2192513368983957,0.2192513368983957,0.22192513368983957,0.22192513368983957,0.22994652406417113,0.22994652406417113,0.232620320855615,0.232620320855615,0.23529411764705882,0.23529411764705882,0.23796791443850268,0.23796791443850268,0.24064171122994651,0.24064171122994651,0.24331550802139038,0.24331550802139038,0.24866310160427807,0.24866310160427807,0.25133689839572193,0.25133689839572193,0.2540106951871658,0.2540106951871658,0.25935828877005346,0.25935828877005346,0.2620320855614973,0.2620320855614973,0.27540106951871657,0.27540106951871657,0.27807486631016043,0.27807486631016043,0.2807486631016043,0.2807486631016043,0.28342245989304815,0.28342245989304815,0.28609625668449196,0.28609625668449196,0.2887700534759358,0.2887700534759358,0.2914438502673797,0.2914438502673797,0.29411764705882354,0.29411764705882354,0.30213903743315507,0.30213903743315507,0.3048128342245989,0.3048128342245989,0.31283422459893045,0.31283422459893045,0.3155080213903743,0.3155080213903743,0.32085561497326204,0.32085561497326204,0.32620320855614976,0.32620320855614976,0.32887700534759357,0.32887700534759357,0.3342245989304813,0.3342245989304813,0.33689839572192515,0.33689839572192515,0.339572192513369,0.339572192513369,0.3422459893048128,0.3422459893048128,0.3449197860962567,0.3449197860962567,0.34759358288770054,0.34759358288770054,0.3582887700534759,0.3582887700534759,0.3663101604278075,0.3663101604278075,0.3689839572192513,0.3689839572192513,0.37433155080213903,0.37433155080213903,0.37967914438502676,0.37967914438502676,0.38235294117647056,0.38235294117647056,0.39037433155080214,0.39037433155080214,0.39572192513368987,0.39572192513368987,0.40106951871657753,0.40106951871657753,0.4117647058823529,0.4117647058823529,0.4144385026737968,0.4144385026737968,0.41711229946524064,0.41711229946524064,0.42245989304812837,0.42245989304812837,0.42513368983957217,0.42513368983957217,0.42780748663101603,0.42780748663101603,0.43315508021390375,0.43315508021390375,0.4358288770053476,0.4358288770053476,0.44919786096256686,0.44919786096256686,0.45187165775401067,0.45187165775401067,0.4679144385026738,0.4679144385026738,0.4732620320855615,0.4732620320855615,0.4946524064171123,0.4946524064171123,0.49732620320855614,0.49732620320855614,0.5026737967914439,0.5026737967914439,0.5133689839572193,0.5133689839572193,0.5187165775401069,0.5187165775401069,0.5213903743315508,0.5213903743315508,0.5267379679144385,0.5267379679144385,0.5320855614973262,0.5320855614973262,0.5347593582887701,0.5347593582887701,0.5374331550802139,0.5374331550802139,0.5454545454545454,0.5454545454545454,0.5614973262032086,0.5614973262032086,0.5641711229946524,0.5641711229946524,0.56951871657754,0.56951871657754,0.5775401069518716,0.5775401069518716,0.5882352941176471,0.5882352941176471,0.5909090909090909,0.5909090909090909,0.6042780748663101,0.6042780748663101,0.6096256684491979,0.6096256684491979,0.6149732620320856,0.6149732620320856,0.6229946524064172,0.6229946524064172,0.6363636363636364,0.6363636363636364,0.6470588235294118,0.6470588235294118,0.660427807486631,0.660427807486631,0.6657754010695187,0.6657754010695187,0.6684491978609626,0.6684491978609626,0.6951871657754011,0.6951871657754011,0.7165775401069518,0.7165775401069518,0.7299465240641712,0.7299465240641712,0.7352941176470589,0.7352941176470589,0.7566844919786097,0.7566844919786097,0.7754010695187166,0.7754010695187166,0.7887700534759359,0.7887700534759359,0.7994652406417112,0.7994652406417112,0.839572192513369,0.839572192513369,0.8556149732620321,0.8556149732620321,0.8796791443850267,0.8796791443850267,0.9117647058823529,0.9117647058823529,1],"xaxis":"x","y":[0,0.00968054211035818,0.013552758954501452,0.016456921587608905,0.01936108422071636,0.021297192642787996,0.02420135527589545,0.026137463697967087,0.02904162633107454,0.030977734753146177,0.03388189738625363,0.0377541142303969,0.1181026137463698,0.12003872216844143,0.1345595353339787,0.1345595353339787,0.1403678606001936,0.1403678606001936,0.1713455953533398,0.1713455953533398,0.18489835430784124,0.18489835430784124,0.23910939012584706,0.23910939012584706,0.26524685382381413,0.26524685382381413,0.2933204259438529,0.2933204259438529,0.356243949661181,0.356243949661181,0.3591481122942885,0.3591481122942885,0.3639883833494676,0.3639883833494676,0.36786060019361083,0.36786060019361083,0.41045498547918685,0.41045498547918685,0.4230396902226525,0.4230396902226525,0.4491771539206196,0.4491771539206196,0.46563407550822844,0.46563407550822844,0.4898354307841239,0.4898354307841239,0.49080348499515974,0.49080348499515974,0.5024201355275896,0.5024201355275896,0.5091965150048403,0.5091965150048403,0.5150048402710552,0.5150048402710552,0.5198451113262342,0.5198451113262342,0.5333978702807357,0.5333978702807357,0.5488867376573088,0.5488867376573088,0.5546950629235237,0.5546950629235237,0.5556631171345595,0.5556631171345595,0.5779283639883833,0.5779283639883833,0.579864472410455,0.579864472410455,0.5943852855759922,0.5943852855759922,0.5953533397870281,0.5953533397870281,0.6176185866408519,0.6176185866408519,0.6253630203291385,0.6253630203291385,0.6272991287512101,0.6272991287512101,0.6282671829622459,0.6282671829622459,0.6350435624394967,0.6350435624394967,0.6447241045498547,0.6447241045498547,0.6534365924491772,0.6534365924491772,0.6544046466602129,0.6544046466602129,0.665053242981607,0.665053242981607,0.6679574056147144,0.6679574056147144,0.6786060019361084,0.6786060019361084,0.6815101645692159,0.6815101645692159,0.7008712487899322,0.7008712487899322,0.7115198451113263,0.7115198451113263,0.7221684414327202,0.7221684414327202,0.7347531461761858,0.7347531461761858,0.7357212003872217,0.7357212003872217,0.7424975798644724,0.7424975798644724,0.7473378509196515,0.7473378509196515,0.755082284607938,0.755082284607938,0.7560503388189739,0.7560503388189739,0.7570183930300097,0.7570183930300097,0.7599225556631172,0.7599225556631172,0.7608906098741529,0.7608906098741529,0.7618586640851888,0.7618586640851888,0.7628267182962246,0.7628267182962246,0.7666989351403679,0.7666989351403679,0.7696030977734754,0.7696030977734754,0.7705711519845111,0.7705711519845111,0.771539206195547,0.771539206195547,0.7744433688286544,0.7744433688286544,0.7754114230396902,0.7754114230396902,0.7812197483059051,0.7812197483059051,0.782187802516941,0.782187802516941,0.7850919651500484,0.7850919651500484,0.7957405614714425,0.7957405614714425,0.7996127783155856,0.7996127783155856,0.8005808325266215,0.8005808325266215,0.8034849951597289,0.8034849951597289,0.8063891577928364,0.8063891577928364,0.8092933204259438,0.8092933204259438,0.8218780251694094,0.8218780251694094,0.8257502420135527,0.8257502420135527,0.8305905130687319,0.8305905130687319,0.8315585672797676,0.8315585672797676,0.8344627299128751,0.8344627299128751,0.8422071636011617,0.8422071636011617,0.8441432720232332,0.8441432720232332,0.8470474346563408,0.8470474346563408,0.8518877057115198,0.8518877057115198,0.8538238141335914,0.8538238141335914,0.8576960309777347,0.8576960309777347,0.8596321393998064,0.8596321393998064,0.8606001936108422,0.8606001936108422,0.8654404646660213,0.8654404646660213,0.8664085188770572,0.8664085188770572,0.8683446272991288,0.8683446272991288,0.8702807357212003,0.8702807357212003,0.8760890609874153,0.8760890609874153,0.8780251694094869,0.8780251694094869,0.8789932236205228,0.8789932236205228,0.8799612778315585,0.8799612778315585,0.882865440464666,0.882865440464666,0.8867376573088093,0.8867376573088093,0.8877057115198451,0.8877057115198451,0.8896418199419167,0.8896418199419167,0.89351403678606,0.89351403678606,0.9002904162633107,0.9002904162633107,0.9022265246853823,0.9022265246853823,0.9031945788964182,0.9031945788964182,0.9051306873184899,0.9051306873184899,0.9070667957405615,0.9070667957405615,0.9080348499515973,0.9080348499515973,0.9157792836398838,0.9157792836398838,0.920619554695063,0.920619554695063,0.9215876089060987,0.9215876089060987,0.9235237173281704,0.9235237173281704,0.9273959341723137,0.9273959341723137,0.9283639883833494,0.9283639883833494,0.9293320425943853,0.9293320425943853,0.9303000968054211,0.9303000968054211,0.9312681510164569,0.9312681510164569,0.9322362052274927,0.9322362052274927,0.9341723136495643,0.9341723136495643,0.936108422071636,0.936108422071636,0.9370764762826719,0.9370764762826719,0.9390125847047435,0.9390125847047435,0.9419167473378509,0.9419167473378509,0.9428848015488868,0.9428848015488868,0.9438528557599225,0.9438528557599225,0.9448209099709584,0.9448209099709584,0.9467570183930301,0.9467570183930301,0.9486931268151017,0.9486931268151017,0.9496611810261375,0.9496611810261375,0.9506292352371732,0.9506292352371732,0.9515972894482091,0.9515972894482091,0.952565343659245,0.952565343659245,0.9545014520813165,0.9545014520813165,0.9554695062923524,0.9554695062923524,0.9564375605033882,0.9564375605033882,0.9603097773475314,0.9603097773475314,0.9612778315585673,0.9612778315585673,0.9622458857696031,0.9622458857696031,0.9632139399806389,0.9632139399806389,0.9661181026137464,0.9661181026137464,0.968054211035818,0.968054211035818,0.9690222652468539,0.9690222652468539,0.9699903194578896,0.9699903194578896,0.9719264278799613,0.9719264278799613,0.972894482090997,0.972894482090997,0.9738625363020329,0.9738625363020329,0.9748305905130688,0.9748305905130688,0.9757986447241046,0.9757986447241046,0.9767666989351403,0.9767666989351403,0.9777347531461762,0.9777347531461762,0.978702807357212,0.978702807357212,0.9806389157792836,0.9806389157792836,0.9816069699903195,0.9816069699903195,0.9825750242013552,0.9825750242013552,0.9835430784123911,0.9835430784123911,0.9845111326234269,0.9845111326234269,0.9854791868344628,0.9854791868344628,0.9864472410454985,0.9864472410454985,0.9874152952565344,0.9874152952565344,0.9883833494675702,0.9883833494675702,0.989351403678606,0.989351403678606,0.9903194578896418,0.9903194578896418,0.9912875121006777,0.9912875121006777,0.9932236205227493,0.9932236205227493,0.9941916747337851,0.9941916747337851,0.995159728944821,0.995159728944821,0.9961277831558567,0.9961277831558567,0.9980638915779284,0.9980638915779284,0.9990319457889641,0.9990319457889641,1,1],"yaxis":"y"}],"layout":{"height":600,"legend":{"tracegroupgap":0},"shapes":[{"line":{"dash":"dash"},"type":"line","x0":0,"x1":1,"y0":0,"y1":1}],"showlegend":false,"template":{"data":{"bar":[{"error_x":{"color":"#2a3f5f"},"error_y":{"color":"#2a3f5f"},"marker":{"line":{"color":"white","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"barpolar":[{"marker":{"line":{"color":"white","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"carpet":[{"aaxis":{"endlinecolor":"#2a3f5f","gridcolor":"#C8D4E3","linecolor":"#C8D4E3","minorgridcolor":"#C8D4E3","startlinecolor":"#2a3f5f"},"baxis":{"endlinecolor":"#2a3f5f","gridcolor":"#C8D4E3","linecolor":"#C8D4E3","minorgridcolor":"#C8D4E3","startlinecolor":"#2a3f5f"},"type":"carpet"}],"choropleth":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"choropleth"}],"contour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"contour"}],"contourcarpet":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"contourcarpet"}],"heatmap":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmap"}],"heatmapgl":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmapgl"}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"histogram2d":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2d"}],"histogram2dcontour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2dcontour"}],"mesh3d":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"mesh3d"}],"parcoords":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"parcoords"}],"pie":[{"automargin":true,"type":"pie"}],"scatter":[{"fillpattern":{"fillmode":"overlay","size":10,"solidity":0.2},"type":"scatter"}],"scatter3d":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter3d"}],"scattercarpet":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattercarpet"}],"scattergeo":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergeo"}],"scattergl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergl"}],"scattermapbox":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattermapbox"}],"scatterpolar":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolar"}],"scatterpolargl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolargl"}],"scatterternary":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterternary"}],"surface":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"surface"}],"table":[{"cells":{"fill":{"color":"#EBF0F8"},"line":{"color":"white"}},"header":{"fill":{"color":"#C8D4E3"},"line":{"color":"white"}},"type":"table"}]},"layout":{"annotationdefaults":{"arrowcolor":"#2a3f5f","arrowhead":0,"arrowwidth":1},"autotypenumbers":"strict","coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]],"sequential":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"sequentialminus":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]]},"colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#2a3f5f"},"geo":{"bgcolor":"white","lakecolor":"white","landcolor":"white","showlakes":true,"showland":true,"subunitcolor":"#C8D4E3"},"hoverlabel":{"align":"left"},"hovermode":"closest","mapbox":{"style":"light"},"paper_bgcolor":"white","plot_bgcolor":"white","polar":{"angularaxis":{"gridcolor":"#EBF0F8","linecolor":"#EBF0F8","ticks":""},"bgcolor":"white","radialaxis":{"gridcolor":"#EBF0F8","linecolor":"#EBF0F8","ticks":""}},"scene":{"xaxis":{"backgroundcolor":"white","gridcolor":"#DFE8F3","gridwidth":2,"linecolor":"#EBF0F8","showbackground":true,"ticks":"","zerolinecolor":"#EBF0F8"},"yaxis":{"backgroundcolor":"white","gridcolor":"#DFE8F3","gridwidth":2,"linecolor":"#EBF0F8","showbackground":true,"ticks":"","zerolinecolor":"#EBF0F8"},"zaxis":{"backgroundcolor":"white","gridcolor":"#DFE8F3","gridwidth":2,"linecolor":"#EBF0F8","showbackground":true,"ticks":"","zerolinecolor":"#EBF0F8"}},"shapedefaults":{"line":{"color":"#2a3f5f"}},"ternary":{"aaxis":{"gridcolor":"#DFE8F3","linecolor":"#A2B1C6","ticks":""},"baxis":{"gridcolor":"#DFE8F3","linecolor":"#A2B1C6","ticks":""},"bgcolor":"white","caxis":{"gridcolor":"#DFE8F3","linecolor":"#A2B1C6","ticks":""}},"title":{"x":0.05},"xaxis":{"automargin":true,"gridcolor":"#EBF0F8","linecolor":"#EBF0F8","ticks":"","title":{"standoff":15},"zerolinecolor":"#EBF0F8","zerolinewidth":2},"yaxis":{"automargin":true,"gridcolor":"#EBF0F8","linecolor":"#EBF0F8","ticks":"","title":{"standoff":15},"zerolinecolor":"#EBF0F8","zerolinewidth":2}}},"title":{"text":"ROC Curve (AUC=0.8840)"},"width":800,"xaxis":{"anchor":"y","constrain":"domain","domain":[0,1],"title":{"text":"False Positive Rate"}},"yaxis":{"anchor":"x","domain":[0,1],"scaleanchor":"x","scaleratio":1,"title":{"text":"True Positive Rate"}}}},"text/html":["<div>                            <div id=\"b84a97ef-fb77-4854-b427-2daa9f54ac31\" class=\"plotly-graph-div\" style=\"height:600px; width:800px;\"></div>            <script type=\"text/javascript\">                require([\"plotly\"], function(Plotly) {                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"b84a97ef-fb77-4854-b427-2daa9f54ac31\")) {                    Plotly.newPlot(                        \"b84a97ef-fb77-4854-b427-2daa9f54ac31\",                        [{\"fillpattern\":{\"shape\":\"\"},\"hovertemplate\":\"False Positive Rate=%{x}\\u003cbr\\u003eTrue Positive Rate=%{y}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"\",\"line\":{\"color\":\"#636efa\"},\"marker\":{\"symbol\":\"circle\"},\"mode\":\"lines\",\"name\":\"\",\"orientation\":\"v\",\"showlegend\":false,\"stackgroup\":\"1\",\"x\":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.00267379679144385,0.00267379679144385,0.0053475935828877,0.0053475935828877,0.008021390374331552,0.008021390374331552,0.0106951871657754,0.0106951871657754,0.013368983957219251,0.013368983957219251,0.016042780748663103,0.016042780748663103,0.01871657754010695,0.01871657754010695,0.0213903743315508,0.0213903743315508,0.02406417112299465,0.02406417112299465,0.026737967914438502,0.026737967914438502,0.029411764705882353,0.029411764705882353,0.03208556149732621,0.03208556149732621,0.034759358288770054,0.034759358288770054,0.0374331550802139,0.0374331550802139,0.040106951871657755,0.040106951871657755,0.0427807486631016,0.0427807486631016,0.045454545454545456,0.045454545454545456,0.0481283422459893,0.0481283422459893,0.05080213903743316,0.05080213903743316,0.053475935828877004,0.053475935828877004,0.058823529411764705,0.058823529411764705,0.06149732620320856,0.06149732620320856,0.06684491978609626,0.06684491978609626,0.06951871657754011,0.06951871657754011,0.07219251336898395,0.07219251336898395,0.0748663101604278,0.0748663101604278,0.07754010695187166,0.07754010695187166,0.0855614973262032,0.0855614973262032,0.08823529411764706,0.08823529411764706,0.0962566844919786,0.0962566844919786,0.09893048128342247,0.09893048128342247,0.10160427807486631,0.10160427807486631,0.10427807486631016,0.10427807486631016,0.10695187165775401,0.10695187165775401,0.10962566844919786,0.10962566844919786,0.11229946524064172,0.11229946524064172,0.11497326203208556,0.11497326203208556,0.11764705882352941,0.11764705882352941,0.12032085561497326,0.12032085561497326,0.12299465240641712,0.12299465240641712,0.12566844919786097,0.12566844919786097,0.12834224598930483,0.12834224598930483,0.13101604278074866,0.13101604278074866,0.13368983957219252,0.13368983957219252,0.13636363636363635,0.13636363636363635,0.13903743315508021,0.13903743315508021,0.14171122994652408,0.14171122994652408,0.1443850267379679,0.1443850267379679,0.14705882352941177,0.14705882352941177,0.1497326203208556,0.1497326203208556,0.15240641711229946,0.15240641711229946,0.15508021390374332,0.15508021390374332,0.15775401069518716,0.15775401069518716,0.16042780748663102,0.16042780748663102,0.1657754010695187,0.1657754010695187,0.16844919786096257,0.16844919786096257,0.17379679144385027,0.17379679144385027,0.17647058823529413,0.17647058823529413,0.17914438502673796,0.17914438502673796,0.18449197860962566,0.18449197860962566,0.18716577540106952,0.18716577540106952,0.18983957219251338,0.18983957219251338,0.1925133689839572,0.1925133689839572,0.19518716577540107,0.19518716577540107,0.19786096256684493,0.19786096256684493,0.20053475935828877,0.20053475935828877,0.20320855614973263,0.20320855614973263,0.20855614973262032,0.20855614973262032,0.21122994652406418,0.21122994652406418,0.21657754010695188,0.21657754010695188,0.2192513368983957,0.2192513368983957,0.22192513368983957,0.22192513368983957,0.22994652406417113,0.22994652406417113,0.232620320855615,0.232620320855615,0.23529411764705882,0.23529411764705882,0.23796791443850268,0.23796791443850268,0.24064171122994651,0.24064171122994651,0.24331550802139038,0.24331550802139038,0.24866310160427807,0.24866310160427807,0.25133689839572193,0.25133689839572193,0.2540106951871658,0.2540106951871658,0.25935828877005346,0.25935828877005346,0.2620320855614973,0.2620320855614973,0.27540106951871657,0.27540106951871657,0.27807486631016043,0.27807486631016043,0.2807486631016043,0.2807486631016043,0.28342245989304815,0.28342245989304815,0.28609625668449196,0.28609625668449196,0.2887700534759358,0.2887700534759358,0.2914438502673797,0.2914438502673797,0.29411764705882354,0.29411764705882354,0.30213903743315507,0.30213903743315507,0.3048128342245989,0.3048128342245989,0.31283422459893045,0.31283422459893045,0.3155080213903743,0.3155080213903743,0.32085561497326204,0.32085561497326204,0.32620320855614976,0.32620320855614976,0.32887700534759357,0.32887700534759357,0.3342245989304813,0.3342245989304813,0.33689839572192515,0.33689839572192515,0.339572192513369,0.339572192513369,0.3422459893048128,0.3422459893048128,0.3449197860962567,0.3449197860962567,0.34759358288770054,0.34759358288770054,0.3582887700534759,0.3582887700534759,0.3663101604278075,0.3663101604278075,0.3689839572192513,0.3689839572192513,0.37433155080213903,0.37433155080213903,0.37967914438502676,0.37967914438502676,0.38235294117647056,0.38235294117647056,0.39037433155080214,0.39037433155080214,0.39572192513368987,0.39572192513368987,0.40106951871657753,0.40106951871657753,0.4117647058823529,0.4117647058823529,0.4144385026737968,0.4144385026737968,0.41711229946524064,0.41711229946524064,0.42245989304812837,0.42245989304812837,0.42513368983957217,0.42513368983957217,0.42780748663101603,0.42780748663101603,0.43315508021390375,0.43315508021390375,0.4358288770053476,0.4358288770053476,0.44919786096256686,0.44919786096256686,0.45187165775401067,0.45187165775401067,0.4679144385026738,0.4679144385026738,0.4732620320855615,0.4732620320855615,0.4946524064171123,0.4946524064171123,0.49732620320855614,0.49732620320855614,0.5026737967914439,0.5026737967914439,0.5133689839572193,0.5133689839572193,0.5187165775401069,0.5187165775401069,0.5213903743315508,0.5213903743315508,0.5267379679144385,0.5267379679144385,0.5320855614973262,0.5320855614973262,0.5347593582887701,0.5347593582887701,0.5374331550802139,0.5374331550802139,0.5454545454545454,0.5454545454545454,0.5614973262032086,0.5614973262032086,0.5641711229946524,0.5641711229946524,0.56951871657754,0.56951871657754,0.5775401069518716,0.5775401069518716,0.5882352941176471,0.5882352941176471,0.5909090909090909,0.5909090909090909,0.6042780748663101,0.6042780748663101,0.6096256684491979,0.6096256684491979,0.6149732620320856,0.6149732620320856,0.6229946524064172,0.6229946524064172,0.6363636363636364,0.6363636363636364,0.6470588235294118,0.6470588235294118,0.660427807486631,0.660427807486631,0.6657754010695187,0.6657754010695187,0.6684491978609626,0.6684491978609626,0.6951871657754011,0.6951871657754011,0.7165775401069518,0.7165775401069518,0.7299465240641712,0.7299465240641712,0.7352941176470589,0.7352941176470589,0.7566844919786097,0.7566844919786097,0.7754010695187166,0.7754010695187166,0.7887700534759359,0.7887700534759359,0.7994652406417112,0.7994652406417112,0.839572192513369,0.839572192513369,0.8556149732620321,0.8556149732620321,0.8796791443850267,0.8796791443850267,0.9117647058823529,0.9117647058823529,1.0],\"xaxis\":\"x\",\"y\":[0.0,0.00968054211035818,0.013552758954501452,0.016456921587608905,0.01936108422071636,0.021297192642787996,0.02420135527589545,0.026137463697967087,0.02904162633107454,0.030977734753146177,0.03388189738625363,0.0377541142303969,0.1181026137463698,0.12003872216844143,0.1345595353339787,0.1345595353339787,0.1403678606001936,0.1403678606001936,0.1713455953533398,0.1713455953533398,0.18489835430784124,0.18489835430784124,0.23910939012584706,0.23910939012584706,0.26524685382381413,0.26524685382381413,0.2933204259438529,0.2933204259438529,0.356243949661181,0.356243949661181,0.3591481122942885,0.3591481122942885,0.3639883833494676,0.3639883833494676,0.36786060019361083,0.36786060019361083,0.41045498547918685,0.41045498547918685,0.4230396902226525,0.4230396902226525,0.4491771539206196,0.4491771539206196,0.46563407550822844,0.46563407550822844,0.4898354307841239,0.4898354307841239,0.49080348499515974,0.49080348499515974,0.5024201355275896,0.5024201355275896,0.5091965150048403,0.5091965150048403,0.5150048402710552,0.5150048402710552,0.5198451113262342,0.5198451113262342,0.5333978702807357,0.5333978702807357,0.5488867376573088,0.5488867376573088,0.5546950629235237,0.5546950629235237,0.5556631171345595,0.5556631171345595,0.5779283639883833,0.5779283639883833,0.579864472410455,0.579864472410455,0.5943852855759922,0.5943852855759922,0.5953533397870281,0.5953533397870281,0.6176185866408519,0.6176185866408519,0.6253630203291385,0.6253630203291385,0.6272991287512101,0.6272991287512101,0.6282671829622459,0.6282671829622459,0.6350435624394967,0.6350435624394967,0.6447241045498547,0.6447241045498547,0.6534365924491772,0.6534365924491772,0.6544046466602129,0.6544046466602129,0.665053242981607,0.665053242981607,0.6679574056147144,0.6679574056147144,0.6786060019361084,0.6786060019361084,0.6815101645692159,0.6815101645692159,0.7008712487899322,0.7008712487899322,0.7115198451113263,0.7115198451113263,0.7221684414327202,0.7221684414327202,0.7347531461761858,0.7347531461761858,0.7357212003872217,0.7357212003872217,0.7424975798644724,0.7424975798644724,0.7473378509196515,0.7473378509196515,0.755082284607938,0.755082284607938,0.7560503388189739,0.7560503388189739,0.7570183930300097,0.7570183930300097,0.7599225556631172,0.7599225556631172,0.7608906098741529,0.7608906098741529,0.7618586640851888,0.7618586640851888,0.7628267182962246,0.7628267182962246,0.7666989351403679,0.7666989351403679,0.7696030977734754,0.7696030977734754,0.7705711519845111,0.7705711519845111,0.771539206195547,0.771539206195547,0.7744433688286544,0.7744433688286544,0.7754114230396902,0.7754114230396902,0.7812197483059051,0.7812197483059051,0.782187802516941,0.782187802516941,0.7850919651500484,0.7850919651500484,0.7957405614714425,0.7957405614714425,0.7996127783155856,0.7996127783155856,0.8005808325266215,0.8005808325266215,0.8034849951597289,0.8034849951597289,0.8063891577928364,0.8063891577928364,0.8092933204259438,0.8092933204259438,0.8218780251694094,0.8218780251694094,0.8257502420135527,0.8257502420135527,0.8305905130687319,0.8305905130687319,0.8315585672797676,0.8315585672797676,0.8344627299128751,0.8344627299128751,0.8422071636011617,0.8422071636011617,0.8441432720232332,0.8441432720232332,0.8470474346563408,0.8470474346563408,0.8518877057115198,0.8518877057115198,0.8538238141335914,0.8538238141335914,0.8576960309777347,0.8576960309777347,0.8596321393998064,0.8596321393998064,0.8606001936108422,0.8606001936108422,0.8654404646660213,0.8654404646660213,0.8664085188770572,0.8664085188770572,0.8683446272991288,0.8683446272991288,0.8702807357212003,0.8702807357212003,0.8760890609874153,0.8760890609874153,0.8780251694094869,0.8780251694094869,0.8789932236205228,0.8789932236205228,0.8799612778315585,0.8799612778315585,0.882865440464666,0.882865440464666,0.8867376573088093,0.8867376573088093,0.8877057115198451,0.8877057115198451,0.8896418199419167,0.8896418199419167,0.89351403678606,0.89351403678606,0.9002904162633107,0.9002904162633107,0.9022265246853823,0.9022265246853823,0.9031945788964182,0.9031945788964182,0.9051306873184899,0.9051306873184899,0.9070667957405615,0.9070667957405615,0.9080348499515973,0.9080348499515973,0.9157792836398838,0.9157792836398838,0.920619554695063,0.920619554695063,0.9215876089060987,0.9215876089060987,0.9235237173281704,0.9235237173281704,0.9273959341723137,0.9273959341723137,0.9283639883833494,0.9283639883833494,0.9293320425943853,0.9293320425943853,0.9303000968054211,0.9303000968054211,0.9312681510164569,0.9312681510164569,0.9322362052274927,0.9322362052274927,0.9341723136495643,0.9341723136495643,0.936108422071636,0.936108422071636,0.9370764762826719,0.9370764762826719,0.9390125847047435,0.9390125847047435,0.9419167473378509,0.9419167473378509,0.9428848015488868,0.9428848015488868,0.9438528557599225,0.9438528557599225,0.9448209099709584,0.9448209099709584,0.9467570183930301,0.9467570183930301,0.9486931268151017,0.9486931268151017,0.9496611810261375,0.9496611810261375,0.9506292352371732,0.9506292352371732,0.9515972894482091,0.9515972894482091,0.952565343659245,0.952565343659245,0.9545014520813165,0.9545014520813165,0.9554695062923524,0.9554695062923524,0.9564375605033882,0.9564375605033882,0.9603097773475314,0.9603097773475314,0.9612778315585673,0.9612778315585673,0.9622458857696031,0.9622458857696031,0.9632139399806389,0.9632139399806389,0.9661181026137464,0.9661181026137464,0.968054211035818,0.968054211035818,0.9690222652468539,0.9690222652468539,0.9699903194578896,0.9699903194578896,0.9719264278799613,0.9719264278799613,0.972894482090997,0.972894482090997,0.9738625363020329,0.9738625363020329,0.9748305905130688,0.9748305905130688,0.9757986447241046,0.9757986447241046,0.9767666989351403,0.9767666989351403,0.9777347531461762,0.9777347531461762,0.978702807357212,0.978702807357212,0.9806389157792836,0.9806389157792836,0.9816069699903195,0.9816069699903195,0.9825750242013552,0.9825750242013552,0.9835430784123911,0.9835430784123911,0.9845111326234269,0.9845111326234269,0.9854791868344628,0.9854791868344628,0.9864472410454985,0.9864472410454985,0.9874152952565344,0.9874152952565344,0.9883833494675702,0.9883833494675702,0.989351403678606,0.989351403678606,0.9903194578896418,0.9903194578896418,0.9912875121006777,0.9912875121006777,0.9932236205227493,0.9932236205227493,0.9941916747337851,0.9941916747337851,0.995159728944821,0.995159728944821,0.9961277831558567,0.9961277831558567,0.9980638915779284,0.9980638915779284,0.9990319457889641,0.9990319457889641,1.0,1.0],\"yaxis\":\"y\",\"type\":\"scatter\"}],                        {\"template\":{\"data\":{\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"white\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"white\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"#C8D4E3\",\"linecolor\":\"#C8D4E3\",\"minorgridcolor\":\"#C8D4E3\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"#C8D4E3\",\"linecolor\":\"#C8D4E3\",\"minorgridcolor\":\"#C8D4E3\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"choropleth\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"choropleth\"}],\"contourcarpet\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"contourcarpet\"}],\"contour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"contour\"}],\"heatmapgl\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmapgl\"}],\"heatmap\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmap\"}],\"histogram2dcontour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2dcontour\"}],\"histogram2d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2d\"}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"mesh3d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"mesh3d\"}],\"parcoords\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"parcoords\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}],\"scatter3d\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatter3d\"}],\"scattercarpet\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattercarpet\"}],\"scattergeo\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergeo\"}],\"scattergl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergl\"}],\"scattermapbox\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattermapbox\"}],\"scatterpolargl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolargl\"}],\"scatterpolar\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolar\"}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"scatterternary\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterternary\"}],\"surface\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"surface\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}]},\"layout\":{\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"autotypenumbers\":\"strict\",\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]],\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]},\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"geo\":{\"bgcolor\":\"white\",\"lakecolor\":\"white\",\"landcolor\":\"white\",\"showlakes\":true,\"showland\":true,\"subunitcolor\":\"#C8D4E3\"},\"hoverlabel\":{\"align\":\"left\"},\"hovermode\":\"closest\",\"mapbox\":{\"style\":\"light\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"white\",\"polar\":{\"angularaxis\":{\"gridcolor\":\"#EBF0F8\",\"linecolor\":\"#EBF0F8\",\"ticks\":\"\"},\"bgcolor\":\"white\",\"radialaxis\":{\"gridcolor\":\"#EBF0F8\",\"linecolor\":\"#EBF0F8\",\"ticks\":\"\"}},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"white\",\"gridcolor\":\"#DFE8F3\",\"gridwidth\":2,\"linecolor\":\"#EBF0F8\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"#EBF0F8\"},\"yaxis\":{\"backgroundcolor\":\"white\",\"gridcolor\":\"#DFE8F3\",\"gridwidth\":2,\"linecolor\":\"#EBF0F8\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"#EBF0F8\"},\"zaxis\":{\"backgroundcolor\":\"white\",\"gridcolor\":\"#DFE8F3\",\"gridwidth\":2,\"linecolor\":\"#EBF0F8\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"#EBF0F8\"}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"ternary\":{\"aaxis\":{\"gridcolor\":\"#DFE8F3\",\"linecolor\":\"#A2B1C6\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"#DFE8F3\",\"linecolor\":\"#A2B1C6\",\"ticks\":\"\"},\"bgcolor\":\"white\",\"caxis\":{\"gridcolor\":\"#DFE8F3\",\"linecolor\":\"#A2B1C6\",\"ticks\":\"\"}},\"title\":{\"x\":0.05},\"xaxis\":{\"automargin\":true,\"gridcolor\":\"#EBF0F8\",\"linecolor\":\"#EBF0F8\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"#EBF0F8\",\"zerolinewidth\":2},\"yaxis\":{\"automargin\":true,\"gridcolor\":\"#EBF0F8\",\"linecolor\":\"#EBF0F8\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"#EBF0F8\",\"zerolinewidth\":2}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"False Positive Rate\"},\"constrain\":\"domain\"},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"True Positive Rate\"},\"scaleanchor\":\"x\",\"scaleratio\":1},\"legend\":{\"tracegroupgap\":0},\"title\":{\"text\":\"ROC Curve (AUC=0.8840)\"},\"height\":600,\"width\":800,\"shapes\":[{\"line\":{\"dash\":\"dash\"},\"type\":\"line\",\"x0\":0,\"x1\":1,\"y0\":0,\"y1\":1}],\"showlegend\":false},                        {\"responsive\": true}                    ).then(function(){\n","                            \n","var gd = document.getElementById('b84a97ef-fb77-4854-b427-2daa9f54ac31');\n","var x = new MutationObserver(function (mutations, observer) {{\n","        var display = window.getComputedStyle(gd).display;\n","        if (!display || display === 'none') {{\n","            console.log([gd, 'removed!']);\n","            Plotly.purge(gd);\n","            observer.disconnect();\n","        }}\n","}});\n","\n","// Listen for the removal of the full notebook cells\n","var notebookContainer = gd.closest('#notebook-container');\n","if (notebookContainer) {{\n","    x.observe(notebookContainer, {childList: true});\n","}}\n","\n","// Listen for the clearing of the current output cell\n","var outputEl = gd.closest('.output');\n","if (outputEl) {{\n","    x.observe(outputEl, {childList: true});\n","}}\n","\n","                        })                };                });            </script>        </div>"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.plotly.v1+json":{"config":{"plotlyServerURL":"https://plot.ly"},"data":[{"fillpattern":{"shape":""},"hovertemplate":"Recall=%{x}<br>Precision=%{y}<extra></extra>","legendgroup":"","line":{"color":"#636efa"},"marker":{"symbol":"circle"},"mode":"lines","name":"","orientation":"v","showlegend":false,"stackgroup":"1","type":"scatter","x":[1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,0.9990319457889641,0.9990319457889641,0.9990319457889641,0.9990319457889641,0.9990319457889641,0.9990319457889641,0.9990319457889641,0.9990319457889641,0.9990319457889641,0.9990319457889641,0.9990319457889641,0.9990319457889641,0.9990319457889641,0.9980638915779284,0.9980638915779284,0.9980638915779284,0.9980638915779284,0.9980638915779284,0.9980638915779284,0.9980638915779284,0.9980638915779284,0.9980638915779284,0.9980638915779284,0.9970958373668926,0.9961277831558567,0.9961277831558567,0.9961277831558567,0.9961277831558567,0.9961277831558567,0.9961277831558567,0.9961277831558567,0.995159728944821,0.995159728944821,0.995159728944821,0.995159728944821,0.995159728944821,0.995159728944821,0.995159728944821,0.995159728944821,0.995159728944821,0.995159728944821,0.995159728944821,0.995159728944821,0.995159728944821,0.995159728944821,0.995159728944821,0.995159728944821,0.9941916747337851,0.9941916747337851,0.9941916747337851,0.9941916747337851,0.9941916747337851,0.9932236205227493,0.9932236205227493,0.9932236205227493,0.9932236205227493,0.9932236205227493,0.9932236205227493,0.9922555663117134,0.9912875121006777,0.9912875121006777,0.9912875121006777,0.9912875121006777,0.9912875121006777,0.9912875121006777,0.9912875121006777,0.9912875121006777,0.9903194578896418,0.9903194578896418,0.9903194578896418,0.9903194578896418,0.9903194578896418,0.9903194578896418,0.9903194578896418,0.9903194578896418,0.9903194578896418,0.989351403678606,0.989351403678606,0.989351403678606,0.9883833494675702,0.9883833494675702,0.9883833494675702,0.9883833494675702,0.9883833494675702,0.9883833494675702,0.9874152952565344,0.9874152952565344,0.9874152952565344,0.9874152952565344,0.9874152952565344,0.9874152952565344,0.9874152952565344,0.9874152952565344,0.9874152952565344,0.9864472410454985,0.9864472410454985,0.9864472410454985,0.9864472410454985,0.9864472410454985,0.9864472410454985,0.9864472410454985,0.9864472410454985,0.9864472410454985,0.9864472410454985,0.9864472410454985,0.9854791868344628,0.9854791868344628,0.9845111326234269,0.9845111326234269,0.9845111326234269,0.9835430784123911,0.9835430784123911,0.9835430784123911,0.9835430784123911,0.9835430784123911,0.9835430784123911,0.9825750242013552,0.9825750242013552,0.9825750242013552,0.9825750242013552,0.9825750242013552,0.9816069699903195,0.9816069699903195,0.9816069699903195,0.9816069699903195,0.9816069699903195,0.9816069699903195,0.9806389157792836,0.9806389157792836,0.9806389157792836,0.9806389157792836,0.9796708615682478,0.978702807357212,0.978702807357212,0.978702807357212,0.9777347531461762,0.9777347531461762,0.9777347531461762,0.9767666989351403,0.9767666989351403,0.9767666989351403,0.9767666989351403,0.9767666989351403,0.9767666989351403,0.9757986447241046,0.9757986447241046,0.9748305905130688,0.9748305905130688,0.9748305905130688,0.9748305905130688,0.9748305905130688,0.9738625363020329,0.9738625363020329,0.9738625363020329,0.9738625363020329,0.972894482090997,0.972894482090997,0.972894482090997,0.9719264278799613,0.9719264278799613,0.9709583736689255,0.9699903194578896,0.9699903194578896,0.9699903194578896,0.9699903194578896,0.9699903194578896,0.9699903194578896,0.9699903194578896,0.9690222652468539,0.9690222652468539,0.9690222652468539,0.9690222652468539,0.968054211035818,0.968054211035818,0.9670861568247822,0.9661181026137464,0.9661181026137464,0.9651500484027106,0.9641819941916747,0.9632139399806389,0.9632139399806389,0.9632139399806389,0.9622458857696031,0.9622458857696031,0.9622458857696031,0.9612778315585673,0.9612778315585673,0.9603097773475314,0.9603097773475314,0.9603097773475314,0.9593417231364957,0.9583736689254598,0.957405614714424,0.9564375605033882,0.9564375605033882,0.9564375605033882,0.9564375605033882,0.9564375605033882,0.9554695062923524,0.9554695062923524,0.9554695062923524,0.9545014520813165,0.9545014520813165,0.9535333978702807,0.952565343659245,0.952565343659245,0.952565343659245,0.952565343659245,0.952565343659245,0.952565343659245,0.952565343659245,0.952565343659245,0.952565343659245,0.9515972894482091,0.9515972894482091,0.9515972894482091,0.9506292352371732,0.9506292352371732,0.9506292352371732,0.9506292352371732,0.9506292352371732,0.9506292352371732,0.9506292352371732,0.9496611810261375,0.9496611810261375,0.9486931268151017,0.9486931268151017,0.9486931268151017,0.9486931268151017,0.9486931268151017,0.9486931268151017,0.9477250726040658,0.9467570183930301,0.9467570183930301,0.9457889641819942,0.9448209099709584,0.9448209099709584,0.9448209099709584,0.9438528557599225,0.9438528557599225,0.9428848015488868,0.9428848015488868,0.9419167473378509,0.9419167473378509,0.9419167473378509,0.9409486931268151,0.9399806389157793,0.9390125847047435,0.9390125847047435,0.9380445304937076,0.9370764762826719,0.9370764762826719,0.936108422071636,0.936108422071636,0.936108422071636,0.936108422071636,0.936108422071636,0.9351403678606002,0.9341723136495643,0.9341723136495643,0.9341723136495643,0.9332042594385286,0.9322362052274927,0.9322362052274927,0.9322362052274927,0.9312681510164569,0.9312681510164569,0.9312681510164569,0.9312681510164569,0.9303000968054211,0.9303000968054211,0.9293320425943853,0.9293320425943853,0.9293320425943853,0.9283639883833494,0.9283639883833494,0.9283639883833494,0.9273959341723137,0.9273959341723137,0.9264278799612778,0.925459825750242,0.9244917715392061,0.9235237173281704,0.9235237173281704,0.9235237173281704,0.9235237173281704,0.9225556631171346,0.9215876089060987,0.9215876089060987,0.9215876089060987,0.9215876089060987,0.9215876089060987,0.920619554695063,0.920619554695063,0.9196515004840271,0.9186834462729913,0.9177153920619555,0.9167473378509197,0.9157792836398838,0.9157792836398838,0.914811229428848,0.9138431752178122,0.9128751210067764,0.9119070667957405,0.9109390125847048,0.9099709583736689,0.9090029041626331,0.9080348499515973,0.9080348499515973,0.9070667957405615,0.9070667957405615,0.9060987415295256,0.9051306873184899,0.9051306873184899,0.904162633107454,0.9031945788964182,0.9031945788964182,0.9031945788964182,0.9022265246853823,0.9022265246853823,0.9012584704743466,0.9002904162633107,0.9002904162633107,0.9002904162633107,0.8993223620522749,0.8983543078412392,0.8973862536302033,0.8964181994191674,0.8954501452081317,0.8944820909970959,0.89351403678606,0.89351403678606,0.89351403678606,0.8925459825750242,0.8915779283639884,0.8906098741529526,0.8896418199419167,0.8896418199419167,0.888673765730881,0.8877057115198451,0.8877057115198451,0.8877057115198451,0.8877057115198451,0.8867376573088093,0.8867376573088093,0.8857696030977735,0.8848015488867377,0.8838334946757018,0.882865440464666,0.882865440464666,0.882865440464666,0.882865440464666,0.8818973862536302,0.8809293320425944,0.8799612778315585,0.8799612778315585,0.8789932236205228,0.8789932236205228,0.8780251694094869,0.8780251694094869,0.8770571151984511,0.8760890609874153,0.8760890609874153,0.8751210067763795,0.8741529525653436,0.8731848983543078,0.872216844143272,0.8712487899322362,0.8702807357212003,0.8702807357212003,0.8693126815101646,0.8683446272991288,0.8683446272991288,0.8673765730880929,0.8664085188770572,0.8664085188770572,0.8654404646660213,0.8654404646660213,0.8654404646660213,0.8654404646660213,0.8654404646660213,0.8654404646660213,0.8644724104549855,0.8635043562439496,0.8625363020329139,0.861568247821878,0.8606001936108422,0.8606001936108422,0.8596321393998064,0.8596321393998064,0.8596321393998064,0.8586640851887706,0.8576960309777347,0.8576960309777347,0.856727976766699,0.8557599225556631,0.8547918683446273,0.8538238141335914,0.8538238141335914,0.8528557599225557,0.8518877057115198,0.8518877057115198,0.8518877057115198,0.850919651500484,0.8499515972894482,0.8489835430784124,0.8480154888673765,0.8470474346563408,0.8470474346563408,0.846079380445305,0.8451113262342691,0.8441432720232332,0.8441432720232332,0.8431752178121975,0.8422071636011617,0.8422071636011617,0.8412391093901258,0.8402710551790901,0.8393030009680542,0.8383349467570184,0.8373668925459826,0.8363988383349468,0.8354307841239109,0.8344627299128751,0.8344627299128751,0.8334946757018393,0.8325266214908035,0.8315585672797676,0.8315585672797676,0.8305905130687319,0.8305905130687319,0.8305905130687319,0.8305905130687319,0.829622458857696,0.8286544046466602,0.8276863504356244,0.8267182962245886,0.8257502420135527,0.8257502420135527,0.8247821878025169,0.8238141335914811,0.8228460793804453,0.8218780251694094,0.8218780251694094,0.8209099709583737,0.8199419167473379,0.818973862536302,0.8180058083252663,0.8170377541142304,0.8160696999031946,0.8151016456921588,0.814133591481123,0.8131655372700871,0.8121974830590513,0.8112294288480155,0.8102613746369797,0.8092933204259438,0.8092933204259438,0.8092933204259438,0.8083252662149081,0.8073572120038722,0.8063891577928364,0.8063891577928364,0.8054211035818006,0.8044530493707648,0.8034849951597289,0.8034849951597289,0.8034849951597289,0.8025169409486931,0.8015488867376573,0.8005808325266215,0.8005808325266215,0.7996127783155856,0.7996127783155856,0.7986447241045499,0.797676669893514,0.7967086156824782,0.7957405614714425,0.7957405614714425,0.7947725072604066,0.7938044530493708,0.7928363988383349,0.7918683446272992,0.7909002904162633,0.7899322362052275,0.7889641819941917,0.7879961277831559,0.78702807357212,0.7860600193610843,0.7850919651500484,0.7850919651500484,0.7841239109390126,0.7831558567279767,0.782187802516941,0.782187802516941,0.7812197483059051,0.7812197483059051,0.7802516940948693,0.7792836398838335,0.7783155856727977,0.7773475314617618,0.7763794772507261,0.7754114230396902,0.7754114230396902,0.7744433688286544,0.7744433688286544,0.7744433688286544,0.7734753146176185,0.7725072604065828,0.771539206195547,0.771539206195547,0.7705711519845111,0.7705711519845111,0.7696030977734754,0.7696030977734754,0.7696030977734754,0.7686350435624395,0.7676669893514037,0.7666989351403679,0.7666989351403679,0.7657308809293321,0.7647628267182962,0.7637947725072604,0.7628267182962246,0.7628267182962246,0.7628267182962246,0.7618586640851888,0.7618586640851888,0.7608906098741529,0.7608906098741529,0.7599225556631172,0.7599225556631172,0.7589545014520813,0.7579864472410455,0.7570183930300097,0.7570183930300097,0.7560503388189739,0.7560503388189739,0.755082284607938,0.755082284607938,0.7541142303969022,0.7531461761858664,0.7521781219748306,0.7512100677637947,0.750242013552759,0.7492739593417231,0.7483059051306873,0.7473378509196515,0.7473378509196515,0.7463697967086157,0.7454017424975798,0.744433688286544,0.7434656340755083,0.7424975798644724,0.7424975798644724,0.7415295256534365,0.7405614714424008,0.739593417231365,0.7386253630203291,0.7376573088092934,0.7366892545982575,0.7357212003872217,0.7357212003872217,0.7347531461761858,0.7347531461761858,0.7337850919651501,0.7328170377541142,0.7318489835430784,0.7308809293320426,0.7299128751210068,0.7289448209099709,0.7279767666989352,0.7270087124878993,0.7260406582768635,0.7250726040658277,0.7241045498547919,0.723136495643756,0.7221684414327202,0.7221684414327202,0.7212003872216844,0.7202323330106486,0.7192642787996127,0.718296224588577,0.7173281703775412,0.7163601161665053,0.7153920619554696,0.7144240077444337,0.7134559535333979,0.712487899322362,0.7115198451113263,0.7115198451113263,0.7105517909002904,0.7095837366892546,0.7086156824782188,0.707647628267183,0.7066795740561471,0.7057115198451114,0.7047434656340755,0.7037754114230397,0.7028073572120038,0.7018393030009681,0.7008712487899322,0.7008712487899322,0.6999031945788964,0.6989351403678606,0.6979670861568248,0.6969990319457889,0.6960309777347532,0.6950629235237173,0.6940948693126815,0.6931268151016456,0.6921587608906099,0.691190706679574,0.6902226524685382,0.6892545982575025,0.6882865440464666,0.6873184898354308,0.686350435624395,0.6853823814133592,0.6844143272023233,0.6834462729912875,0.6824782187802517,0.6815101645692159,0.6815101645692159,0.68054211035818,0.6795740561471443,0.6786060019361084,0.6786060019361084,0.6776379477250726,0.6766698935140368,0.675701839303001,0.6747337850919651,0.6737657308809293,0.6727976766698935,0.6718296224588577,0.6708615682478218,0.6698935140367861,0.6689254598257502,0.6679574056147144,0.6679574056147144,0.6669893514036787,0.6660212971926428,0.665053242981607,0.665053242981607,0.6640851887705711,0.6631171345595354,0.6621490803484995,0.6611810261374637,0.6602129719264279,0.6592449177153921,0.6582768635043562,0.6573088092933205,0.6563407550822846,0.6553727008712488,0.6544046466602129,0.6544046466602129,0.6534365924491772,0.6534365924491772,0.6524685382381413,0.6515004840271055,0.6505324298160697,0.6495643756050339,0.648596321393998,0.6476282671829623,0.6466602129719264,0.6456921587608906,0.6447241045498547,0.6447241045498547,0.643756050338819,0.6427879961277831,0.6418199419167473,0.6408518877057116,0.6398838334946757,0.6389157792836399,0.6379477250726041,0.6369796708615683,0.6360116166505324,0.6350435624394967,0.6350435624394967,0.6340755082284608,0.633107454017425,0.6321393998063891,0.6311713455953534,0.6302032913843175,0.6292352371732817,0.6282671829622459,0.6282671829622459,0.6272991287512101,0.6272991287512101,0.6263310745401742,0.6253630203291385,0.6253630203291385,0.6243949661181026,0.6234269119070668,0.6224588576960309,0.6214908034849952,0.6205227492739593,0.6195546950629235,0.6185866408518877,0.6176185866408519,0.6176185866408519,0.6176185866408519,0.6176185866408519,0.616650532429816,0.6156824782187803,0.6147144240077445,0.6137463697967086,0.6127783155856728,0.611810261374637,0.6108422071636012,0.6098741529525653,0.6089060987415296,0.6079380445304937,0.6069699903194579,0.6060019361084221,0.6050338818973863,0.6040658276863504,0.6030977734753146,0.6021297192642788,0.601161665053243,0.6001936108422071,0.5992255566311714,0.5982575024201355,0.5972894482090997,0.5963213939980639,0.5953533397870281,0.5953533397870281,0.5943852855759922,0.5943852855759922,0.5943852855759922,0.5943852855759922,0.5934172313649564,0.5924491771539206,0.5914811229428848,0.590513068731849,0.5895450145208132,0.5885769603097774,0.5876089060987415,0.5866408518877058,0.5856727976766699,0.5847047434656341,0.5837366892545982,0.5827686350435625,0.5818005808325266,0.5808325266214908,0.579864472410455,0.579864472410455,0.5788964181994192,0.5779283639883833,0.5779283639883833,0.5769603097773476,0.5759922555663117,0.5750242013552759,0.57405614714424,0.5730880929332043,0.5721200387221684,0.5711519845111326,0.5701839303000968,0.569215876089061,0.5682478218780251,0.5672797676669894,0.5663117134559535,0.5653436592449177,0.5643756050338818,0.5634075508228461,0.5624394966118103,0.5614714424007744,0.5605033881897387,0.5595353339787028,0.558567279767667,0.5575992255566312,0.5566311713455954,0.5556631171345595,0.5556631171345595,0.5546950629235237,0.5546950629235237,0.5537270087124879,0.5527589545014521,0.5517909002904162,0.5508228460793805,0.5498547918683446,0.5488867376573088,0.5488867376573088,0.5488867376573088,0.547918683446273,0.5469506292352372,0.5459825750242013,0.5450145208131656,0.5440464666021297,0.5430784123910939,0.542110358180058,0.5411423039690223,0.5401742497579864,0.5392061955469506,0.5382381413359149,0.537270087124879,0.5363020329138432,0.5353339787028074,0.5343659244917716,0.5333978702807357,0.5333978702807357,0.5324298160696999,0.5314617618586641,0.5304937076476283,0.5295256534365924,0.5285575992255567,0.5275895450145208,0.526621490803485,0.5256534365924492,0.5246853823814134,0.5237173281703775,0.5227492739593417,0.5217812197483059,0.5208131655372701,0.5198451113262342,0.5198451113262342,0.5198451113262342,0.5188770571151985,0.5179090029041626,0.5169409486931268,0.515972894482091,0.5150048402710552,0.5150048402710552,0.5140367860600193,0.5130687318489835,0.5121006776379478,0.5111326234269119,0.510164569215876,0.5091965150048403,0.5091965150048403,0.5082284607938045,0.5072604065827686,0.5062923523717329,0.505324298160697,0.5043562439496612,0.5033881897386253,0.5024201355275896,0.5024201355275896,0.5014520813165537,0.5004840271055179,0.4995159728944821,0.4985479186834463,0.4975798644724105,0.49661181026137463,0.49564375605033884,0.494675701839303,0.4937076476282672,0.4927395934172314,0.49177153920619554,0.49080348499515974,0.49080348499515974,0.4898354307841239,0.4898354307841239,0.4888673765730881,0.4878993223620523,0.48693126815101645,0.48596321393998065,0.4849951597289448,0.484027105517909,0.4830590513068732,0.48209099709583736,0.48112294288480156,0.4801548886737657,0.4791868344627299,0.4782187802516941,0.47725072604065827,0.4762826718296225,0.4753146176185866,0.4743465634075508,0.47337850919651503,0.4724104549854792,0.4714424007744434,0.47047434656340753,0.46950629235237173,0.46853823814133594,0.4675701839303001,0.4666021297192643,0.46563407550822844,0.46563407550822844,0.46466602129719264,0.46369796708615685,0.462729912875121,0.4617618586640852,0.46079380445304935,0.45982575024201355,0.45885769603097776,0.4578896418199419,0.4569215876089061,0.45595353339787026,0.45498547918683446,0.45401742497579867,0.4530493707647628,0.452081316553727,0.45111326234269117,0.45014520813165537,0.4491771539206196,0.4491771539206196,0.4482090997095837,0.44724104549854793,0.4462729912875121,0.4453049370764763,0.4443368828654405,0.44336882865440463,0.44240077444336884,0.441432720232333,0.4404646660212972,0.4394966118102614,0.43852855759922554,0.43756050338818975,0.4365924491771539,0.4356243949661181,0.4346563407550823,0.43368828654404645,0.43272023233301066,0.4317521781219748,0.430784123910939,0.4298160696999032,0.42884801548886736,0.42787996127783157,0.4269119070667957,0.4259438528557599,0.4249757986447241,0.42400774443368827,0.4230396902226525,0.4230396902226525,0.4220716360116166,0.42110358180058083,0.42013552758954503,0.4191674733785092,0.4181994191674734,0.41723136495643753,0.41626331074540174,0.41529525653436594,0.4143272023233301,0.4133591481122943,0.41239109390125844,0.41142303969022265,0.41045498547918685,0.41045498547918685,0.409486931268151,0.4085188770571152,0.4075508228460794,0.40658276863504356,0.40561471442400776,0.4046466602129719,0.4036786060019361,0.4027105517909003,0.40174249757986447,0.40077444336882867,0.3998063891577928,0.398838334946757,0.3978702807357212,0.3969022265246854,0.3959341723136496,0.39496611810261373,0.39399806389157793,0.39303000968054214,0.3920619554695063,0.3910939012584705,0.39012584704743464,0.38915779283639884,0.38818973862536305,0.3872216844143272,0.3862536302032914,0.38528557599225555,0.38431752178121975,0.38334946757018395,0.3823814133591481,0.3814133591481123,0.38044530493707646,0.37947725072604066,0.37850919651500486,0.377541142303969,0.3765730880929332,0.37560503388189737,0.37463697967086157,0.3736689254598258,0.3727008712487899,0.3717328170377541,0.3707647628267183,0.3697967086156825,0.3688286544046467,0.36786060019361083,0.36786060019361083,0.36689254598257504,0.3659244917715392,0.3649564375605034,0.3639883833494676,0.3639883833494676,0.36302032913843174,0.36205227492739595,0.3610842207163601,0.3601161665053243,0.3591481122942885,0.3591481122942885,0.35818005808325265,0.35721200387221685,0.356243949661181,0.356243949661181,0.3552758954501452,0.3543078412391094,0.35333978702807356,0.35237173281703776,0.3514036786060019,0.3504356243949661,0.3494675701839303,0.34849951597289447,0.3475314617618587,0.3465634075508228,0.345595353339787,0.34462729912875123,0.3436592449177154,0.3426911907066796,0.34172313649564373,0.34075508228460794,0.33978702807357214,0.3388189738625363,0.3378509196515005,0.33688286544046464,0.33591481122942884,0.33494675701839305,0.3339787028073572,0.3330106485963214,0.33204259438528555,0.33107454017424975,0.33010648596321396,0.3291384317521781,0.3281703775411423,0.32720232333010646,0.32623426911907066,0.32526621490803487,0.324298160696999,0.3233301064859632,0.32236205227492737,0.3213939980638916,0.3204259438528558,0.3194578896418199,0.31848983543078413,0.31752178121974833,0.3165537270087125,0.3155856727976767,0.31461761858664083,0.31364956437560504,0.31268151016456924,0.3117134559535334,0.3107454017424976,0.30977734753146174,0.30880929332042595,0.30784123910939015,0.3068731848983543,0.3059051306873185,0.30493707647628265,0.30396902226524686,0.30300096805421106,0.3020329138431752,0.3010648596321394,0.30009680542110356,0.29912875121006777,0.29816069699903197,0.2971926427879961,0.2962245885769603,0.2952565343659245,0.2942884801548887,0.2933204259438529,0.2933204259438529,0.29235237173281703,0.29138431752178123,0.2904162633107454,0.2894482090997096,0.2884801548886738,0.28751210067763794,0.28654404646660214,0.2855759922555663,0.2846079380445305,0.2836398838334947,0.28267182962245885,0.28170377541142305,0.2807357212003872,0.2797676669893514,0.2787996127783156,0.27783155856727976,0.27686350435624396,0.2758954501452081,0.2749273959341723,0.2739593417231365,0.27299128751210067,0.27202323330106487,0.271055179090029,0.2700871248789932,0.2691190706679574,0.2681510164569216,0.2671829622458858,0.26621490803484993,0.26524685382381413,0.26524685382381413,0.26427879961277834,0.2633107454017425,0.2623426911907067,0.26137463697967084,0.26040658276863504,0.25943852855759925,0.2584704743465634,0.2575024201355276,0.25653436592449175,0.25556631171345595,0.25459825750242016,0.2536302032913843,0.2526621490803485,0.25169409486931266,0.25072604065827686,0.24975798644724104,0.24878993223620524,0.24782187802516942,0.2468538238141336,0.24588576960309777,0.24491771539206195,0.24394966118102615,0.24298160696999033,0.2420135527589545,0.24104549854791868,0.24007744433688286,0.23910939012584706,0.23910939012584706,0.23814133591481124,0.2371732817037754,0.2362052274927396,0.23523717328170377,0.23426911907066797,0.23330106485963215,0.23233301064859632,0.2313649564375605,0.23039690222652467,0.22942884801548888,0.22846079380445306,0.22749273959341723,0.2265246853823814,0.22555663117134558,0.2245885769603098,0.22362052274927396,0.22265246853823814,0.22168441432720232,0.2207163601161665,0.2197483059051307,0.21878025169409487,0.21781219748305905,0.21684414327202323,0.2158760890609874,0.2149080348499516,0.21393998063891578,0.21297192642787996,0.21200387221684414,0.2110358180058083,0.21006776379477252,0.2090997095837367,0.20813165537270087,0.20716360116166505,0.20619554695062922,0.20522749273959343,0.2042594385285576,0.20329138431752178,0.20232333010648595,0.20135527589545016,0.20038722168441434,0.1994191674733785,0.1984511132623427,0.19748305905130686,0.19651500484027107,0.19554695062923524,0.19457889641819942,0.1936108422071636,0.19264278799612777,0.19167473378509198,0.19070667957405615,0.18973862536302033,0.1887705711519845,0.18780251694094868,0.1868344627299129,0.18586640851887706,0.18489835430784124,0.18489835430784124,0.18393030009680542,0.1829622458857696,0.1819941916747338,0.18102613746369797,0.18005808325266215,0.17909002904162633,0.1781219748305905,0.1771539206195547,0.17618586640851888,0.17521781219748306,0.17424975798644723,0.1732817037754114,0.17231364956437561,0.1713455953533398,0.1713455953533398,0.17037754114230397,0.16940948693126814,0.16844143272023232,0.16747337850919652,0.1665053242981607,0.16553727008712488,0.16456921587608905,0.16360116166505323,0.16263310745401743,0.1616650532429816,0.1606969990319458,0.15972894482090996,0.15876089060987417,0.15779283639883834,0.15682478218780252,0.1558567279767667,0.15488867376573087,0.15392061955469508,0.15295256534365925,0.15198451113262343,0.1510164569215876,0.15004840271055178,0.14908034849951599,0.14811229428848016,0.14714424007744434,0.14617618586640851,0.1452081316553727,0.1442400774443369,0.14327202323330107,0.14230396902226525,0.14133591481122942,0.1403678606001936,0.1403678606001936,0.1393998063891578,0.13843175217812198,0.13746369796708616,0.13649564375605033,0.1355275895450145,0.1345595353339787,0.1345595353339787,0.1335914811229429,0.13262342691190707,0.13165537270087124,0.13068731848983542,0.12971926427879962,0.1287512100677638,0.12778315585672798,0.12681510164569215,0.12584704743465633,0.12487899322362052,0.12391093901258471,0.12294288480154889,0.12197483059051308,0.12100677637947725,0.12003872216844143,0.1181026137463698,0.11713455953533398,0.11616650532429816,0.11519845111326234,0.11423039690222653,0.1132623426911907,0.1122942884801549,0.11132623426911907,0.11035818005808325,0.10939012584704744,0.10842207163601161,0.1074540174249758,0.10648596321393998,0.10551790900290416,0.10454985479186835,0.10358180058083252,0.10261374636979671,0.10164569215876089,0.10067763794772508,0.09970958373668926,0.09874152952565343,0.09777347531461762,0.0968054211035818,0.09583736689254599,0.09486931268151017,0.09390125847047434,0.09293320425943853,0.09196515004840271,0.0909970958373669,0.09002904162633107,0.08906098741529525,0.08809293320425944,0.08712487899322362,0.08615682478218781,0.08518877057115198,0.08422071636011616,0.08325266214908035,0.08228460793804453,0.08131655372700872,0.0803484995159729,0.07938044530493708,0.07841239109390126,0.07744433688286544,0.07647628267182963,0.0755082284607938,0.07454017424975799,0.07357212003872217,0.07260406582768635,0.07163601161665054,0.07066795740561471,0.0696999031945789,0.06873184898354308,0.06776379477250725,0.06679574056147145,0.06582768635043562,0.06485963213939981,0.06389157792836399,0.06292352371732816,0.061955469506292354,0.06098741529525654,0.060019361084220714,0.0590513068731849,0.05808325266214908,0.057115198451113264,0.05614714424007745,0.05517909002904162,0.05421103581800581,0.05324298160696999,0.05227492739593417,0.051306873184898356,0.05033881897386254,0.049370764762826716,0.0484027105517909,0.04743465634075508,0.046466602129719266,0.04549854791868345,0.044530493707647625,0.04356243949661181,0.04259438528557599,0.041626331074540175,0.04065827686350436,0.03969022265246854,0.03872216844143272,0.0377541142303969,0.03388189738625363,0.03291384317521781,0.031945788964181994,0.030977734753146177,0.02904162633107454,0.028073572120038724,0.027105517909002903,0.026137463697967087,0.02420135527589545,0.023233301064859633,0.022265246853823813,0.021297192642787996,0.01936108422071636,0.016456921587608905,0.015488867376573089,0.01452081316553727,0.013552758954501452,0.011616650532429816,0.00968054211035818,0],"xaxis":"x","y":[0.7341862117981521,0.7347083926031295,0.7352313167259786,0.7357549857549858,0.7362794012829651,0.7368045649072753,0.7373304782298359,0.7378571428571429,0.7383845604002859,0.7389127324749643,0.7394416607015032,0.7399713467048711,0.7405017921146954,0.7410329985652798,0.741564967695621,0.7420977011494253,0.7426312005751258,0.7431654676258993,0.7437005039596832,0.7442363112391931,0.7447728911319395,0.7453102453102453,0.7458483754512636,0.7463872832369942,0.7469269703543022,0.7474674384949349,0.7480086893555394,0.7485507246376811,0.7490935460478607,0.7496371552975326,0.7501815541031227,0.7507267441860465,0.7512727272727273,0.7518195050946143,0.7516387472687546,0.7521865889212828,0.7527352297592997,0.7532846715328467,0.7538349159970782,0.7543859649122807,0.7549378200438918,0.7554904831625183,0.756043956043956,0.7565982404692082,0.7571533382245048,0.7577092511013216,0.7582659808963997,0.7580882352941176,0.7586460632818248,0.7592047128129602,0.7597641857037583,0.7603244837758112,0.7608856088560886,0.7614475627769571,0.762010347376201,0.7625739644970414,0.7631384159881569,0.762962962962963,0.7627872498146775,0.7633531157270029,0.7639198218262806,0.7644873699851411,0.7650557620817844,0.765625,0.7661950856291884,0.7660208643815202,0.7665920954511558,0.7671641791044777,0.7677371172516804,0.7683109118086696,0.768885564697083,0.7694610778443114,0.7700374531835206,0.7706146926536732,0.7711927981995499,0.7717717717717718,0.772351615326822,0.7729323308270677,0.7735139202407826,0.7740963855421686,0.774679728711379,0.7745098039215687,0.7750943396226415,0.775679758308157,0.7762660619803476,0.7768532526475038,0.7766843300529902,0.7772727272727272,0.7778620166793025,0.7784522003034902,0.7790432801822323,0.7796352583586627,0.779467680608365,0.7792998477929984,0.7798933739527799,0.7804878048780488,0.7810831426392068,0.7816793893129771,0.7822765469824293,0.7828746177370031,0.7834736036725325,0.7833078101071975,0.7839080459770115,0.7845092024539877,0.7851112816577129,0.7857142857142857,0.7863182167563413,0.786923076923077,0.7875288683602771,0.788135593220339,0.7879722436391673,0.7885802469135802,0.7891891891891892,0.7890262751159196,0.7896365042536736,0.7902476780185759,0.790859798605732,0.7914728682170543,0.7920868890612878,0.7919254658385093,0.7925407925407926,0.7931570762052877,0.7937743190661478,0.794392523364486,0.7950116913484022,0.7956318252730109,0.7962529274004684,0.796875,0.7967161845191556,0.797339593114241,0.79796397807361,0.79858934169279,0.7992156862745098,0.7998430141287284,0.800471327572663,0.8011006289308176,0.8017309205350118,0.8023622047244094,0.8029944838455477,0.8028391167192429,0.8034727703235991,0.8033175355450237,0.8039525691699605,0.8045886075949367,0.8044338875692795,0.8050713153724247,0.8057097541633624,0.8063492063492064,0.806989674344718,0.8076311605723371,0.807478122513922,0.8081210191082803,0.8087649402390438,0.8094098883572568,0.8100558659217877,0.8099041533546326,0.8105515587529976,0.8112,0.8118494795836669,0.8125,0.8131515637530072,0.8130016051364366,0.8136546184738955,0.8143086816720257,0.8149637972646823,0.8148148148148148,0.814665592264303,0.8153225806451613,0.8159806295399515,0.815831987075929,0.8164915117219078,0.8171521035598706,0.8170040485829959,0.8176661264181524,0.8183292781832928,0.8189935064935064,0.8196588139723802,0.8203252032520325,0.8201790073230268,0.8208469055374593,0.8207008964955175,0.8213703099510603,0.8220408163265306,0.8227124183006536,0.8233851185609158,0.823240589198036,0.823914823914824,0.8245901639344262,0.8252666119770303,0.8251231527093597,0.8258011503697618,0.8264802631578947,0.8263374485596707,0.8270181219110379,0.8268755152514426,0.8267326732673267,0.8274153592072667,0.828099173553719,0.8287841191066998,0.8294701986754967,0.8301574150787076,0.8308457711442786,0.8307053941908714,0.8313953488372093,0.8320864505403158,0.8327787021630616,0.832639467110741,0.8333333333333334,0.8331943286071727,0.8330550918196995,0.83375104427736,0.8336120401337793,0.8334728033472804,0.8333333333333334,0.8340318524727578,0.834731543624161,0.834592779177162,0.8352941176470589,0.8359966358284272,0.8358585858585859,0.8365627632687447,0.836424957841484,0.8371308016877637,0.8378378378378378,0.8377007607776839,0.8375634517766497,0.8374259102455546,0.8372881355932204,0.8379983036471587,0.8387096774193549,0.8394222599830077,0.8401360544217688,0.84,0.8407155025553663,0.8414322250639387,0.841296928327645,0.8420153714773698,0.8418803418803419,0.8417450812660393,0.8424657534246576,0.8431876606683805,0.8439108061749572,0.8446351931330472,0.845360824742268,0.8460877042132416,0.846815834767642,0.8475452196382429,0.8474137931034482,0.8481449525452976,0.8488773747841105,0.8487467588591184,0.8494809688581315,0.8502164502164502,0.8509532062391681,0.8516912402428447,0.8524305555555556,0.8531711555169418,0.8530434782608696,0.8537859007832899,0.8536585365853658,0.8544027898866609,0.8551483420593369,0.8558951965065502,0.8566433566433567,0.8573928258967629,0.8572679509632224,0.8571428571428571,0.8578947368421053,0.8577699736611062,0.8576449912126538,0.8583992963940194,0.8591549295774648,0.8590308370044053,0.8597883597883598,0.8596646072374228,0.8604240282685512,0.8603006189213086,0.8610619469026549,0.8618246235606731,0.8617021276595744,0.8615794143744454,0.8614564831261101,0.8622222222222222,0.8620996441281139,0.8619768477292965,0.8627450980392157,0.8626226583407671,0.8633928571428572,0.8641644325290437,0.8649373881932021,0.8657117278424351,0.8655913978494624,0.8654708520179372,0.8662477558348295,0.867026055705301,0.8669064748201439,0.8667866786678667,0.8675675675675676,0.8683498647430117,0.868231046931408,0.8690153568202349,0.8698010849909584,0.8705882352941177,0.8704710144927537,0.8712601994560291,0.8711433756805808,0.8719346049046321,0.8727272727272727,0.8726114649681529,0.8734061930783242,0.8742023701002735,0.8740875912408759,0.8748858447488584,0.8747714808043876,0.8746569075937786,0.8745421245421245,0.8744271310724107,0.8752293577981651,0.8760330578512396,0.8768382352941176,0.8767249310027599,0.8766114180478821,0.8774193548387097,0.8782287822878229,0.8790397045244691,0.8798521256931608,0.879740980573543,0.8805555555555555,0.8804448563484708,0.8803339517625232,0.8802228412256268,0.8801115241635687,0.88,0.8808193668528864,0.880708294501398,0.8805970149253731,0.880485527544351,0.8803738317757009,0.8802619270346118,0.8801498127340824,0.880037488284911,0.8799249530956847,0.8807511737089202,0.8806390977443609,0.8814675446848542,0.8813559322033898,0.88124410933082,0.8820754716981132,0.8819641170915958,0.8818525519848771,0.8826868495742668,0.8835227272727273,0.8834123222748815,0.8842504743833017,0.8841405508072174,0.8840304182509505,0.884871550903901,0.8857142857142857,0.8856053384175405,0.8854961832061069,0.8853868194842407,0.8852772466539197,0.8851674641148325,0.8850574712643678,0.8849472674976031,0.8857965451055663,0.8866474543707973,0.8865384615384615,0.8864292589027911,0.8863198458574181,0.8862102217936355,0.887065637065637,0.8869565217391304,0.8868471953578336,0.8877057115198451,0.8885658914728682,0.8894277400581959,0.8893203883495145,0.8901846452866861,0.8900778210116731,0.8899707887049659,0.8898635477582846,0.8897560975609756,0.890625,0.8914956011730205,0.8923679060665362,0.8922624877571009,0.8921568627450981,0.8920510304219823,0.8929273084479371,0.8928220255653884,0.8937007874015748,0.8935960591133005,0.8944773175542406,0.8943731490621916,0.8942687747035574,0.8951533135509396,0.8950495049504951,0.8949454905847374,0.8948412698412699,0.8947368421052632,0.8946322067594433,0.8945273631840795,0.8954183266932271,0.8953140578265204,0.8952095808383234,0.8961038961038961,0.896,0.8958958958958959,0.8967935871743486,0.8966900702106319,0.8975903614457831,0.8984924623115578,0.8993963782696177,0.9003021148036254,0.9012096774193549,0.9011099899091827,0.901010101010101,0.9009100101112234,0.9008097165991903,0.900709219858156,0.9016227180527383,0.9015228426395939,0.9024390243902439,0.9033570701932858,0.9032586558044806,0.9031600407747197,0.9040816326530612,0.9039836567926456,0.9038854805725971,0.9037871033776868,0.9036885245901639,0.9046153846153846,0.9045174537987679,0.9044193216855088,0.9053497942386831,0.9062821833161689,0.9061855670103093,0.9060887512899897,0.90599173553719,0.905894519131334,0.9057971014492754,0.9067357512953368,0.9066390041493776,0.9065420560747663,0.9064449064449065,0.9073881373569199,0.9072916666666667,0.9071949947862357,0.9081419624217119,0.9080459770114943,0.9079497907949791,0.9078534031413612,0.9077568134171907,0.9076600209863589,0.907563025210084,0.907465825446898,0.9073684210526316,0.9083245521601686,0.9082278481012658,0.9081309398099261,0.9080338266384778,0.908994708994709,0.9088983050847458,0.9098621420996819,0.910828025477707,0.9117959617428267,0.9117021276595745,0.9116080937167199,0.9115138592750534,0.9114194236926361,0.9113247863247863,0.9122994652406418,0.9122055674518201,0.9121114683815649,0.9120171673819742,0.9119226638023631,0.9129032258064517,0.9128094725511302,0.9127155172413793,0.912621359223301,0.9125269978401728,0.9124324324324324,0.9123376623376623,0.9122426868905742,0.9121475054229935,0.9120521172638436,0.9119565217391304,0.911860718171926,0.9117647058823529,0.9116684841875682,0.9126637554585153,0.9136612021857924,0.9135667396061269,0.9134720700985761,0.9133771929824561,0.9143798024149287,0.9142857142857143,0.9141914191419142,0.9140969162995595,0.9151047409040793,0.9161147902869757,0.9160220994475138,0.915929203539823,0.9158361018826136,0.9168514412416852,0.9167591564927858,0.9177777777777778,0.917686318131257,0.9175946547884187,0.9175027870680045,0.9174107142857143,0.9184357541899442,0.9183445190156599,0.9182530795072789,0.9181614349775785,0.9180695847362514,0.9179775280898876,0.9178852643419573,0.9177927927927928,0.9177001127395716,0.917607223476298,0.9175141242937853,0.917420814479638,0.9184597961494904,0.9183673469387755,0.9182746878547106,0.9181818181818182,0.919226393629124,0.9191343963553531,0.9201824401368301,0.9200913242009132,0.92,0.919908466819222,0.9198167239404352,0.9197247706422018,0.9196326061997704,0.9206896551724137,0.9205983889528193,0.9216589861751152,0.922722029988466,0.9226327944572749,0.922543352601156,0.9224537037037037,0.9235225955967555,0.9234338747099768,0.924506387921022,0.9244186046511628,0.9254947613504074,0.9265734265734266,0.926487747957993,0.9264018691588785,0.9263157894736842,0.927400468384075,0.9273153575615475,0.9272300469483568,0.927144535840188,0.9270588235294117,0.928150765606596,0.9292452830188679,0.9291617473435655,0.9302600472813238,0.9301775147928995,0.9312796208530806,0.9311981020166074,0.9323040380047506,0.9322235434007135,0.9321428571428572,0.932061978545888,0.9331742243436754,0.9330943847072879,0.9342105263157895,0.9341317365269461,0.935251798561151,0.9351740696278511,0.9350961538461539,0.9350180505415162,0.9349397590361446,0.9348612786489746,0.9347826086956522,0.9347037484885127,0.9346246973365617,0.9357575757575758,0.9356796116504854,0.9356014580801945,0.9355231143552312,0.9354445797807551,0.9353658536585366,0.9365079365079365,0.9364303178484108,0.9363525091799265,0.9362745098039216,0.9361963190184049,0.9361179361179361,0.9360393603936039,0.9359605911330049,0.93711467324291,0.937037037037037,0.9381953028430161,0.9381188118811881,0.9380421313506815,0.9379652605459057,0.937888198757764,0.9378109452736318,0.937733499377335,0.9376558603491272,0.9375780274656679,0.9375,0.9374217772215269,0.9373433583959899,0.9372647427854455,0.9371859296482412,0.9383647798742139,0.9382871536523929,0.9382093316519546,0.9381313131313131,0.9380530973451328,0.9379746835443038,0.9378960709759189,0.9378172588832487,0.9377382465057179,0.9376590330788804,0.9375796178343949,0.9375,0.9386973180076629,0.9386189258312021,0.93854033290653,0.9384615384615385,0.938382541720154,0.9383033419023136,0.9382239382239382,0.9381443298969072,0.9380645161290323,0.937984496124031,0.9379042690815006,0.9378238341968912,0.9390402075226978,0.938961038961039,0.9388816644993498,0.9388020833333334,0.9387222946544981,0.9386422976501305,0.938562091503268,0.9384816753926701,0.9384010484927916,0.9383202099737533,0.938239159001314,0.9381578947368421,0.9380764163372859,0.9379947229551451,0.9379128137384413,0.9378306878306878,0.937748344370861,0.9376657824933687,0.9375830013280213,0.9375,0.9374167776298269,0.9386666666666666,0.9385847797062751,0.9385026737967914,0.9384203480589023,0.9396782841823056,0.9395973154362416,0.9395161290322581,0.9394347240915208,0.9393530997304582,0.9392712550607287,0.9391891891891891,0.939106901217862,0.9390243902439024,0.9389416553595658,0.938858695652174,0.9387755102040817,0.9400544959128065,0.9399727148703957,0.9398907103825137,0.9398084815321477,0.9410958904109589,0.9410150891632373,0.9409340659340659,0.9408528198074277,0.940771349862259,0.9406896551724138,0.9406077348066298,0.9405255878284924,0.9404432132963989,0.9403606102635229,0.9402777777777778,0.9401947148817803,0.9415041782729805,0.9414225941422594,0.9427374301675978,0.9426573426573427,0.9425770308123249,0.9424964936886395,0.9424157303370787,0.9423347398030942,0.9422535211267605,0.9421720733427362,0.942090395480226,0.942008486562942,0.943342776203966,0.9432624113475178,0.9431818181818182,0.9431009957325747,0.9430199430199431,0.9429386590584878,0.9428571428571428,0.9427753934191703,0.9426934097421203,0.9426111908177905,0.9425287356321839,0.943884892086331,0.9438040345821326,0.9437229437229437,0.9436416184971098,0.9435600578871202,0.9434782608695652,0.9433962264150944,0.9433139534883721,0.9446870451237264,0.9446064139941691,0.945985401459854,0.945906432748538,0.9458272327964861,0.9472140762463344,0.947136563876652,0.9470588235294117,0.946980854197349,0.9469026548672567,0.946824224519941,0.9467455621301775,0.9466666666666667,0.9465875370919882,0.9479940564635958,0.9494047619047619,0.9508196721311475,0.9507462686567164,0.9506726457399103,0.9505988023952096,0.9505247376311844,0.9504504504504504,0.9503759398496241,0.9503012048192772,0.9502262443438914,0.9501510574018127,0.9500756429652042,0.95,0.9499241274658573,0.9498480243161094,0.9497716894977168,0.9496951219512195,0.9496183206106871,0.9495412844036697,0.9494640122511485,0.9493865030674846,0.9493087557603687,0.9492307692307692,0.9491525423728814,0.9490740740740741,0.9505409582689336,0.9504643962848297,0.951937984496124,0.953416149068323,0.9548989113530326,0.9548286604361371,0.9547581903276131,0.9546875,0.9546165884194053,0.9545454545454546,0.9544740973312402,0.9544025157232704,0.9543307086614173,0.9542586750788643,0.9541864139020537,0.9541139240506329,0.9540412044374009,0.953968253968254,0.9538950715421304,0.9538216560509554,0.9553429027113237,0.9552715654952076,0.9552,0.9567307692307693,0.956661316211878,0.9565916398713826,0.9565217391304348,0.9564516129032258,0.9563812600969306,0.9563106796116505,0.9562398703403565,0.9561688311688312,0.9560975609756097,0.9560260586319218,0.9559543230016313,0.9558823529411765,0.955810147299509,0.9557377049180328,0.9556650246305419,0.9555921052631579,0.9555189456342669,0.9554455445544554,0.9553719008264463,0.9552980132450332,0.9552238805970149,0.9551495016611296,0.9550748752079867,0.9566666666666667,0.9565943238731218,0.9581939799331104,0.9581239530988275,0.9580536912751678,0.957983193277311,0.9579124579124579,0.9578414839797639,0.9577702702702703,0.9593908629441624,0.9610169491525423,0.9609507640067911,0.9608843537414966,0.9608177172061328,0.9607508532423208,0.9606837606837607,0.9606164383561644,0.9605488850771869,0.9604810996563574,0.9604130808950087,0.9603448275862069,0.9602763385146805,0.9602076124567474,0.9601386481802426,0.9600694444444444,0.96,0.9599303135888502,0.9616055846422339,0.9615384615384616,0.9614711033274956,0.9614035087719298,0.961335676625659,0.9612676056338029,0.9611992945326279,0.9611307420494699,0.9610619469026549,0.9609929078014184,0.9609236234458259,0.9608540925266904,0.9607843137254902,0.9607142857142857,0.960644007155635,0.9623655913978495,0.9640933572710951,0.9640287769784173,0.963963963963964,0.9638989169675091,0.9638336347197106,0.9637681159420289,0.9655172413793104,0.9654545454545455,0.9653916211293261,0.9653284671532847,0.9652650822669104,0.9652014652014652,0.9651376146788991,0.9669117647058824,0.9668508287292817,0.966789667896679,0.966728280961183,0.9666666666666667,0.9666048237476809,0.966542750929368,0.9664804469273743,0.9682835820895522,0.9682242990654205,0.9681647940074907,0.9681050656660413,0.9680451127819549,0.967984934086629,0.9679245283018868,0.9678638941398866,0.9678030303030303,0.967741935483871,0.967680608365019,0.9676190476190476,0.9675572519083969,0.9694072657743786,0.9693486590038314,0.9712092130518234,0.9711538461538461,0.9710982658959537,0.971042471042471,0.9709864603481625,0.9709302325581395,0.970873786407767,0.9708171206225681,0.9707602339181286,0.970703125,0.9706457925636007,0.9705882352941176,0.9705304518664047,0.9704724409448819,0.9704142011834319,0.9703557312252964,0.9702970297029703,0.9702380952380952,0.9701789264413518,0.9701195219123506,0.9700598802395209,0.97,0.969939879759519,0.9698795180722891,0.9698189134808853,0.969758064516129,0.9717171717171718,0.97165991902834,0.9716024340770791,0.9715447154471545,0.9714867617107943,0.9714285714285714,0.9713701431492843,0.9713114754098361,0.971252566735113,0.9711934156378601,0.9711340206185567,0.9710743801652892,0.9710144927536232,0.970954356846473,0.9708939708939709,0.9708333333333333,0.9707724425887265,0.9707112970711297,0.9727463312368972,0.9726890756302521,0.9726315789473684,0.9725738396624473,0.9725158562367865,0.972457627118644,0.9723991507430998,0.9723404255319149,0.9722814498933902,0.9722222222222222,0.9721627408993576,0.9721030042918455,0.9720430107526882,0.9719827586206896,0.9719222462203023,0.9718614718614719,0.9718004338394793,0.9717391304347827,0.971677559912854,0.9716157205240175,0.9715536105032823,0.9714912280701754,0.9714285714285714,0.9713656387665198,0.9713024282560706,0.9712389380530974,0.9711751662971175,0.9711111111111111,0.9732739420935412,0.9732142857142857,0.9731543624161074,0.9730941704035875,0.9730337078651685,0.972972972972973,0.9729119638826185,0.9728506787330317,0.9727891156462585,0.9727272727272728,0.9726651480637813,0.9726027397260274,0.9725400457665904,0.9724770642201835,0.9747126436781609,0.9746543778801844,0.9745958429561201,0.9745370370370371,0.974477958236659,0.9744186046511628,0.9743589743589743,0.9742990654205608,0.9742388758782201,0.9741784037558685,0.9741176470588235,0.9740566037735849,0.9739952718676123,0.9739336492890995,0.9738717339667459,0.9738095238095238,0.9737470167064439,0.9736842105263158,0.973621103117506,0.9735576923076923,0.9734939759036144,0.9734299516908212,0.9733656174334141,0.9733009708737864,0.9732360097323601,0.973170731707317,0.9731051344743277,0.9730392156862745,0.972972972972973,0.9729064039408867,0.9728395061728395,0.9727722772277227,0.9727047146401985,0.972636815920398,0.972568578553616,0.9725,0.9724310776942355,0.9723618090452262,0.9722921914357683,0.9722222222222222,0.9721518987341772,0.9720812182741116,0.9720101781170484,0.9719387755102041,0.9718670076726342,0.9743589743589743,0.974293059125964,0.9742268041237113,0.9741602067183462,0.9740932642487047,0.9766233766233766,0.9765625,0.9765013054830287,0.9764397905759162,0.9763779527559056,0.9763157894736842,0.978891820580475,0.9788359788359788,0.9787798408488063,0.9787234042553191,0.9813333333333333,0.9812834224598931,0.9812332439678284,0.9811827956989247,0.9811320754716981,0.981081081081081,0.981029810298103,0.9809782608695652,0.9809264305177112,0.9808743169398907,0.9808219178082191,0.9807692307692307,0.9807162534435262,0.9806629834254144,0.9806094182825484,0.9805555555555555,0.9805013927576601,0.9804469273743017,0.9803921568627451,0.9803370786516854,0.9802816901408451,0.980225988700565,0.9801699716713881,0.9801136363636364,0.98005698005698,0.98,0.9799426934097422,0.9798850574712644,0.9798270893371758,0.9797687861271677,0.9797101449275363,0.9796511627906976,0.9795918367346939,0.97953216374269,0.9794721407624634,0.9794117647058823,0.9793510324483776,0.9792899408284024,0.9792284866468842,0.9791666666666666,0.9791044776119403,0.9790419161676647,0.978978978978979,0.9789156626506024,0.9788519637462235,0.9787878787878788,0.9787234042553191,0.9786585365853658,0.9785932721712538,0.9785276073619632,0.9784615384615385,0.9783950617283951,0.978328173374613,0.9782608695652174,0.9781931464174455,0.978125,0.9780564263322884,0.9779874213836478,0.9779179810725552,0.9778481012658228,0.9777777777777777,0.9777070063694268,0.9776357827476039,0.9775641025641025,0.977491961414791,0.9774193548387097,0.9805825242718447,0.9805194805194806,0.9804560260586319,0.9803921568627451,0.980327868852459,0.9802631578947368,0.9801980198019802,0.9801324503311258,0.9800664451827242,0.98,0.979933110367893,0.9798657718120806,0.9797979797979798,0.9797297297297297,0.9796610169491525,0.9795918367346939,0.9795221843003413,0.9794520547945206,0.979381443298969,0.9793103448275862,0.9792387543252595,0.9791666666666666,0.9790940766550522,0.9790209790209791,0.9789473684210527,0.9788732394366197,0.9787985865724381,0.9787234042553191,0.9786476868327402,0.9785714285714285,0.982078853046595,0.9820143884892086,0.9819494584837545,0.9818840579710145,0.9818181818181818,0.9817518248175182,0.9816849816849816,0.9816176470588235,0.981549815498155,0.9814814814814815,0.9814126394052045,0.9813432835820896,0.9812734082397003,0.981203007518797,0.9811320754716981,0.9810606060606061,0.9809885931558935,0.9809160305343512,0.9808429118773946,0.9807692307692307,0.9806949806949807,0.9806201550387597,0.980544747081712,0.98046875,0.9803921568627451,0.9803149606299213,0.9802371541501976,0.9801587301587301,0.9840637450199203,0.984,0.9839357429718876,0.9838709677419355,0.9838056680161943,0.983739837398374,0.9836734693877551,0.9836065573770492,0.9835390946502057,0.9834710743801653,0.983402489626556,0.9833333333333333,0.9832635983263598,0.9831932773109243,0.9831223628691983,0.9830508474576272,0.9829787234042553,0.9829059829059829,0.9828326180257511,0.9827586206896551,0.9826839826839827,0.9826086956521739,0.982532751091703,0.9824561403508771,0.9823788546255506,0.9823008849557522,0.9822222222222222,0.9821428571428571,0.9820627802690582,0.9819819819819819,0.9819004524886877,0.9818181818181818,0.9817351598173516,0.981651376146789,0.9815668202764977,0.9814814814814815,0.9813953488372092,0.9813084112149533,0.9812206572769953,0.9811320754716981,0.981042654028436,0.9809523809523809,0.9808612440191388,0.9807692307692307,0.9806763285024155,0.9805825242718447,0.9804878048780488,0.9803921568627451,0.9802955665024631,0.9801980198019802,0.9800995024875622,0.98,0.9798994974874372,0.9797979797979798,0.9796954314720813,0.9795918367346939,0.9794871794871794,0.9845360824742269,0.9844559585492227,0.984375,0.9842931937172775,0.9842105263157894,0.9841269841269841,0.9840425531914894,0.983957219251337,0.9838709677419355,0.9837837837837838,0.9836956521739131,0.9836065573770492,0.9835164835164835,0.9834254143646409,0.9833333333333333,0.9888268156424581,0.9887640449438202,0.9887005649717514,0.9886363636363636,0.9885714285714285,0.9885057471264368,0.9884393063583815,0.9883720930232558,0.9883040935672515,0.9882352941176471,0.9881656804733728,0.9880952380952381,0.9880239520958084,0.9879518072289156,0.9878787878787879,0.9878048780487805,0.9877300613496932,0.9876543209876543,0.9875776397515528,0.9875,0.9874213836477987,0.9873417721518988,0.9872611464968153,0.9871794871794872,0.9870967741935484,0.987012987012987,0.9869281045751634,0.9868421052631579,0.9867549668874173,0.9866666666666667,0.9865771812080537,0.9864864864864865,0.9863945578231292,0.9931506849315068,0.993103448275862,0.9930555555555556,0.993006993006993,0.9929577464788732,0.9929078014184397,0.9928571428571429,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1],"yaxis":"y"}],"layout":{"height":600,"legend":{"tracegroupgap":0},"shapes":[{"line":{"dash":"dash"},"type":"line","x0":0,"x1":1,"y0":1,"y1":0}],"showlegend":false,"template":{"data":{"bar":[{"error_x":{"color":"#2a3f5f"},"error_y":{"color":"#2a3f5f"},"marker":{"line":{"color":"white","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"barpolar":[{"marker":{"line":{"color":"white","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"carpet":[{"aaxis":{"endlinecolor":"#2a3f5f","gridcolor":"#C8D4E3","linecolor":"#C8D4E3","minorgridcolor":"#C8D4E3","startlinecolor":"#2a3f5f"},"baxis":{"endlinecolor":"#2a3f5f","gridcolor":"#C8D4E3","linecolor":"#C8D4E3","minorgridcolor":"#C8D4E3","startlinecolor":"#2a3f5f"},"type":"carpet"}],"choropleth":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"choropleth"}],"contour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"contour"}],"contourcarpet":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"contourcarpet"}],"heatmap":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmap"}],"heatmapgl":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmapgl"}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"histogram2d":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2d"}],"histogram2dcontour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2dcontour"}],"mesh3d":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"mesh3d"}],"parcoords":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"parcoords"}],"pie":[{"automargin":true,"type":"pie"}],"scatter":[{"fillpattern":{"fillmode":"overlay","size":10,"solidity":0.2},"type":"scatter"}],"scatter3d":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter3d"}],"scattercarpet":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattercarpet"}],"scattergeo":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergeo"}],"scattergl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergl"}],"scattermapbox":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattermapbox"}],"scatterpolar":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolar"}],"scatterpolargl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolargl"}],"scatterternary":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterternary"}],"surface":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"surface"}],"table":[{"cells":{"fill":{"color":"#EBF0F8"},"line":{"color":"white"}},"header":{"fill":{"color":"#C8D4E3"},"line":{"color":"white"}},"type":"table"}]},"layout":{"annotationdefaults":{"arrowcolor":"#2a3f5f","arrowhead":0,"arrowwidth":1},"autotypenumbers":"strict","coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]],"sequential":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"sequentialminus":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]]},"colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#2a3f5f"},"geo":{"bgcolor":"white","lakecolor":"white","landcolor":"white","showlakes":true,"showland":true,"subunitcolor":"#C8D4E3"},"hoverlabel":{"align":"left"},"hovermode":"closest","mapbox":{"style":"light"},"paper_bgcolor":"white","plot_bgcolor":"white","polar":{"angularaxis":{"gridcolor":"#EBF0F8","linecolor":"#EBF0F8","ticks":""},"bgcolor":"white","radialaxis":{"gridcolor":"#EBF0F8","linecolor":"#EBF0F8","ticks":""}},"scene":{"xaxis":{"backgroundcolor":"white","gridcolor":"#DFE8F3","gridwidth":2,"linecolor":"#EBF0F8","showbackground":true,"ticks":"","zerolinecolor":"#EBF0F8"},"yaxis":{"backgroundcolor":"white","gridcolor":"#DFE8F3","gridwidth":2,"linecolor":"#EBF0F8","showbackground":true,"ticks":"","zerolinecolor":"#EBF0F8"},"zaxis":{"backgroundcolor":"white","gridcolor":"#DFE8F3","gridwidth":2,"linecolor":"#EBF0F8","showbackground":true,"ticks":"","zerolinecolor":"#EBF0F8"}},"shapedefaults":{"line":{"color":"#2a3f5f"}},"ternary":{"aaxis":{"gridcolor":"#DFE8F3","linecolor":"#A2B1C6","ticks":""},"baxis":{"gridcolor":"#DFE8F3","linecolor":"#A2B1C6","ticks":""},"bgcolor":"white","caxis":{"gridcolor":"#DFE8F3","linecolor":"#A2B1C6","ticks":""}},"title":{"x":0.05},"xaxis":{"automargin":true,"gridcolor":"#EBF0F8","linecolor":"#EBF0F8","ticks":"","title":{"standoff":15},"zerolinecolor":"#EBF0F8","zerolinewidth":2},"yaxis":{"automargin":true,"gridcolor":"#EBF0F8","linecolor":"#EBF0F8","ticks":"","title":{"standoff":15},"zerolinecolor":"#EBF0F8","zerolinewidth":2}}},"title":{"text":"Precision-Recall Curve (AUC=0.8840)"},"width":800,"xaxis":{"anchor":"y","constrain":"domain","domain":[0,1],"title":{"text":"Recall"}},"yaxis":{"anchor":"x","domain":[0,1],"scaleanchor":"x","scaleratio":1,"title":{"text":"Precision"}}}},"text/html":["<div>                            <div id=\"4a823447-63c4-43bf-9439-2f88513fa38e\" class=\"plotly-graph-div\" style=\"height:600px; width:800px;\"></div>            <script type=\"text/javascript\">                require([\"plotly\"], function(Plotly) {                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"4a823447-63c4-43bf-9439-2f88513fa38e\")) {                    Plotly.newPlot(                        \"4a823447-63c4-43bf-9439-2f88513fa38e\",                        [{\"fillpattern\":{\"shape\":\"\"},\"hovertemplate\":\"Recall=%{x}\\u003cbr\\u003ePrecision=%{y}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"\",\"line\":{\"color\":\"#636efa\"},\"marker\":{\"symbol\":\"circle\"},\"mode\":\"lines\",\"name\":\"\",\"orientation\":\"v\",\"showlegend\":false,\"stackgroup\":\"1\",\"x\":[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,0.9990319457889641,0.9990319457889641,0.9990319457889641,0.9990319457889641,0.9990319457889641,0.9990319457889641,0.9990319457889641,0.9990319457889641,0.9990319457889641,0.9990319457889641,0.9990319457889641,0.9990319457889641,0.9990319457889641,0.9980638915779284,0.9980638915779284,0.9980638915779284,0.9980638915779284,0.9980638915779284,0.9980638915779284,0.9980638915779284,0.9980638915779284,0.9980638915779284,0.9980638915779284,0.9970958373668926,0.9961277831558567,0.9961277831558567,0.9961277831558567,0.9961277831558567,0.9961277831558567,0.9961277831558567,0.9961277831558567,0.995159728944821,0.995159728944821,0.995159728944821,0.995159728944821,0.995159728944821,0.995159728944821,0.995159728944821,0.995159728944821,0.995159728944821,0.995159728944821,0.995159728944821,0.995159728944821,0.995159728944821,0.995159728944821,0.995159728944821,0.995159728944821,0.9941916747337851,0.9941916747337851,0.9941916747337851,0.9941916747337851,0.9941916747337851,0.9932236205227493,0.9932236205227493,0.9932236205227493,0.9932236205227493,0.9932236205227493,0.9932236205227493,0.9922555663117134,0.9912875121006777,0.9912875121006777,0.9912875121006777,0.9912875121006777,0.9912875121006777,0.9912875121006777,0.9912875121006777,0.9912875121006777,0.9903194578896418,0.9903194578896418,0.9903194578896418,0.9903194578896418,0.9903194578896418,0.9903194578896418,0.9903194578896418,0.9903194578896418,0.9903194578896418,0.989351403678606,0.989351403678606,0.989351403678606,0.9883833494675702,0.9883833494675702,0.9883833494675702,0.9883833494675702,0.9883833494675702,0.9883833494675702,0.9874152952565344,0.9874152952565344,0.9874152952565344,0.9874152952565344,0.9874152952565344,0.9874152952565344,0.9874152952565344,0.9874152952565344,0.9874152952565344,0.9864472410454985,0.9864472410454985,0.9864472410454985,0.9864472410454985,0.9864472410454985,0.9864472410454985,0.9864472410454985,0.9864472410454985,0.9864472410454985,0.9864472410454985,0.9864472410454985,0.9854791868344628,0.9854791868344628,0.9845111326234269,0.9845111326234269,0.9845111326234269,0.9835430784123911,0.9835430784123911,0.9835430784123911,0.9835430784123911,0.9835430784123911,0.9835430784123911,0.9825750242013552,0.9825750242013552,0.9825750242013552,0.9825750242013552,0.9825750242013552,0.9816069699903195,0.9816069699903195,0.9816069699903195,0.9816069699903195,0.9816069699903195,0.9816069699903195,0.9806389157792836,0.9806389157792836,0.9806389157792836,0.9806389157792836,0.9796708615682478,0.978702807357212,0.978702807357212,0.978702807357212,0.9777347531461762,0.9777347531461762,0.9777347531461762,0.9767666989351403,0.9767666989351403,0.9767666989351403,0.9767666989351403,0.9767666989351403,0.9767666989351403,0.9757986447241046,0.9757986447241046,0.9748305905130688,0.9748305905130688,0.9748305905130688,0.9748305905130688,0.9748305905130688,0.9738625363020329,0.9738625363020329,0.9738625363020329,0.9738625363020329,0.972894482090997,0.972894482090997,0.972894482090997,0.9719264278799613,0.9719264278799613,0.9709583736689255,0.9699903194578896,0.9699903194578896,0.9699903194578896,0.9699903194578896,0.9699903194578896,0.9699903194578896,0.9699903194578896,0.9690222652468539,0.9690222652468539,0.9690222652468539,0.9690222652468539,0.968054211035818,0.968054211035818,0.9670861568247822,0.9661181026137464,0.9661181026137464,0.9651500484027106,0.9641819941916747,0.9632139399806389,0.9632139399806389,0.9632139399806389,0.9622458857696031,0.9622458857696031,0.9622458857696031,0.9612778315585673,0.9612778315585673,0.9603097773475314,0.9603097773475314,0.9603097773475314,0.9593417231364957,0.9583736689254598,0.957405614714424,0.9564375605033882,0.9564375605033882,0.9564375605033882,0.9564375605033882,0.9564375605033882,0.9554695062923524,0.9554695062923524,0.9554695062923524,0.9545014520813165,0.9545014520813165,0.9535333978702807,0.952565343659245,0.952565343659245,0.952565343659245,0.952565343659245,0.952565343659245,0.952565343659245,0.952565343659245,0.952565343659245,0.952565343659245,0.9515972894482091,0.9515972894482091,0.9515972894482091,0.9506292352371732,0.9506292352371732,0.9506292352371732,0.9506292352371732,0.9506292352371732,0.9506292352371732,0.9506292352371732,0.9496611810261375,0.9496611810261375,0.9486931268151017,0.9486931268151017,0.9486931268151017,0.9486931268151017,0.9486931268151017,0.9486931268151017,0.9477250726040658,0.9467570183930301,0.9467570183930301,0.9457889641819942,0.9448209099709584,0.9448209099709584,0.9448209099709584,0.9438528557599225,0.9438528557599225,0.9428848015488868,0.9428848015488868,0.9419167473378509,0.9419167473378509,0.9419167473378509,0.9409486931268151,0.9399806389157793,0.9390125847047435,0.9390125847047435,0.9380445304937076,0.9370764762826719,0.9370764762826719,0.936108422071636,0.936108422071636,0.936108422071636,0.936108422071636,0.936108422071636,0.9351403678606002,0.9341723136495643,0.9341723136495643,0.9341723136495643,0.9332042594385286,0.9322362052274927,0.9322362052274927,0.9322362052274927,0.9312681510164569,0.9312681510164569,0.9312681510164569,0.9312681510164569,0.9303000968054211,0.9303000968054211,0.9293320425943853,0.9293320425943853,0.9293320425943853,0.9283639883833494,0.9283639883833494,0.9283639883833494,0.9273959341723137,0.9273959341723137,0.9264278799612778,0.925459825750242,0.9244917715392061,0.9235237173281704,0.9235237173281704,0.9235237173281704,0.9235237173281704,0.9225556631171346,0.9215876089060987,0.9215876089060987,0.9215876089060987,0.9215876089060987,0.9215876089060987,0.920619554695063,0.920619554695063,0.9196515004840271,0.9186834462729913,0.9177153920619555,0.9167473378509197,0.9157792836398838,0.9157792836398838,0.914811229428848,0.9138431752178122,0.9128751210067764,0.9119070667957405,0.9109390125847048,0.9099709583736689,0.9090029041626331,0.9080348499515973,0.9080348499515973,0.9070667957405615,0.9070667957405615,0.9060987415295256,0.9051306873184899,0.9051306873184899,0.904162633107454,0.9031945788964182,0.9031945788964182,0.9031945788964182,0.9022265246853823,0.9022265246853823,0.9012584704743466,0.9002904162633107,0.9002904162633107,0.9002904162633107,0.8993223620522749,0.8983543078412392,0.8973862536302033,0.8964181994191674,0.8954501452081317,0.8944820909970959,0.89351403678606,0.89351403678606,0.89351403678606,0.8925459825750242,0.8915779283639884,0.8906098741529526,0.8896418199419167,0.8896418199419167,0.888673765730881,0.8877057115198451,0.8877057115198451,0.8877057115198451,0.8877057115198451,0.8867376573088093,0.8867376573088093,0.8857696030977735,0.8848015488867377,0.8838334946757018,0.882865440464666,0.882865440464666,0.882865440464666,0.882865440464666,0.8818973862536302,0.8809293320425944,0.8799612778315585,0.8799612778315585,0.8789932236205228,0.8789932236205228,0.8780251694094869,0.8780251694094869,0.8770571151984511,0.8760890609874153,0.8760890609874153,0.8751210067763795,0.8741529525653436,0.8731848983543078,0.872216844143272,0.8712487899322362,0.8702807357212003,0.8702807357212003,0.8693126815101646,0.8683446272991288,0.8683446272991288,0.8673765730880929,0.8664085188770572,0.8664085188770572,0.8654404646660213,0.8654404646660213,0.8654404646660213,0.8654404646660213,0.8654404646660213,0.8654404646660213,0.8644724104549855,0.8635043562439496,0.8625363020329139,0.861568247821878,0.8606001936108422,0.8606001936108422,0.8596321393998064,0.8596321393998064,0.8596321393998064,0.8586640851887706,0.8576960309777347,0.8576960309777347,0.856727976766699,0.8557599225556631,0.8547918683446273,0.8538238141335914,0.8538238141335914,0.8528557599225557,0.8518877057115198,0.8518877057115198,0.8518877057115198,0.850919651500484,0.8499515972894482,0.8489835430784124,0.8480154888673765,0.8470474346563408,0.8470474346563408,0.846079380445305,0.8451113262342691,0.8441432720232332,0.8441432720232332,0.8431752178121975,0.8422071636011617,0.8422071636011617,0.8412391093901258,0.8402710551790901,0.8393030009680542,0.8383349467570184,0.8373668925459826,0.8363988383349468,0.8354307841239109,0.8344627299128751,0.8344627299128751,0.8334946757018393,0.8325266214908035,0.8315585672797676,0.8315585672797676,0.8305905130687319,0.8305905130687319,0.8305905130687319,0.8305905130687319,0.829622458857696,0.8286544046466602,0.8276863504356244,0.8267182962245886,0.8257502420135527,0.8257502420135527,0.8247821878025169,0.8238141335914811,0.8228460793804453,0.8218780251694094,0.8218780251694094,0.8209099709583737,0.8199419167473379,0.818973862536302,0.8180058083252663,0.8170377541142304,0.8160696999031946,0.8151016456921588,0.814133591481123,0.8131655372700871,0.8121974830590513,0.8112294288480155,0.8102613746369797,0.8092933204259438,0.8092933204259438,0.8092933204259438,0.8083252662149081,0.8073572120038722,0.8063891577928364,0.8063891577928364,0.8054211035818006,0.8044530493707648,0.8034849951597289,0.8034849951597289,0.8034849951597289,0.8025169409486931,0.8015488867376573,0.8005808325266215,0.8005808325266215,0.7996127783155856,0.7996127783155856,0.7986447241045499,0.797676669893514,0.7967086156824782,0.7957405614714425,0.7957405614714425,0.7947725072604066,0.7938044530493708,0.7928363988383349,0.7918683446272992,0.7909002904162633,0.7899322362052275,0.7889641819941917,0.7879961277831559,0.78702807357212,0.7860600193610843,0.7850919651500484,0.7850919651500484,0.7841239109390126,0.7831558567279767,0.782187802516941,0.782187802516941,0.7812197483059051,0.7812197483059051,0.7802516940948693,0.7792836398838335,0.7783155856727977,0.7773475314617618,0.7763794772507261,0.7754114230396902,0.7754114230396902,0.7744433688286544,0.7744433688286544,0.7744433688286544,0.7734753146176185,0.7725072604065828,0.771539206195547,0.771539206195547,0.7705711519845111,0.7705711519845111,0.7696030977734754,0.7696030977734754,0.7696030977734754,0.7686350435624395,0.7676669893514037,0.7666989351403679,0.7666989351403679,0.7657308809293321,0.7647628267182962,0.7637947725072604,0.7628267182962246,0.7628267182962246,0.7628267182962246,0.7618586640851888,0.7618586640851888,0.7608906098741529,0.7608906098741529,0.7599225556631172,0.7599225556631172,0.7589545014520813,0.7579864472410455,0.7570183930300097,0.7570183930300097,0.7560503388189739,0.7560503388189739,0.755082284607938,0.755082284607938,0.7541142303969022,0.7531461761858664,0.7521781219748306,0.7512100677637947,0.750242013552759,0.7492739593417231,0.7483059051306873,0.7473378509196515,0.7473378509196515,0.7463697967086157,0.7454017424975798,0.744433688286544,0.7434656340755083,0.7424975798644724,0.7424975798644724,0.7415295256534365,0.7405614714424008,0.739593417231365,0.7386253630203291,0.7376573088092934,0.7366892545982575,0.7357212003872217,0.7357212003872217,0.7347531461761858,0.7347531461761858,0.7337850919651501,0.7328170377541142,0.7318489835430784,0.7308809293320426,0.7299128751210068,0.7289448209099709,0.7279767666989352,0.7270087124878993,0.7260406582768635,0.7250726040658277,0.7241045498547919,0.723136495643756,0.7221684414327202,0.7221684414327202,0.7212003872216844,0.7202323330106486,0.7192642787996127,0.718296224588577,0.7173281703775412,0.7163601161665053,0.7153920619554696,0.7144240077444337,0.7134559535333979,0.712487899322362,0.7115198451113263,0.7115198451113263,0.7105517909002904,0.7095837366892546,0.7086156824782188,0.707647628267183,0.7066795740561471,0.7057115198451114,0.7047434656340755,0.7037754114230397,0.7028073572120038,0.7018393030009681,0.7008712487899322,0.7008712487899322,0.6999031945788964,0.6989351403678606,0.6979670861568248,0.6969990319457889,0.6960309777347532,0.6950629235237173,0.6940948693126815,0.6931268151016456,0.6921587608906099,0.691190706679574,0.6902226524685382,0.6892545982575025,0.6882865440464666,0.6873184898354308,0.686350435624395,0.6853823814133592,0.6844143272023233,0.6834462729912875,0.6824782187802517,0.6815101645692159,0.6815101645692159,0.68054211035818,0.6795740561471443,0.6786060019361084,0.6786060019361084,0.6776379477250726,0.6766698935140368,0.675701839303001,0.6747337850919651,0.6737657308809293,0.6727976766698935,0.6718296224588577,0.6708615682478218,0.6698935140367861,0.6689254598257502,0.6679574056147144,0.6679574056147144,0.6669893514036787,0.6660212971926428,0.665053242981607,0.665053242981607,0.6640851887705711,0.6631171345595354,0.6621490803484995,0.6611810261374637,0.6602129719264279,0.6592449177153921,0.6582768635043562,0.6573088092933205,0.6563407550822846,0.6553727008712488,0.6544046466602129,0.6544046466602129,0.6534365924491772,0.6534365924491772,0.6524685382381413,0.6515004840271055,0.6505324298160697,0.6495643756050339,0.648596321393998,0.6476282671829623,0.6466602129719264,0.6456921587608906,0.6447241045498547,0.6447241045498547,0.643756050338819,0.6427879961277831,0.6418199419167473,0.6408518877057116,0.6398838334946757,0.6389157792836399,0.6379477250726041,0.6369796708615683,0.6360116166505324,0.6350435624394967,0.6350435624394967,0.6340755082284608,0.633107454017425,0.6321393998063891,0.6311713455953534,0.6302032913843175,0.6292352371732817,0.6282671829622459,0.6282671829622459,0.6272991287512101,0.6272991287512101,0.6263310745401742,0.6253630203291385,0.6253630203291385,0.6243949661181026,0.6234269119070668,0.6224588576960309,0.6214908034849952,0.6205227492739593,0.6195546950629235,0.6185866408518877,0.6176185866408519,0.6176185866408519,0.6176185866408519,0.6176185866408519,0.616650532429816,0.6156824782187803,0.6147144240077445,0.6137463697967086,0.6127783155856728,0.611810261374637,0.6108422071636012,0.6098741529525653,0.6089060987415296,0.6079380445304937,0.6069699903194579,0.6060019361084221,0.6050338818973863,0.6040658276863504,0.6030977734753146,0.6021297192642788,0.601161665053243,0.6001936108422071,0.5992255566311714,0.5982575024201355,0.5972894482090997,0.5963213939980639,0.5953533397870281,0.5953533397870281,0.5943852855759922,0.5943852855759922,0.5943852855759922,0.5943852855759922,0.5934172313649564,0.5924491771539206,0.5914811229428848,0.590513068731849,0.5895450145208132,0.5885769603097774,0.5876089060987415,0.5866408518877058,0.5856727976766699,0.5847047434656341,0.5837366892545982,0.5827686350435625,0.5818005808325266,0.5808325266214908,0.579864472410455,0.579864472410455,0.5788964181994192,0.5779283639883833,0.5779283639883833,0.5769603097773476,0.5759922555663117,0.5750242013552759,0.57405614714424,0.5730880929332043,0.5721200387221684,0.5711519845111326,0.5701839303000968,0.569215876089061,0.5682478218780251,0.5672797676669894,0.5663117134559535,0.5653436592449177,0.5643756050338818,0.5634075508228461,0.5624394966118103,0.5614714424007744,0.5605033881897387,0.5595353339787028,0.558567279767667,0.5575992255566312,0.5566311713455954,0.5556631171345595,0.5556631171345595,0.5546950629235237,0.5546950629235237,0.5537270087124879,0.5527589545014521,0.5517909002904162,0.5508228460793805,0.5498547918683446,0.5488867376573088,0.5488867376573088,0.5488867376573088,0.547918683446273,0.5469506292352372,0.5459825750242013,0.5450145208131656,0.5440464666021297,0.5430784123910939,0.542110358180058,0.5411423039690223,0.5401742497579864,0.5392061955469506,0.5382381413359149,0.537270087124879,0.5363020329138432,0.5353339787028074,0.5343659244917716,0.5333978702807357,0.5333978702807357,0.5324298160696999,0.5314617618586641,0.5304937076476283,0.5295256534365924,0.5285575992255567,0.5275895450145208,0.526621490803485,0.5256534365924492,0.5246853823814134,0.5237173281703775,0.5227492739593417,0.5217812197483059,0.5208131655372701,0.5198451113262342,0.5198451113262342,0.5198451113262342,0.5188770571151985,0.5179090029041626,0.5169409486931268,0.515972894482091,0.5150048402710552,0.5150048402710552,0.5140367860600193,0.5130687318489835,0.5121006776379478,0.5111326234269119,0.510164569215876,0.5091965150048403,0.5091965150048403,0.5082284607938045,0.5072604065827686,0.5062923523717329,0.505324298160697,0.5043562439496612,0.5033881897386253,0.5024201355275896,0.5024201355275896,0.5014520813165537,0.5004840271055179,0.4995159728944821,0.4985479186834463,0.4975798644724105,0.49661181026137463,0.49564375605033884,0.494675701839303,0.4937076476282672,0.4927395934172314,0.49177153920619554,0.49080348499515974,0.49080348499515974,0.4898354307841239,0.4898354307841239,0.4888673765730881,0.4878993223620523,0.48693126815101645,0.48596321393998065,0.4849951597289448,0.484027105517909,0.4830590513068732,0.48209099709583736,0.48112294288480156,0.4801548886737657,0.4791868344627299,0.4782187802516941,0.47725072604065827,0.4762826718296225,0.4753146176185866,0.4743465634075508,0.47337850919651503,0.4724104549854792,0.4714424007744434,0.47047434656340753,0.46950629235237173,0.46853823814133594,0.4675701839303001,0.4666021297192643,0.46563407550822844,0.46563407550822844,0.46466602129719264,0.46369796708615685,0.462729912875121,0.4617618586640852,0.46079380445304935,0.45982575024201355,0.45885769603097776,0.4578896418199419,0.4569215876089061,0.45595353339787026,0.45498547918683446,0.45401742497579867,0.4530493707647628,0.452081316553727,0.45111326234269117,0.45014520813165537,0.4491771539206196,0.4491771539206196,0.4482090997095837,0.44724104549854793,0.4462729912875121,0.4453049370764763,0.4443368828654405,0.44336882865440463,0.44240077444336884,0.441432720232333,0.4404646660212972,0.4394966118102614,0.43852855759922554,0.43756050338818975,0.4365924491771539,0.4356243949661181,0.4346563407550823,0.43368828654404645,0.43272023233301066,0.4317521781219748,0.430784123910939,0.4298160696999032,0.42884801548886736,0.42787996127783157,0.4269119070667957,0.4259438528557599,0.4249757986447241,0.42400774443368827,0.4230396902226525,0.4230396902226525,0.4220716360116166,0.42110358180058083,0.42013552758954503,0.4191674733785092,0.4181994191674734,0.41723136495643753,0.41626331074540174,0.41529525653436594,0.4143272023233301,0.4133591481122943,0.41239109390125844,0.41142303969022265,0.41045498547918685,0.41045498547918685,0.409486931268151,0.4085188770571152,0.4075508228460794,0.40658276863504356,0.40561471442400776,0.4046466602129719,0.4036786060019361,0.4027105517909003,0.40174249757986447,0.40077444336882867,0.3998063891577928,0.398838334946757,0.3978702807357212,0.3969022265246854,0.3959341723136496,0.39496611810261373,0.39399806389157793,0.39303000968054214,0.3920619554695063,0.3910939012584705,0.39012584704743464,0.38915779283639884,0.38818973862536305,0.3872216844143272,0.3862536302032914,0.38528557599225555,0.38431752178121975,0.38334946757018395,0.3823814133591481,0.3814133591481123,0.38044530493707646,0.37947725072604066,0.37850919651500486,0.377541142303969,0.3765730880929332,0.37560503388189737,0.37463697967086157,0.3736689254598258,0.3727008712487899,0.3717328170377541,0.3707647628267183,0.3697967086156825,0.3688286544046467,0.36786060019361083,0.36786060019361083,0.36689254598257504,0.3659244917715392,0.3649564375605034,0.3639883833494676,0.3639883833494676,0.36302032913843174,0.36205227492739595,0.3610842207163601,0.3601161665053243,0.3591481122942885,0.3591481122942885,0.35818005808325265,0.35721200387221685,0.356243949661181,0.356243949661181,0.3552758954501452,0.3543078412391094,0.35333978702807356,0.35237173281703776,0.3514036786060019,0.3504356243949661,0.3494675701839303,0.34849951597289447,0.3475314617618587,0.3465634075508228,0.345595353339787,0.34462729912875123,0.3436592449177154,0.3426911907066796,0.34172313649564373,0.34075508228460794,0.33978702807357214,0.3388189738625363,0.3378509196515005,0.33688286544046464,0.33591481122942884,0.33494675701839305,0.3339787028073572,0.3330106485963214,0.33204259438528555,0.33107454017424975,0.33010648596321396,0.3291384317521781,0.3281703775411423,0.32720232333010646,0.32623426911907066,0.32526621490803487,0.324298160696999,0.3233301064859632,0.32236205227492737,0.3213939980638916,0.3204259438528558,0.3194578896418199,0.31848983543078413,0.31752178121974833,0.3165537270087125,0.3155856727976767,0.31461761858664083,0.31364956437560504,0.31268151016456924,0.3117134559535334,0.3107454017424976,0.30977734753146174,0.30880929332042595,0.30784123910939015,0.3068731848983543,0.3059051306873185,0.30493707647628265,0.30396902226524686,0.30300096805421106,0.3020329138431752,0.3010648596321394,0.30009680542110356,0.29912875121006777,0.29816069699903197,0.2971926427879961,0.2962245885769603,0.2952565343659245,0.2942884801548887,0.2933204259438529,0.2933204259438529,0.29235237173281703,0.29138431752178123,0.2904162633107454,0.2894482090997096,0.2884801548886738,0.28751210067763794,0.28654404646660214,0.2855759922555663,0.2846079380445305,0.2836398838334947,0.28267182962245885,0.28170377541142305,0.2807357212003872,0.2797676669893514,0.2787996127783156,0.27783155856727976,0.27686350435624396,0.2758954501452081,0.2749273959341723,0.2739593417231365,0.27299128751210067,0.27202323330106487,0.271055179090029,0.2700871248789932,0.2691190706679574,0.2681510164569216,0.2671829622458858,0.26621490803484993,0.26524685382381413,0.26524685382381413,0.26427879961277834,0.2633107454017425,0.2623426911907067,0.26137463697967084,0.26040658276863504,0.25943852855759925,0.2584704743465634,0.2575024201355276,0.25653436592449175,0.25556631171345595,0.25459825750242016,0.2536302032913843,0.2526621490803485,0.25169409486931266,0.25072604065827686,0.24975798644724104,0.24878993223620524,0.24782187802516942,0.2468538238141336,0.24588576960309777,0.24491771539206195,0.24394966118102615,0.24298160696999033,0.2420135527589545,0.24104549854791868,0.24007744433688286,0.23910939012584706,0.23910939012584706,0.23814133591481124,0.2371732817037754,0.2362052274927396,0.23523717328170377,0.23426911907066797,0.23330106485963215,0.23233301064859632,0.2313649564375605,0.23039690222652467,0.22942884801548888,0.22846079380445306,0.22749273959341723,0.2265246853823814,0.22555663117134558,0.2245885769603098,0.22362052274927396,0.22265246853823814,0.22168441432720232,0.2207163601161665,0.2197483059051307,0.21878025169409487,0.21781219748305905,0.21684414327202323,0.2158760890609874,0.2149080348499516,0.21393998063891578,0.21297192642787996,0.21200387221684414,0.2110358180058083,0.21006776379477252,0.2090997095837367,0.20813165537270087,0.20716360116166505,0.20619554695062922,0.20522749273959343,0.2042594385285576,0.20329138431752178,0.20232333010648595,0.20135527589545016,0.20038722168441434,0.1994191674733785,0.1984511132623427,0.19748305905130686,0.19651500484027107,0.19554695062923524,0.19457889641819942,0.1936108422071636,0.19264278799612777,0.19167473378509198,0.19070667957405615,0.18973862536302033,0.1887705711519845,0.18780251694094868,0.1868344627299129,0.18586640851887706,0.18489835430784124,0.18489835430784124,0.18393030009680542,0.1829622458857696,0.1819941916747338,0.18102613746369797,0.18005808325266215,0.17909002904162633,0.1781219748305905,0.1771539206195547,0.17618586640851888,0.17521781219748306,0.17424975798644723,0.1732817037754114,0.17231364956437561,0.1713455953533398,0.1713455953533398,0.17037754114230397,0.16940948693126814,0.16844143272023232,0.16747337850919652,0.1665053242981607,0.16553727008712488,0.16456921587608905,0.16360116166505323,0.16263310745401743,0.1616650532429816,0.1606969990319458,0.15972894482090996,0.15876089060987417,0.15779283639883834,0.15682478218780252,0.1558567279767667,0.15488867376573087,0.15392061955469508,0.15295256534365925,0.15198451113262343,0.1510164569215876,0.15004840271055178,0.14908034849951599,0.14811229428848016,0.14714424007744434,0.14617618586640851,0.1452081316553727,0.1442400774443369,0.14327202323330107,0.14230396902226525,0.14133591481122942,0.1403678606001936,0.1403678606001936,0.1393998063891578,0.13843175217812198,0.13746369796708616,0.13649564375605033,0.1355275895450145,0.1345595353339787,0.1345595353339787,0.1335914811229429,0.13262342691190707,0.13165537270087124,0.13068731848983542,0.12971926427879962,0.1287512100677638,0.12778315585672798,0.12681510164569215,0.12584704743465633,0.12487899322362052,0.12391093901258471,0.12294288480154889,0.12197483059051308,0.12100677637947725,0.12003872216844143,0.1181026137463698,0.11713455953533398,0.11616650532429816,0.11519845111326234,0.11423039690222653,0.1132623426911907,0.1122942884801549,0.11132623426911907,0.11035818005808325,0.10939012584704744,0.10842207163601161,0.1074540174249758,0.10648596321393998,0.10551790900290416,0.10454985479186835,0.10358180058083252,0.10261374636979671,0.10164569215876089,0.10067763794772508,0.09970958373668926,0.09874152952565343,0.09777347531461762,0.0968054211035818,0.09583736689254599,0.09486931268151017,0.09390125847047434,0.09293320425943853,0.09196515004840271,0.0909970958373669,0.09002904162633107,0.08906098741529525,0.08809293320425944,0.08712487899322362,0.08615682478218781,0.08518877057115198,0.08422071636011616,0.08325266214908035,0.08228460793804453,0.08131655372700872,0.0803484995159729,0.07938044530493708,0.07841239109390126,0.07744433688286544,0.07647628267182963,0.0755082284607938,0.07454017424975799,0.07357212003872217,0.07260406582768635,0.07163601161665054,0.07066795740561471,0.0696999031945789,0.06873184898354308,0.06776379477250725,0.06679574056147145,0.06582768635043562,0.06485963213939981,0.06389157792836399,0.06292352371732816,0.061955469506292354,0.06098741529525654,0.060019361084220714,0.0590513068731849,0.05808325266214908,0.057115198451113264,0.05614714424007745,0.05517909002904162,0.05421103581800581,0.05324298160696999,0.05227492739593417,0.051306873184898356,0.05033881897386254,0.049370764762826716,0.0484027105517909,0.04743465634075508,0.046466602129719266,0.04549854791868345,0.044530493707647625,0.04356243949661181,0.04259438528557599,0.041626331074540175,0.04065827686350436,0.03969022265246854,0.03872216844143272,0.0377541142303969,0.03388189738625363,0.03291384317521781,0.031945788964181994,0.030977734753146177,0.02904162633107454,0.028073572120038724,0.027105517909002903,0.026137463697967087,0.02420135527589545,0.023233301064859633,0.022265246853823813,0.021297192642787996,0.01936108422071636,0.016456921587608905,0.015488867376573089,0.01452081316553727,0.013552758954501452,0.011616650532429816,0.00968054211035818,0.0],\"xaxis\":\"x\",\"y\":[0.7341862117981521,0.7347083926031295,0.7352313167259786,0.7357549857549858,0.7362794012829651,0.7368045649072753,0.7373304782298359,0.7378571428571429,0.7383845604002859,0.7389127324749643,0.7394416607015032,0.7399713467048711,0.7405017921146954,0.7410329985652798,0.741564967695621,0.7420977011494253,0.7426312005751258,0.7431654676258993,0.7437005039596832,0.7442363112391931,0.7447728911319395,0.7453102453102453,0.7458483754512636,0.7463872832369942,0.7469269703543022,0.7474674384949349,0.7480086893555394,0.7485507246376811,0.7490935460478607,0.7496371552975326,0.7501815541031227,0.7507267441860465,0.7512727272727273,0.7518195050946143,0.7516387472687546,0.7521865889212828,0.7527352297592997,0.7532846715328467,0.7538349159970782,0.7543859649122807,0.7549378200438918,0.7554904831625183,0.756043956043956,0.7565982404692082,0.7571533382245048,0.7577092511013216,0.7582659808963997,0.7580882352941176,0.7586460632818248,0.7592047128129602,0.7597641857037583,0.7603244837758112,0.7608856088560886,0.7614475627769571,0.762010347376201,0.7625739644970414,0.7631384159881569,0.762962962962963,0.7627872498146775,0.7633531157270029,0.7639198218262806,0.7644873699851411,0.7650557620817844,0.765625,0.7661950856291884,0.7660208643815202,0.7665920954511558,0.7671641791044777,0.7677371172516804,0.7683109118086696,0.768885564697083,0.7694610778443114,0.7700374531835206,0.7706146926536732,0.7711927981995499,0.7717717717717718,0.772351615326822,0.7729323308270677,0.7735139202407826,0.7740963855421686,0.774679728711379,0.7745098039215687,0.7750943396226415,0.775679758308157,0.7762660619803476,0.7768532526475038,0.7766843300529902,0.7772727272727272,0.7778620166793025,0.7784522003034902,0.7790432801822323,0.7796352583586627,0.779467680608365,0.7792998477929984,0.7798933739527799,0.7804878048780488,0.7810831426392068,0.7816793893129771,0.7822765469824293,0.7828746177370031,0.7834736036725325,0.7833078101071975,0.7839080459770115,0.7845092024539877,0.7851112816577129,0.7857142857142857,0.7863182167563413,0.786923076923077,0.7875288683602771,0.788135593220339,0.7879722436391673,0.7885802469135802,0.7891891891891892,0.7890262751159196,0.7896365042536736,0.7902476780185759,0.790859798605732,0.7914728682170543,0.7920868890612878,0.7919254658385093,0.7925407925407926,0.7931570762052877,0.7937743190661478,0.794392523364486,0.7950116913484022,0.7956318252730109,0.7962529274004684,0.796875,0.7967161845191556,0.797339593114241,0.79796397807361,0.79858934169279,0.7992156862745098,0.7998430141287284,0.800471327572663,0.8011006289308176,0.8017309205350118,0.8023622047244094,0.8029944838455477,0.8028391167192429,0.8034727703235991,0.8033175355450237,0.8039525691699605,0.8045886075949367,0.8044338875692795,0.8050713153724247,0.8057097541633624,0.8063492063492064,0.806989674344718,0.8076311605723371,0.807478122513922,0.8081210191082803,0.8087649402390438,0.8094098883572568,0.8100558659217877,0.8099041533546326,0.8105515587529976,0.8112,0.8118494795836669,0.8125,0.8131515637530072,0.8130016051364366,0.8136546184738955,0.8143086816720257,0.8149637972646823,0.8148148148148148,0.814665592264303,0.8153225806451613,0.8159806295399515,0.815831987075929,0.8164915117219078,0.8171521035598706,0.8170040485829959,0.8176661264181524,0.8183292781832928,0.8189935064935064,0.8196588139723802,0.8203252032520325,0.8201790073230268,0.8208469055374593,0.8207008964955175,0.8213703099510603,0.8220408163265306,0.8227124183006536,0.8233851185609158,0.823240589198036,0.823914823914824,0.8245901639344262,0.8252666119770303,0.8251231527093597,0.8258011503697618,0.8264802631578947,0.8263374485596707,0.8270181219110379,0.8268755152514426,0.8267326732673267,0.8274153592072667,0.828099173553719,0.8287841191066998,0.8294701986754967,0.8301574150787076,0.8308457711442786,0.8307053941908714,0.8313953488372093,0.8320864505403158,0.8327787021630616,0.832639467110741,0.8333333333333334,0.8331943286071727,0.8330550918196995,0.83375104427736,0.8336120401337793,0.8334728033472804,0.8333333333333334,0.8340318524727578,0.834731543624161,0.834592779177162,0.8352941176470589,0.8359966358284272,0.8358585858585859,0.8365627632687447,0.836424957841484,0.8371308016877637,0.8378378378378378,0.8377007607776839,0.8375634517766497,0.8374259102455546,0.8372881355932204,0.8379983036471587,0.8387096774193549,0.8394222599830077,0.8401360544217688,0.84,0.8407155025553663,0.8414322250639387,0.841296928327645,0.8420153714773698,0.8418803418803419,0.8417450812660393,0.8424657534246576,0.8431876606683805,0.8439108061749572,0.8446351931330472,0.845360824742268,0.8460877042132416,0.846815834767642,0.8475452196382429,0.8474137931034482,0.8481449525452976,0.8488773747841105,0.8487467588591184,0.8494809688581315,0.8502164502164502,0.8509532062391681,0.8516912402428447,0.8524305555555556,0.8531711555169418,0.8530434782608696,0.8537859007832899,0.8536585365853658,0.8544027898866609,0.8551483420593369,0.8558951965065502,0.8566433566433567,0.8573928258967629,0.8572679509632224,0.8571428571428571,0.8578947368421053,0.8577699736611062,0.8576449912126538,0.8583992963940194,0.8591549295774648,0.8590308370044053,0.8597883597883598,0.8596646072374228,0.8604240282685512,0.8603006189213086,0.8610619469026549,0.8618246235606731,0.8617021276595744,0.8615794143744454,0.8614564831261101,0.8622222222222222,0.8620996441281139,0.8619768477292965,0.8627450980392157,0.8626226583407671,0.8633928571428572,0.8641644325290437,0.8649373881932021,0.8657117278424351,0.8655913978494624,0.8654708520179372,0.8662477558348295,0.867026055705301,0.8669064748201439,0.8667866786678667,0.8675675675675676,0.8683498647430117,0.868231046931408,0.8690153568202349,0.8698010849909584,0.8705882352941177,0.8704710144927537,0.8712601994560291,0.8711433756805808,0.8719346049046321,0.8727272727272727,0.8726114649681529,0.8734061930783242,0.8742023701002735,0.8740875912408759,0.8748858447488584,0.8747714808043876,0.8746569075937786,0.8745421245421245,0.8744271310724107,0.8752293577981651,0.8760330578512396,0.8768382352941176,0.8767249310027599,0.8766114180478821,0.8774193548387097,0.8782287822878229,0.8790397045244691,0.8798521256931608,0.879740980573543,0.8805555555555555,0.8804448563484708,0.8803339517625232,0.8802228412256268,0.8801115241635687,0.88,0.8808193668528864,0.880708294501398,0.8805970149253731,0.880485527544351,0.8803738317757009,0.8802619270346118,0.8801498127340824,0.880037488284911,0.8799249530956847,0.8807511737089202,0.8806390977443609,0.8814675446848542,0.8813559322033898,0.88124410933082,0.8820754716981132,0.8819641170915958,0.8818525519848771,0.8826868495742668,0.8835227272727273,0.8834123222748815,0.8842504743833017,0.8841405508072174,0.8840304182509505,0.884871550903901,0.8857142857142857,0.8856053384175405,0.8854961832061069,0.8853868194842407,0.8852772466539197,0.8851674641148325,0.8850574712643678,0.8849472674976031,0.8857965451055663,0.8866474543707973,0.8865384615384615,0.8864292589027911,0.8863198458574181,0.8862102217936355,0.887065637065637,0.8869565217391304,0.8868471953578336,0.8877057115198451,0.8885658914728682,0.8894277400581959,0.8893203883495145,0.8901846452866861,0.8900778210116731,0.8899707887049659,0.8898635477582846,0.8897560975609756,0.890625,0.8914956011730205,0.8923679060665362,0.8922624877571009,0.8921568627450981,0.8920510304219823,0.8929273084479371,0.8928220255653884,0.8937007874015748,0.8935960591133005,0.8944773175542406,0.8943731490621916,0.8942687747035574,0.8951533135509396,0.8950495049504951,0.8949454905847374,0.8948412698412699,0.8947368421052632,0.8946322067594433,0.8945273631840795,0.8954183266932271,0.8953140578265204,0.8952095808383234,0.8961038961038961,0.896,0.8958958958958959,0.8967935871743486,0.8966900702106319,0.8975903614457831,0.8984924623115578,0.8993963782696177,0.9003021148036254,0.9012096774193549,0.9011099899091827,0.901010101010101,0.9009100101112234,0.9008097165991903,0.900709219858156,0.9016227180527383,0.9015228426395939,0.9024390243902439,0.9033570701932858,0.9032586558044806,0.9031600407747197,0.9040816326530612,0.9039836567926456,0.9038854805725971,0.9037871033776868,0.9036885245901639,0.9046153846153846,0.9045174537987679,0.9044193216855088,0.9053497942386831,0.9062821833161689,0.9061855670103093,0.9060887512899897,0.90599173553719,0.905894519131334,0.9057971014492754,0.9067357512953368,0.9066390041493776,0.9065420560747663,0.9064449064449065,0.9073881373569199,0.9072916666666667,0.9071949947862357,0.9081419624217119,0.9080459770114943,0.9079497907949791,0.9078534031413612,0.9077568134171907,0.9076600209863589,0.907563025210084,0.907465825446898,0.9073684210526316,0.9083245521601686,0.9082278481012658,0.9081309398099261,0.9080338266384778,0.908994708994709,0.9088983050847458,0.9098621420996819,0.910828025477707,0.9117959617428267,0.9117021276595745,0.9116080937167199,0.9115138592750534,0.9114194236926361,0.9113247863247863,0.9122994652406418,0.9122055674518201,0.9121114683815649,0.9120171673819742,0.9119226638023631,0.9129032258064517,0.9128094725511302,0.9127155172413793,0.912621359223301,0.9125269978401728,0.9124324324324324,0.9123376623376623,0.9122426868905742,0.9121475054229935,0.9120521172638436,0.9119565217391304,0.911860718171926,0.9117647058823529,0.9116684841875682,0.9126637554585153,0.9136612021857924,0.9135667396061269,0.9134720700985761,0.9133771929824561,0.9143798024149287,0.9142857142857143,0.9141914191419142,0.9140969162995595,0.9151047409040793,0.9161147902869757,0.9160220994475138,0.915929203539823,0.9158361018826136,0.9168514412416852,0.9167591564927858,0.9177777777777778,0.917686318131257,0.9175946547884187,0.9175027870680045,0.9174107142857143,0.9184357541899442,0.9183445190156599,0.9182530795072789,0.9181614349775785,0.9180695847362514,0.9179775280898876,0.9178852643419573,0.9177927927927928,0.9177001127395716,0.917607223476298,0.9175141242937853,0.917420814479638,0.9184597961494904,0.9183673469387755,0.9182746878547106,0.9181818181818182,0.919226393629124,0.9191343963553531,0.9201824401368301,0.9200913242009132,0.92,0.919908466819222,0.9198167239404352,0.9197247706422018,0.9196326061997704,0.9206896551724137,0.9205983889528193,0.9216589861751152,0.922722029988466,0.9226327944572749,0.922543352601156,0.9224537037037037,0.9235225955967555,0.9234338747099768,0.924506387921022,0.9244186046511628,0.9254947613504074,0.9265734265734266,0.926487747957993,0.9264018691588785,0.9263157894736842,0.927400468384075,0.9273153575615475,0.9272300469483568,0.927144535840188,0.9270588235294117,0.928150765606596,0.9292452830188679,0.9291617473435655,0.9302600472813238,0.9301775147928995,0.9312796208530806,0.9311981020166074,0.9323040380047506,0.9322235434007135,0.9321428571428572,0.932061978545888,0.9331742243436754,0.9330943847072879,0.9342105263157895,0.9341317365269461,0.935251798561151,0.9351740696278511,0.9350961538461539,0.9350180505415162,0.9349397590361446,0.9348612786489746,0.9347826086956522,0.9347037484885127,0.9346246973365617,0.9357575757575758,0.9356796116504854,0.9356014580801945,0.9355231143552312,0.9354445797807551,0.9353658536585366,0.9365079365079365,0.9364303178484108,0.9363525091799265,0.9362745098039216,0.9361963190184049,0.9361179361179361,0.9360393603936039,0.9359605911330049,0.93711467324291,0.937037037037037,0.9381953028430161,0.9381188118811881,0.9380421313506815,0.9379652605459057,0.937888198757764,0.9378109452736318,0.937733499377335,0.9376558603491272,0.9375780274656679,0.9375,0.9374217772215269,0.9373433583959899,0.9372647427854455,0.9371859296482412,0.9383647798742139,0.9382871536523929,0.9382093316519546,0.9381313131313131,0.9380530973451328,0.9379746835443038,0.9378960709759189,0.9378172588832487,0.9377382465057179,0.9376590330788804,0.9375796178343949,0.9375,0.9386973180076629,0.9386189258312021,0.93854033290653,0.9384615384615385,0.938382541720154,0.9383033419023136,0.9382239382239382,0.9381443298969072,0.9380645161290323,0.937984496124031,0.9379042690815006,0.9378238341968912,0.9390402075226978,0.938961038961039,0.9388816644993498,0.9388020833333334,0.9387222946544981,0.9386422976501305,0.938562091503268,0.9384816753926701,0.9384010484927916,0.9383202099737533,0.938239159001314,0.9381578947368421,0.9380764163372859,0.9379947229551451,0.9379128137384413,0.9378306878306878,0.937748344370861,0.9376657824933687,0.9375830013280213,0.9375,0.9374167776298269,0.9386666666666666,0.9385847797062751,0.9385026737967914,0.9384203480589023,0.9396782841823056,0.9395973154362416,0.9395161290322581,0.9394347240915208,0.9393530997304582,0.9392712550607287,0.9391891891891891,0.939106901217862,0.9390243902439024,0.9389416553595658,0.938858695652174,0.9387755102040817,0.9400544959128065,0.9399727148703957,0.9398907103825137,0.9398084815321477,0.9410958904109589,0.9410150891632373,0.9409340659340659,0.9408528198074277,0.940771349862259,0.9406896551724138,0.9406077348066298,0.9405255878284924,0.9404432132963989,0.9403606102635229,0.9402777777777778,0.9401947148817803,0.9415041782729805,0.9414225941422594,0.9427374301675978,0.9426573426573427,0.9425770308123249,0.9424964936886395,0.9424157303370787,0.9423347398030942,0.9422535211267605,0.9421720733427362,0.942090395480226,0.942008486562942,0.943342776203966,0.9432624113475178,0.9431818181818182,0.9431009957325747,0.9430199430199431,0.9429386590584878,0.9428571428571428,0.9427753934191703,0.9426934097421203,0.9426111908177905,0.9425287356321839,0.943884892086331,0.9438040345821326,0.9437229437229437,0.9436416184971098,0.9435600578871202,0.9434782608695652,0.9433962264150944,0.9433139534883721,0.9446870451237264,0.9446064139941691,0.945985401459854,0.945906432748538,0.9458272327964861,0.9472140762463344,0.947136563876652,0.9470588235294117,0.946980854197349,0.9469026548672567,0.946824224519941,0.9467455621301775,0.9466666666666667,0.9465875370919882,0.9479940564635958,0.9494047619047619,0.9508196721311475,0.9507462686567164,0.9506726457399103,0.9505988023952096,0.9505247376311844,0.9504504504504504,0.9503759398496241,0.9503012048192772,0.9502262443438914,0.9501510574018127,0.9500756429652042,0.95,0.9499241274658573,0.9498480243161094,0.9497716894977168,0.9496951219512195,0.9496183206106871,0.9495412844036697,0.9494640122511485,0.9493865030674846,0.9493087557603687,0.9492307692307692,0.9491525423728814,0.9490740740740741,0.9505409582689336,0.9504643962848297,0.951937984496124,0.953416149068323,0.9548989113530326,0.9548286604361371,0.9547581903276131,0.9546875,0.9546165884194053,0.9545454545454546,0.9544740973312402,0.9544025157232704,0.9543307086614173,0.9542586750788643,0.9541864139020537,0.9541139240506329,0.9540412044374009,0.953968253968254,0.9538950715421304,0.9538216560509554,0.9553429027113237,0.9552715654952076,0.9552,0.9567307692307693,0.956661316211878,0.9565916398713826,0.9565217391304348,0.9564516129032258,0.9563812600969306,0.9563106796116505,0.9562398703403565,0.9561688311688312,0.9560975609756097,0.9560260586319218,0.9559543230016313,0.9558823529411765,0.955810147299509,0.9557377049180328,0.9556650246305419,0.9555921052631579,0.9555189456342669,0.9554455445544554,0.9553719008264463,0.9552980132450332,0.9552238805970149,0.9551495016611296,0.9550748752079867,0.9566666666666667,0.9565943238731218,0.9581939799331104,0.9581239530988275,0.9580536912751678,0.957983193277311,0.9579124579124579,0.9578414839797639,0.9577702702702703,0.9593908629441624,0.9610169491525423,0.9609507640067911,0.9608843537414966,0.9608177172061328,0.9607508532423208,0.9606837606837607,0.9606164383561644,0.9605488850771869,0.9604810996563574,0.9604130808950087,0.9603448275862069,0.9602763385146805,0.9602076124567474,0.9601386481802426,0.9600694444444444,0.96,0.9599303135888502,0.9616055846422339,0.9615384615384616,0.9614711033274956,0.9614035087719298,0.961335676625659,0.9612676056338029,0.9611992945326279,0.9611307420494699,0.9610619469026549,0.9609929078014184,0.9609236234458259,0.9608540925266904,0.9607843137254902,0.9607142857142857,0.960644007155635,0.9623655913978495,0.9640933572710951,0.9640287769784173,0.963963963963964,0.9638989169675091,0.9638336347197106,0.9637681159420289,0.9655172413793104,0.9654545454545455,0.9653916211293261,0.9653284671532847,0.9652650822669104,0.9652014652014652,0.9651376146788991,0.9669117647058824,0.9668508287292817,0.966789667896679,0.966728280961183,0.9666666666666667,0.9666048237476809,0.966542750929368,0.9664804469273743,0.9682835820895522,0.9682242990654205,0.9681647940074907,0.9681050656660413,0.9680451127819549,0.967984934086629,0.9679245283018868,0.9678638941398866,0.9678030303030303,0.967741935483871,0.967680608365019,0.9676190476190476,0.9675572519083969,0.9694072657743786,0.9693486590038314,0.9712092130518234,0.9711538461538461,0.9710982658959537,0.971042471042471,0.9709864603481625,0.9709302325581395,0.970873786407767,0.9708171206225681,0.9707602339181286,0.970703125,0.9706457925636007,0.9705882352941176,0.9705304518664047,0.9704724409448819,0.9704142011834319,0.9703557312252964,0.9702970297029703,0.9702380952380952,0.9701789264413518,0.9701195219123506,0.9700598802395209,0.97,0.969939879759519,0.9698795180722891,0.9698189134808853,0.969758064516129,0.9717171717171718,0.97165991902834,0.9716024340770791,0.9715447154471545,0.9714867617107943,0.9714285714285714,0.9713701431492843,0.9713114754098361,0.971252566735113,0.9711934156378601,0.9711340206185567,0.9710743801652892,0.9710144927536232,0.970954356846473,0.9708939708939709,0.9708333333333333,0.9707724425887265,0.9707112970711297,0.9727463312368972,0.9726890756302521,0.9726315789473684,0.9725738396624473,0.9725158562367865,0.972457627118644,0.9723991507430998,0.9723404255319149,0.9722814498933902,0.9722222222222222,0.9721627408993576,0.9721030042918455,0.9720430107526882,0.9719827586206896,0.9719222462203023,0.9718614718614719,0.9718004338394793,0.9717391304347827,0.971677559912854,0.9716157205240175,0.9715536105032823,0.9714912280701754,0.9714285714285714,0.9713656387665198,0.9713024282560706,0.9712389380530974,0.9711751662971175,0.9711111111111111,0.9732739420935412,0.9732142857142857,0.9731543624161074,0.9730941704035875,0.9730337078651685,0.972972972972973,0.9729119638826185,0.9728506787330317,0.9727891156462585,0.9727272727272728,0.9726651480637813,0.9726027397260274,0.9725400457665904,0.9724770642201835,0.9747126436781609,0.9746543778801844,0.9745958429561201,0.9745370370370371,0.974477958236659,0.9744186046511628,0.9743589743589743,0.9742990654205608,0.9742388758782201,0.9741784037558685,0.9741176470588235,0.9740566037735849,0.9739952718676123,0.9739336492890995,0.9738717339667459,0.9738095238095238,0.9737470167064439,0.9736842105263158,0.973621103117506,0.9735576923076923,0.9734939759036144,0.9734299516908212,0.9733656174334141,0.9733009708737864,0.9732360097323601,0.973170731707317,0.9731051344743277,0.9730392156862745,0.972972972972973,0.9729064039408867,0.9728395061728395,0.9727722772277227,0.9727047146401985,0.972636815920398,0.972568578553616,0.9725,0.9724310776942355,0.9723618090452262,0.9722921914357683,0.9722222222222222,0.9721518987341772,0.9720812182741116,0.9720101781170484,0.9719387755102041,0.9718670076726342,0.9743589743589743,0.974293059125964,0.9742268041237113,0.9741602067183462,0.9740932642487047,0.9766233766233766,0.9765625,0.9765013054830287,0.9764397905759162,0.9763779527559056,0.9763157894736842,0.978891820580475,0.9788359788359788,0.9787798408488063,0.9787234042553191,0.9813333333333333,0.9812834224598931,0.9812332439678284,0.9811827956989247,0.9811320754716981,0.981081081081081,0.981029810298103,0.9809782608695652,0.9809264305177112,0.9808743169398907,0.9808219178082191,0.9807692307692307,0.9807162534435262,0.9806629834254144,0.9806094182825484,0.9805555555555555,0.9805013927576601,0.9804469273743017,0.9803921568627451,0.9803370786516854,0.9802816901408451,0.980225988700565,0.9801699716713881,0.9801136363636364,0.98005698005698,0.98,0.9799426934097422,0.9798850574712644,0.9798270893371758,0.9797687861271677,0.9797101449275363,0.9796511627906976,0.9795918367346939,0.97953216374269,0.9794721407624634,0.9794117647058823,0.9793510324483776,0.9792899408284024,0.9792284866468842,0.9791666666666666,0.9791044776119403,0.9790419161676647,0.978978978978979,0.9789156626506024,0.9788519637462235,0.9787878787878788,0.9787234042553191,0.9786585365853658,0.9785932721712538,0.9785276073619632,0.9784615384615385,0.9783950617283951,0.978328173374613,0.9782608695652174,0.9781931464174455,0.978125,0.9780564263322884,0.9779874213836478,0.9779179810725552,0.9778481012658228,0.9777777777777777,0.9777070063694268,0.9776357827476039,0.9775641025641025,0.977491961414791,0.9774193548387097,0.9805825242718447,0.9805194805194806,0.9804560260586319,0.9803921568627451,0.980327868852459,0.9802631578947368,0.9801980198019802,0.9801324503311258,0.9800664451827242,0.98,0.979933110367893,0.9798657718120806,0.9797979797979798,0.9797297297297297,0.9796610169491525,0.9795918367346939,0.9795221843003413,0.9794520547945206,0.979381443298969,0.9793103448275862,0.9792387543252595,0.9791666666666666,0.9790940766550522,0.9790209790209791,0.9789473684210527,0.9788732394366197,0.9787985865724381,0.9787234042553191,0.9786476868327402,0.9785714285714285,0.982078853046595,0.9820143884892086,0.9819494584837545,0.9818840579710145,0.9818181818181818,0.9817518248175182,0.9816849816849816,0.9816176470588235,0.981549815498155,0.9814814814814815,0.9814126394052045,0.9813432835820896,0.9812734082397003,0.981203007518797,0.9811320754716981,0.9810606060606061,0.9809885931558935,0.9809160305343512,0.9808429118773946,0.9807692307692307,0.9806949806949807,0.9806201550387597,0.980544747081712,0.98046875,0.9803921568627451,0.9803149606299213,0.9802371541501976,0.9801587301587301,0.9840637450199203,0.984,0.9839357429718876,0.9838709677419355,0.9838056680161943,0.983739837398374,0.9836734693877551,0.9836065573770492,0.9835390946502057,0.9834710743801653,0.983402489626556,0.9833333333333333,0.9832635983263598,0.9831932773109243,0.9831223628691983,0.9830508474576272,0.9829787234042553,0.9829059829059829,0.9828326180257511,0.9827586206896551,0.9826839826839827,0.9826086956521739,0.982532751091703,0.9824561403508771,0.9823788546255506,0.9823008849557522,0.9822222222222222,0.9821428571428571,0.9820627802690582,0.9819819819819819,0.9819004524886877,0.9818181818181818,0.9817351598173516,0.981651376146789,0.9815668202764977,0.9814814814814815,0.9813953488372092,0.9813084112149533,0.9812206572769953,0.9811320754716981,0.981042654028436,0.9809523809523809,0.9808612440191388,0.9807692307692307,0.9806763285024155,0.9805825242718447,0.9804878048780488,0.9803921568627451,0.9802955665024631,0.9801980198019802,0.9800995024875622,0.98,0.9798994974874372,0.9797979797979798,0.9796954314720813,0.9795918367346939,0.9794871794871794,0.9845360824742269,0.9844559585492227,0.984375,0.9842931937172775,0.9842105263157894,0.9841269841269841,0.9840425531914894,0.983957219251337,0.9838709677419355,0.9837837837837838,0.9836956521739131,0.9836065573770492,0.9835164835164835,0.9834254143646409,0.9833333333333333,0.9888268156424581,0.9887640449438202,0.9887005649717514,0.9886363636363636,0.9885714285714285,0.9885057471264368,0.9884393063583815,0.9883720930232558,0.9883040935672515,0.9882352941176471,0.9881656804733728,0.9880952380952381,0.9880239520958084,0.9879518072289156,0.9878787878787879,0.9878048780487805,0.9877300613496932,0.9876543209876543,0.9875776397515528,0.9875,0.9874213836477987,0.9873417721518988,0.9872611464968153,0.9871794871794872,0.9870967741935484,0.987012987012987,0.9869281045751634,0.9868421052631579,0.9867549668874173,0.9866666666666667,0.9865771812080537,0.9864864864864865,0.9863945578231292,0.9931506849315068,0.993103448275862,0.9930555555555556,0.993006993006993,0.9929577464788732,0.9929078014184397,0.9928571428571429,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0],\"yaxis\":\"y\",\"type\":\"scatter\"}],                        {\"template\":{\"data\":{\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"white\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"white\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"#C8D4E3\",\"linecolor\":\"#C8D4E3\",\"minorgridcolor\":\"#C8D4E3\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"#C8D4E3\",\"linecolor\":\"#C8D4E3\",\"minorgridcolor\":\"#C8D4E3\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"choropleth\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"choropleth\"}],\"contourcarpet\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"contourcarpet\"}],\"contour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"contour\"}],\"heatmapgl\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmapgl\"}],\"heatmap\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmap\"}],\"histogram2dcontour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2dcontour\"}],\"histogram2d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2d\"}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"mesh3d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"mesh3d\"}],\"parcoords\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"parcoords\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}],\"scatter3d\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatter3d\"}],\"scattercarpet\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattercarpet\"}],\"scattergeo\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergeo\"}],\"scattergl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergl\"}],\"scattermapbox\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattermapbox\"}],\"scatterpolargl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolargl\"}],\"scatterpolar\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolar\"}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"scatterternary\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterternary\"}],\"surface\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"surface\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}]},\"layout\":{\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"autotypenumbers\":\"strict\",\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]],\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]},\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"geo\":{\"bgcolor\":\"white\",\"lakecolor\":\"white\",\"landcolor\":\"white\",\"showlakes\":true,\"showland\":true,\"subunitcolor\":\"#C8D4E3\"},\"hoverlabel\":{\"align\":\"left\"},\"hovermode\":\"closest\",\"mapbox\":{\"style\":\"light\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"white\",\"polar\":{\"angularaxis\":{\"gridcolor\":\"#EBF0F8\",\"linecolor\":\"#EBF0F8\",\"ticks\":\"\"},\"bgcolor\":\"white\",\"radialaxis\":{\"gridcolor\":\"#EBF0F8\",\"linecolor\":\"#EBF0F8\",\"ticks\":\"\"}},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"white\",\"gridcolor\":\"#DFE8F3\",\"gridwidth\":2,\"linecolor\":\"#EBF0F8\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"#EBF0F8\"},\"yaxis\":{\"backgroundcolor\":\"white\",\"gridcolor\":\"#DFE8F3\",\"gridwidth\":2,\"linecolor\":\"#EBF0F8\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"#EBF0F8\"},\"zaxis\":{\"backgroundcolor\":\"white\",\"gridcolor\":\"#DFE8F3\",\"gridwidth\":2,\"linecolor\":\"#EBF0F8\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"#EBF0F8\"}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"ternary\":{\"aaxis\":{\"gridcolor\":\"#DFE8F3\",\"linecolor\":\"#A2B1C6\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"#DFE8F3\",\"linecolor\":\"#A2B1C6\",\"ticks\":\"\"},\"bgcolor\":\"white\",\"caxis\":{\"gridcolor\":\"#DFE8F3\",\"linecolor\":\"#A2B1C6\",\"ticks\":\"\"}},\"title\":{\"x\":0.05},\"xaxis\":{\"automargin\":true,\"gridcolor\":\"#EBF0F8\",\"linecolor\":\"#EBF0F8\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"#EBF0F8\",\"zerolinewidth\":2},\"yaxis\":{\"automargin\":true,\"gridcolor\":\"#EBF0F8\",\"linecolor\":\"#EBF0F8\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"#EBF0F8\",\"zerolinewidth\":2}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"Recall\"},\"constrain\":\"domain\"},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"Precision\"},\"scaleanchor\":\"x\",\"scaleratio\":1},\"legend\":{\"tracegroupgap\":0},\"title\":{\"text\":\"Precision-Recall Curve (AUC=0.8840)\"},\"height\":600,\"width\":800,\"shapes\":[{\"line\":{\"dash\":\"dash\"},\"type\":\"line\",\"x0\":0,\"x1\":1,\"y0\":1,\"y1\":0}],\"showlegend\":false},                        {\"responsive\": true}                    ).then(function(){\n","                            \n","var gd = document.getElementById('4a823447-63c4-43bf-9439-2f88513fa38e');\n","var x = new MutationObserver(function (mutations, observer) {{\n","        var display = window.getComputedStyle(gd).display;\n","        if (!display || display === 'none') {{\n","            console.log([gd, 'removed!']);\n","            Plotly.purge(gd);\n","            observer.disconnect();\n","        }}\n","}});\n","\n","// Listen for the removal of the full notebook cells\n","var notebookContainer = gd.closest('#notebook-container');\n","if (notebookContainer) {{\n","    x.observe(notebookContainer, {childList: true});\n","}}\n","\n","// Listen for the clearing of the current output cell\n","var outputEl = gd.closest('.output');\n","if (outputEl) {{\n","    x.observe(outputEl, {childList: true});\n","}}\n","\n","                        })                };                });            </script>        </div>"]},"metadata":{},"output_type":"display_data"}],"source":["target_score = extra_trees_clf.predict_proba(features_valid)[:, 1]\n","\n","fpr, tpr, thresholds = roc_curve(target_valid, target_score)\n","\n","fig = px.area(\n","    x=fpr, y=tpr,\n","    title=f'ROC Curve (AUC={auc(fpr, tpr):.4f})',\n","    labels=dict(x='False Positive Rate', y='True Positive Rate'),\n","    width=800, height=600\n",")\n","fig.add_shape(\n","    type='line', line=dict(dash='dash'),\n","    x0=0, x1=1, y0=0, y1=1\n",")\n","\n","fig.update_yaxes(scaleanchor=\"x\", scaleratio=1)\n","fig.update_xaxes(constrain='domain')\n","fig.update_layout(showlegend=False)\n","fig.show()\n","\n","precision, recall, thresholds = precision_recall_curve(target_valid, target_score)\n","\n","fig = px.area(\n","    x=recall, y=precision,\n","    title=f'Precision-Recall Curve (AUC={auc(fpr, tpr):.4f})',\n","    labels=dict(x='Recall', y='Precision'),\n","    width=800, height=600\n",")\n","fig.add_shape(\n","    type='line', line=dict(dash='dash'),\n","    x0=0, x1=1, y0=1, y1=0\n",")\n","fig.update_yaxes(scaleanchor=\"x\", scaleratio=1)\n","fig.update_xaxes(constrain='domain')\n","fig.update_layout(showlegend=False)\n","fig.show()"]},{"cell_type":"markdown","metadata":{},"source":["--------"]},{"cell_type":"markdown","metadata":{},"source":["# Logistic Regression"]},{"cell_type":"code","execution_count":146,"metadata":{"execution":{"iopub.execute_input":"2023-11-30T16:53:23.562118Z","iopub.status.busy":"2023-11-30T16:53:23.561856Z","iopub.status.idle":"2023-11-30T16:53:41.358165Z","shell.execute_reply":"2023-11-30T16:53:41.356738Z","shell.execute_reply.started":"2023-11-30T16:53:23.562094Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/homebrew/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:1223: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 17.\n","  warnings.warn(\n","/opt/homebrew/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:1223: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 17.\n","  warnings.warn(\n","/opt/homebrew/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:1223: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 17.\n","  warnings.warn(\n","/opt/homebrew/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:1223: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 17.\n","  warnings.warn(\n","/opt/homebrew/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:1223: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 17.\n","  warnings.warn(\n","/opt/homebrew/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:1223: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 17.\n","  warnings.warn(\n","/opt/homebrew/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:1223: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 17.\n","  warnings.warn(\n","/opt/homebrew/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:1223: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 17.\n","  warnings.warn(\n","/opt/homebrew/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:1223: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 17.\n","  warnings.warn(\n","/opt/homebrew/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:1223: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 17.\n","  warnings.warn(\n","/opt/homebrew/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:1223: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 17.\n","  warnings.warn(\n","/opt/homebrew/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:1223: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 17.\n","  warnings.warn(\n","/opt/homebrew/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:1223: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 17.\n","  warnings.warn(\n","/opt/homebrew/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:1223: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 17.\n","  warnings.warn(\n","/opt/homebrew/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:1223: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 17.\n","  warnings.warn(\n","/opt/homebrew/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:1223: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 17.\n","  warnings.warn(\n","/opt/homebrew/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:1223: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 17.\n","  warnings.warn(\n","/opt/homebrew/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:1223: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 17.\n","  warnings.warn(\n","/opt/homebrew/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:1223: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 17.\n","  warnings.warn(\n","/opt/homebrew/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:1223: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 17.\n","  warnings.warn(\n","/opt/homebrew/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:1223: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 17.\n","  warnings.warn(\n","/opt/homebrew/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:1223: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 17.\n","  warnings.warn(\n","/opt/homebrew/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:1223: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 17.\n","  warnings.warn(\n","/opt/homebrew/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:1223: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 17.\n","  warnings.warn(\n","/opt/homebrew/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:1223: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 17.\n","  warnings.warn(\n","/opt/homebrew/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:1223: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 17.\n","  warnings.warn(\n","/opt/homebrew/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:1223: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 17.\n","  warnings.warn(\n","/opt/homebrew/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:1223: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 17.\n","  warnings.warn(\n","/opt/homebrew/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:1223: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 17.\n","  warnings.warn(\n","/opt/homebrew/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:1223: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 17.\n","  warnings.warn(\n","/opt/homebrew/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:1223: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 17.\n","  warnings.warn(\n","/opt/homebrew/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:1223: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 17.\n","  warnings.warn(\n","/opt/homebrew/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:1223: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 17.\n","  warnings.warn(\n","/opt/homebrew/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:1223: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 17.\n","  warnings.warn(\n","/opt/homebrew/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:1223: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 17.\n","  warnings.warn(\n","/opt/homebrew/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:1223: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 17.\n","  warnings.warn(\n","/opt/homebrew/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:1223: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 17.\n","  warnings.warn(\n","/opt/homebrew/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:1223: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 17.\n","  warnings.warn(\n","/opt/homebrew/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:1223: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 17.\n","  warnings.warn(\n","/opt/homebrew/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:1223: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 17.\n","  warnings.warn(\n","/opt/homebrew/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:1223: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 17.\n","  warnings.warn(\n","/opt/homebrew/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:1223: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 17.\n","  warnings.warn(\n","/opt/homebrew/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:1223: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 17.\n","  warnings.warn(\n","/opt/homebrew/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:1223: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 17.\n","  warnings.warn(\n","/opt/homebrew/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:1223: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 17.\n","  warnings.warn(\n","/opt/homebrew/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:1223: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 17.\n","  warnings.warn(\n","/opt/homebrew/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:1223: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 17.\n","  warnings.warn(\n","/opt/homebrew/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:1223: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 17.\n","  warnings.warn(\n","/opt/homebrew/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:1223: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 17.\n","  warnings.warn(\n","/opt/homebrew/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:1223: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 17.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["Runtime:\n","CPU times: user 157 ms, sys: 73.3 ms, total: 231 ms\n","Wall time: 1.3 s\n"]}],"source":["%%time\n","log_model = LogisticRegression()\n","log_parameters = [{\"solver\": ['lbfgs', 'liblinear', 'newton-cg', 'newton-cholesky', 'sag', 'saga'],\n","                      \"fit_intercept\": [True, False],\n","                       \"penalty\": ['l1', 'l2', 'elasticnet'],\n","                      \"n_jobs\": list(range(1,200))}]\n","\n","log_clf = RandomizedSearchCV(log_model, log_parameters, scoring='roc_auc', n_jobs=-1, cv=cv)\n","log_clf.fit(features_train, target_train)\n","# create a variable for the best model\n","best_log = log_clf.best_estimator_\n","log_pred = best_log.predict(features_valid)\n","print('Runtime:')"]},{"cell_type":"code","execution_count":123,"metadata":{"execution":{"iopub.execute_input":"2023-11-30T16:53:41.367003Z","iopub.status.busy":"2023-11-30T16:53:41.363014Z","iopub.status.idle":"2023-11-30T16:53:45.451418Z","shell.execute_reply":"2023-11-30T16:53:45.450399Z","shell.execute_reply.started":"2023-11-30T16:53:41.366952Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Cross Validation Scores: [0.85124724 0.8327765  0.85178571 0.78652074 0.81972926 0.8421947\n"," 0.80411866 0.80244816 0.8662159  0.83342002 0.81100896 0.81016705\n"," 0.83533986 0.82379032 0.84467166 0.81984447 0.8406682  0.80745968\n"," 0.84688511 0.83786986 0.78523355 0.84017857 0.85815092 0.8328341\n"," 0.83398618 0.85092166 0.84161866 0.8296659  0.82209316 0.79449838\n"," 0.82300885 0.81800115 0.82263825 0.8344182  0.85351382 0.85711406\n"," 0.81486175 0.81431452 0.83685853 0.82714979 0.82764842 0.82730415\n"," 0.81641705 0.80270737 0.83554147 0.82926267 0.85374424 0.83767281\n"," 0.83240869 0.83619394]\n"]}],"source":["log_scores = cross_val_score(log_model, features_train, target_train, cv=cv, scoring='roc_auc')\n","print('Cross Validation Scores: {}'.format(log_scores))"]},{"cell_type":"code","execution_count":124,"metadata":{"execution":{"iopub.execute_input":"2023-11-30T16:53:45.453259Z","iopub.status.busy":"2023-11-30T16:53:45.452705Z","iopub.status.idle":"2023-11-30T16:53:45.479560Z","shell.execute_reply":"2023-11-30T16:53:45.478438Z","shell.execute_reply.started":"2023-11-30T16:53:45.453224Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Best hyperparameters: {'solver': 'sag', 'penalty': 'none', 'n_jobs': 71, 'fit_intercept': True}\n","\n","Best score: 0.8294281078869193\n","\n","Average Cross Validation Score: 0.829162458233739\n","\n","ROC AUC Score - Validation Dataset: 0.8557806295976105\n"]}],"source":["# summary\n","print('Best hyperparameters:',  log_clf.best_params_)\n","print()\n","print('Best score:',  log_clf.best_score_)\n","print()\n","print('Average Cross Validation Score: {}'.format(log_scores.mean()))\n","print()\n","print('ROC AUC Score - Validation Dataset:',  roc_auc_score(target_valid, log_clf.predict_proba(features_valid)[:, 1]))"]},{"cell_type":"markdown","metadata":{},"source":["# ROC AUC Curve - LogisticRegression"]},{"cell_type":"code","execution_count":125,"metadata":{"execution":{"iopub.execute_input":"2023-11-30T16:53:45.489716Z","iopub.status.busy":"2023-11-30T16:53:45.485889Z","iopub.status.idle":"2023-11-30T16:53:45.726171Z","shell.execute_reply":"2023-11-30T16:53:45.725272Z","shell.execute_reply.started":"2023-11-30T16:53:45.489647Z"},"trusted":true},"outputs":[{"data":{"application/vnd.plotly.v1+json":{"config":{"plotlyServerURL":"https://plot.ly"},"data":[{"fillpattern":{"shape":""},"hovertemplate":"False Positive Rate=%{x}<br>True Positive Rate=%{y}<extra></extra>","legendgroup":"","line":{"color":"#636efa"},"marker":{"symbol":"circle"},"mode":"lines","name":"","orientation":"v","showlegend":false,"stackgroup":"1","type":"scatter","x":[0,0,0,0.00267379679144385,0.00267379679144385,0.0053475935828877,0.0053475935828877,0.008021390374331552,0.008021390374331552,0.0106951871657754,0.0106951871657754,0.013368983957219251,0.013368983957219251,0.016042780748663103,0.016042780748663103,0.01871657754010695,0.01871657754010695,0.0213903743315508,0.0213903743315508,0.02406417112299465,0.02406417112299465,0.026737967914438502,0.026737967914438502,0.029411764705882353,0.029411764705882353,0.03208556149732621,0.03208556149732621,0.034759358288770054,0.034759358288770054,0.0374331550802139,0.0374331550802139,0.040106951871657755,0.040106951871657755,0.0427807486631016,0.0427807486631016,0.045454545454545456,0.045454545454545456,0.0481283422459893,0.0481283422459893,0.05080213903743316,0.05080213903743316,0.053475935828877004,0.053475935828877004,0.05614973262032086,0.05614973262032086,0.058823529411764705,0.058823529411764705,0.06149732620320856,0.06149732620320856,0.06417112299465241,0.06417112299465241,0.06684491978609626,0.06684491978609626,0.06951871657754011,0.06951871657754011,0.06951871657754011,0.07219251336898395,0.07219251336898395,0.0748663101604278,0.0748663101604278,0.07754010695187166,0.07754010695187166,0.08021390374331551,0.08021390374331551,0.08288770053475936,0.08288770053475936,0.0855614973262032,0.0855614973262032,0.08823529411764706,0.08823529411764706,0.09090909090909091,0.09090909090909091,0.09358288770053476,0.09358288770053476,0.09893048128342247,0.09893048128342247,0.10160427807486631,0.10160427807486631,0.10695187165775401,0.10695187165775401,0.10962566844919786,0.10962566844919786,0.11229946524064172,0.11229946524064172,0.11497326203208556,0.11497326203208556,0.11764705882352941,0.11764705882352941,0.12032085561497326,0.12032085561497326,0.12299465240641712,0.12299465240641712,0.12566844919786097,0.12566844919786097,0.12834224598930483,0.12834224598930483,0.13101604278074866,0.13101604278074866,0.13368983957219252,0.13368983957219252,0.13636363636363635,0.13636363636363635,0.13903743315508021,0.13903743315508021,0.1443850267379679,0.1443850267379679,0.14705882352941177,0.14705882352941177,0.1497326203208556,0.1497326203208556,0.15240641711229946,0.15240641711229946,0.15508021390374332,0.15508021390374332,0.15775401069518716,0.15775401069518716,0.16042780748663102,0.16042780748663102,0.16844919786096257,0.16844919786096257,0.1711229946524064,0.1711229946524064,0.17379679144385027,0.17379679144385027,0.17914438502673796,0.17914438502673796,0.18181818181818182,0.18181818181818182,0.18449197860962566,0.18449197860962566,0.18983957219251338,0.18983957219251338,0.1925133689839572,0.1925133689839572,0.19518716577540107,0.19518716577540107,0.19786096256684493,0.19786096256684493,0.20053475935828877,0.20053475935828877,0.20320855614973263,0.20320855614973263,0.20855614973262032,0.20855614973262032,0.21122994652406418,0.21122994652406418,0.21657754010695188,0.21657754010695188,0.2192513368983957,0.2192513368983957,0.22192513368983957,0.22192513368983957,0.22727272727272727,0.22727272727272727,0.22994652406417113,0.22994652406417113,0.23529411764705882,0.23529411764705882,0.24331550802139038,0.24331550802139038,0.24866310160427807,0.24866310160427807,0.25133689839572193,0.25133689839572193,0.2540106951871658,0.2540106951871658,0.25668449197860965,0.25668449197860965,0.25935828877005346,0.25935828877005346,0.26737967914438504,0.26737967914438504,0.2727272727272727,0.2727272727272727,0.27540106951871657,0.27540106951871657,0.27807486631016043,0.27807486631016043,0.28342245989304815,0.28342245989304815,0.28609625668449196,0.28609625668449196,0.2887700534759358,0.2887700534759358,0.2914438502673797,0.2914438502673797,0.29411764705882354,0.29411764705882354,0.2967914438502674,0.2967914438502674,0.3074866310160428,0.3074866310160428,0.31016042780748665,0.31016042780748665,0.3155080213903743,0.3155080213903743,0.3235294117647059,0.3235294117647059,0.32620320855614976,0.32620320855614976,0.32887700534759357,0.32887700534759357,0.3315508021390374,0.3315508021390374,0.339572192513369,0.339572192513369,0.3422459893048128,0.3422459893048128,0.3609625668449198,0.3609625668449198,0.36363636363636365,0.36363636363636365,0.3663101604278075,0.3663101604278075,0.3689839572192513,0.3689839572192513,0.3716577540106952,0.3716577540106952,0.3770053475935829,0.3770053475935829,0.38235294117647056,0.38235294117647056,0.39037433155080214,0.39037433155080214,0.39572192513368987,0.39572192513368987,0.40106951871657753,0.40106951871657753,0.40641711229946526,0.40641711229946526,0.4090909090909091,0.4090909090909091,0.4117647058823529,0.4117647058823529,0.4144385026737968,0.4144385026737968,0.42513368983957217,0.42513368983957217,0.4358288770053476,0.4358288770053476,0.4385026737967914,0.4385026737967914,0.4411764705882353,0.4411764705882353,0.44919786096256686,0.44919786096256686,0.45187165775401067,0.45187165775401067,0.4572192513368984,0.4572192513368984,0.47058823529411764,0.47058823529411764,0.4732620320855615,0.4732620320855615,0.47593582887700536,0.47593582887700536,0.48663101604278075,0.48663101604278075,0.4893048128342246,0.4893048128342246,0.4946524064171123,0.4946524064171123,0.5026737967914439,0.5026737967914439,0.5080213903743316,0.5080213903743316,0.516042780748663,0.516042780748663,0.5294117647058824,0.5294117647058824,0.5347593582887701,0.5347593582887701,0.5481283422459893,0.5481283422459893,0.5508021390374331,0.5508021390374331,0.553475935828877,0.553475935828877,0.5588235294117647,0.5588235294117647,0.5641711229946524,0.5641711229946524,0.5721925133689839,0.5721925133689839,0.5802139037433155,0.5802139037433155,0.5909090909090909,0.5909090909090909,0.6096256684491979,0.6096256684491979,0.6149732620320856,0.6149732620320856,0.6176470588235294,0.6176470588235294,0.6203208556149733,0.6203208556149733,0.6229946524064172,0.6229946524064172,0.6283422459893048,0.6283422459893048,0.6550802139037433,0.6550802139037433,0.6684491978609626,0.6684491978609626,0.6818181818181818,0.6818181818181818,0.6898395721925134,0.6898395721925134,0.6951871657754011,0.6951871657754011,0.7005347593582888,0.7005347593582888,0.7032085561497327,0.7032085561497327,0.7058823529411765,0.7058823529411765,0.7085561497326203,0.7085561497326203,0.7406417112299465,0.7406417112299465,0.7513368983957219,0.7513368983957219,0.7727272727272727,0.7727272727272727,0.7941176470588235,0.7941176470588235,0.7967914438502673,0.7967914438502673,0.8021390374331551,0.8021390374331551,0.8048128342245989,0.8048128342245989,0.8315508021390374,0.8315508021390374,0.839572192513369,0.839572192513369,0.8663101604278075,0.8663101604278075,0.8716577540106952,0.8716577540106952,0.8850267379679144,0.8850267379679144,0.8903743315508021,0.8903743315508021,0.8957219251336899,0.8957219251336899,0.9385026737967914,0.9385026737967914,0.9438502673796791,0.9438502673796791,0.983957219251337,0.983957219251337,1],"xaxis":"x","y":[0,0.000968054211035818,0.0377541142303969,0.0377541142303969,0.06485963213939981,0.06485963213939981,0.08518877057115198,0.08518877057115198,0.11713455953533398,0.11713455953533398,0.12681510164569215,0.12681510164569215,0.17521781219748306,0.17521781219748306,0.1994191674733785,0.1994191674733785,0.2362052274927396,0.2362052274927396,0.2681510164569216,0.2681510164569216,0.2962245885769603,0.2962245885769603,0.30880929332042595,0.30880929332042595,0.32720232333010646,0.32720232333010646,0.33494675701839305,0.33494675701839305,0.34172313649564373,0.34172313649564373,0.3543078412391094,0.3543078412391094,0.35721200387221685,0.35721200387221685,0.3727008712487899,0.3727008712487899,0.3862536302032914,0.3862536302032914,0.4027105517909003,0.4027105517909003,0.40658276863504356,0.40658276863504356,0.46079380445304935,0.46079380445304935,0.4927395934172314,0.4927395934172314,0.4995159728944821,0.4995159728944821,0.5140367860600193,0.5140367860600193,0.5217812197483059,0.5217812197483059,0.558567279767667,0.558567279767667,0.5605033881897387,0.5682478218780251,0.5682478218780251,0.57405614714424,0.57405614714424,0.5779283639883833,0.5779283639883833,0.5818005808325266,0.5818005808325266,0.5895450145208132,0.5895450145208132,0.5924491771539206,0.5924491771539206,0.5934172313649564,0.5934172313649564,0.5963213939980639,0.5963213939980639,0.6040658276863504,0.6040658276863504,0.6205227492739593,0.6205227492739593,0.6224588576960309,0.6224588576960309,0.6302032913843175,0.6302032913843175,0.6311713455953534,0.6311713455953534,0.6360116166505324,0.6360116166505324,0.6398838334946757,0.6398838334946757,0.6505324298160697,0.6505324298160697,0.6563407550822846,0.6563407550822846,0.6582768635043562,0.6582768635043562,0.6592449177153921,0.6592449177153921,0.6689254598257502,0.6689254598257502,0.675701839303001,0.675701839303001,0.6776379477250726,0.6776379477250726,0.6786060019361084,0.6786060019361084,0.6853823814133592,0.6853823814133592,0.6873184898354308,0.6873184898354308,0.6940948693126815,0.6940948693126815,0.6989351403678606,0.6989351403678606,0.7037754114230397,0.7037754114230397,0.712487899322362,0.712487899322362,0.7173281703775412,0.7173281703775412,0.7212003872216844,0.7212003872216844,0.7221684414327202,0.7221684414327202,0.7279767666989352,0.7279767666989352,0.7318489835430784,0.7318489835430784,0.7386253630203291,0.7386253630203291,0.7424975798644724,0.7424975798644724,0.7434656340755083,0.7434656340755083,0.750242013552759,0.750242013552759,0.7512100677637947,0.7512100677637947,0.7608906098741529,0.7608906098741529,0.7618586640851888,0.7618586640851888,0.7637947725072604,0.7637947725072604,0.7696030977734754,0.7696030977734754,0.7734753146176185,0.7734753146176185,0.7754114230396902,0.7754114230396902,0.782187802516941,0.782187802516941,0.7841239109390126,0.7841239109390126,0.7850919651500484,0.7850919651500484,0.7918683446272992,0.7918683446272992,0.7947725072604066,0.7947725072604066,0.7967086156824782,0.7967086156824782,0.7986447241045499,0.7986447241045499,0.8015488867376573,0.8015488867376573,0.8054211035818006,0.8054211035818006,0.8112294288480155,0.8112294288480155,0.814133591481123,0.814133591481123,0.8170377541142304,0.8170377541142304,0.8180058083252663,0.8180058083252663,0.8199419167473379,0.8199419167473379,0.8228460793804453,0.8228460793804453,0.8247821878025169,0.8247821878025169,0.8267182962245886,0.8267182962245886,0.8334946757018393,0.8334946757018393,0.8363988383349468,0.8363988383349468,0.8373668925459826,0.8373668925459826,0.8383349467570184,0.8383349467570184,0.8402710551790901,0.8402710551790901,0.8431752178121975,0.8431752178121975,0.8441432720232332,0.8441432720232332,0.8451113262342691,0.8451113262342691,0.846079380445305,0.846079380445305,0.8480154888673765,0.8480154888673765,0.8499515972894482,0.8499515972894482,0.850919651500484,0.850919651500484,0.8518877057115198,0.8518877057115198,0.8538238141335914,0.8538238141335914,0.8547918683446273,0.8547918683446273,0.8557599225556631,0.8557599225556631,0.8606001936108422,0.8606001936108422,0.861568247821878,0.861568247821878,0.8673765730880929,0.8673765730880929,0.872216844143272,0.872216844143272,0.8731848983543078,0.8731848983543078,0.8751210067763795,0.8751210067763795,0.8770571151984511,0.8770571151984511,0.8789932236205228,0.8789932236205228,0.8809293320425944,0.8809293320425944,0.8867376573088093,0.8867376573088093,0.888673765730881,0.888673765730881,0.8906098741529526,0.8906098741529526,0.8925459825750242,0.8925459825750242,0.89351403678606,0.89351403678606,0.8964181994191674,0.8964181994191674,0.8973862536302033,0.8973862536302033,0.8993223620522749,0.8993223620522749,0.9002904162633107,0.9002904162633107,0.9099709583736689,0.9099709583736689,0.9186834462729913,0.9186834462729913,0.9196515004840271,0.9196515004840271,0.925459825750242,0.925459825750242,0.9273959341723137,0.9273959341723137,0.9293320425943853,0.9293320425943853,0.9303000968054211,0.9303000968054211,0.9332042594385286,0.9332042594385286,0.936108422071636,0.936108422071636,0.9370764762826719,0.9370764762826719,0.9380445304937076,0.9380445304937076,0.9409486931268151,0.9409486931268151,0.9428848015488868,0.9428848015488868,0.9467570183930301,0.9467570183930301,0.9477250726040658,0.9477250726040658,0.9496611810261375,0.9496611810261375,0.9515972894482091,0.9515972894482091,0.9545014520813165,0.9545014520813165,0.9554695062923524,0.9554695062923524,0.9564375605033882,0.9564375605033882,0.957405614714424,0.957405614714424,0.9593417231364957,0.9593417231364957,0.9603097773475314,0.9603097773475314,0.9612778315585673,0.9612778315585673,0.9622458857696031,0.9622458857696031,0.9632139399806389,0.9632139399806389,0.9641819941916747,0.9641819941916747,0.9651500484027106,0.9651500484027106,0.9670861568247822,0.9670861568247822,0.9690222652468539,0.9690222652468539,0.9709583736689255,0.9709583736689255,0.9719264278799613,0.9719264278799613,0.9748305905130688,0.9748305905130688,0.9777347531461762,0.9777347531461762,0.978702807357212,0.978702807357212,0.9796708615682478,0.9796708615682478,0.9816069699903195,0.9816069699903195,0.9845111326234269,0.9845111326234269,0.9854791868344628,0.9854791868344628,0.9864472410454985,0.9864472410454985,0.9874152952565344,0.9874152952565344,0.9883833494675702,0.9883833494675702,0.989351403678606,0.989351403678606,0.9912875121006777,0.9912875121006777,0.9922555663117134,0.9922555663117134,0.9932236205227493,0.9932236205227493,0.9941916747337851,0.9941916747337851,0.995159728944821,0.995159728944821,0.9961277831558567,0.9961277831558567,0.9970958373668926,0.9970958373668926,0.9980638915779284,0.9980638915779284,0.9990319457889641,0.9990319457889641,1,1],"yaxis":"y"}],"layout":{"height":600,"legend":{"tracegroupgap":0},"shapes":[{"line":{"dash":"dash"},"type":"line","x0":0,"x1":1,"y0":0,"y1":1}],"showlegend":false,"template":{"data":{"bar":[{"error_x":{"color":"#2a3f5f"},"error_y":{"color":"#2a3f5f"},"marker":{"line":{"color":"white","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"barpolar":[{"marker":{"line":{"color":"white","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"carpet":[{"aaxis":{"endlinecolor":"#2a3f5f","gridcolor":"#C8D4E3","linecolor":"#C8D4E3","minorgridcolor":"#C8D4E3","startlinecolor":"#2a3f5f"},"baxis":{"endlinecolor":"#2a3f5f","gridcolor":"#C8D4E3","linecolor":"#C8D4E3","minorgridcolor":"#C8D4E3","startlinecolor":"#2a3f5f"},"type":"carpet"}],"choropleth":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"choropleth"}],"contour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"contour"}],"contourcarpet":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"contourcarpet"}],"heatmap":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmap"}],"heatmapgl":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmapgl"}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"histogram2d":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2d"}],"histogram2dcontour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2dcontour"}],"mesh3d":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"mesh3d"}],"parcoords":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"parcoords"}],"pie":[{"automargin":true,"type":"pie"}],"scatter":[{"fillpattern":{"fillmode":"overlay","size":10,"solidity":0.2},"type":"scatter"}],"scatter3d":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter3d"}],"scattercarpet":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattercarpet"}],"scattergeo":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergeo"}],"scattergl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergl"}],"scattermapbox":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattermapbox"}],"scatterpolar":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolar"}],"scatterpolargl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolargl"}],"scatterternary":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterternary"}],"surface":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"surface"}],"table":[{"cells":{"fill":{"color":"#EBF0F8"},"line":{"color":"white"}},"header":{"fill":{"color":"#C8D4E3"},"line":{"color":"white"}},"type":"table"}]},"layout":{"annotationdefaults":{"arrowcolor":"#2a3f5f","arrowhead":0,"arrowwidth":1},"autotypenumbers":"strict","coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]],"sequential":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"sequentialminus":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]]},"colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#2a3f5f"},"geo":{"bgcolor":"white","lakecolor":"white","landcolor":"white","showlakes":true,"showland":true,"subunitcolor":"#C8D4E3"},"hoverlabel":{"align":"left"},"hovermode":"closest","mapbox":{"style":"light"},"paper_bgcolor":"white","plot_bgcolor":"white","polar":{"angularaxis":{"gridcolor":"#EBF0F8","linecolor":"#EBF0F8","ticks":""},"bgcolor":"white","radialaxis":{"gridcolor":"#EBF0F8","linecolor":"#EBF0F8","ticks":""}},"scene":{"xaxis":{"backgroundcolor":"white","gridcolor":"#DFE8F3","gridwidth":2,"linecolor":"#EBF0F8","showbackground":true,"ticks":"","zerolinecolor":"#EBF0F8"},"yaxis":{"backgroundcolor":"white","gridcolor":"#DFE8F3","gridwidth":2,"linecolor":"#EBF0F8","showbackground":true,"ticks":"","zerolinecolor":"#EBF0F8"},"zaxis":{"backgroundcolor":"white","gridcolor":"#DFE8F3","gridwidth":2,"linecolor":"#EBF0F8","showbackground":true,"ticks":"","zerolinecolor":"#EBF0F8"}},"shapedefaults":{"line":{"color":"#2a3f5f"}},"ternary":{"aaxis":{"gridcolor":"#DFE8F3","linecolor":"#A2B1C6","ticks":""},"baxis":{"gridcolor":"#DFE8F3","linecolor":"#A2B1C6","ticks":""},"bgcolor":"white","caxis":{"gridcolor":"#DFE8F3","linecolor":"#A2B1C6","ticks":""}},"title":{"x":0.05},"xaxis":{"automargin":true,"gridcolor":"#EBF0F8","linecolor":"#EBF0F8","ticks":"","title":{"standoff":15},"zerolinecolor":"#EBF0F8","zerolinewidth":2},"yaxis":{"automargin":true,"gridcolor":"#EBF0F8","linecolor":"#EBF0F8","ticks":"","title":{"standoff":15},"zerolinecolor":"#EBF0F8","zerolinewidth":2}}},"title":{"text":"ROC Curve (AUC=0.8558)"},"width":800,"xaxis":{"anchor":"y","constrain":"domain","domain":[0,1],"title":{"text":"False Positive Rate"}},"yaxis":{"anchor":"x","domain":[0,1],"scaleanchor":"x","scaleratio":1,"title":{"text":"True Positive Rate"}}}},"text/html":["<div>                            <div id=\"b2932fa2-3d5b-4263-ad36-a12878ac0f36\" class=\"plotly-graph-div\" style=\"height:600px; width:800px;\"></div>            <script type=\"text/javascript\">                require([\"plotly\"], function(Plotly) {                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"b2932fa2-3d5b-4263-ad36-a12878ac0f36\")) {                    Plotly.newPlot(                        \"b2932fa2-3d5b-4263-ad36-a12878ac0f36\",                        [{\"fillpattern\":{\"shape\":\"\"},\"hovertemplate\":\"False Positive Rate=%{x}\\u003cbr\\u003eTrue Positive Rate=%{y}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"\",\"line\":{\"color\":\"#636efa\"},\"marker\":{\"symbol\":\"circle\"},\"mode\":\"lines\",\"name\":\"\",\"orientation\":\"v\",\"showlegend\":false,\"stackgroup\":\"1\",\"x\":[0.0,0.0,0.0,0.00267379679144385,0.00267379679144385,0.0053475935828877,0.0053475935828877,0.008021390374331552,0.008021390374331552,0.0106951871657754,0.0106951871657754,0.013368983957219251,0.013368983957219251,0.016042780748663103,0.016042780748663103,0.01871657754010695,0.01871657754010695,0.0213903743315508,0.0213903743315508,0.02406417112299465,0.02406417112299465,0.026737967914438502,0.026737967914438502,0.029411764705882353,0.029411764705882353,0.03208556149732621,0.03208556149732621,0.034759358288770054,0.034759358288770054,0.0374331550802139,0.0374331550802139,0.040106951871657755,0.040106951871657755,0.0427807486631016,0.0427807486631016,0.045454545454545456,0.045454545454545456,0.0481283422459893,0.0481283422459893,0.05080213903743316,0.05080213903743316,0.053475935828877004,0.053475935828877004,0.05614973262032086,0.05614973262032086,0.058823529411764705,0.058823529411764705,0.06149732620320856,0.06149732620320856,0.06417112299465241,0.06417112299465241,0.06684491978609626,0.06684491978609626,0.06951871657754011,0.06951871657754011,0.06951871657754011,0.07219251336898395,0.07219251336898395,0.0748663101604278,0.0748663101604278,0.07754010695187166,0.07754010695187166,0.08021390374331551,0.08021390374331551,0.08288770053475936,0.08288770053475936,0.0855614973262032,0.0855614973262032,0.08823529411764706,0.08823529411764706,0.09090909090909091,0.09090909090909091,0.09358288770053476,0.09358288770053476,0.09893048128342247,0.09893048128342247,0.10160427807486631,0.10160427807486631,0.10695187165775401,0.10695187165775401,0.10962566844919786,0.10962566844919786,0.11229946524064172,0.11229946524064172,0.11497326203208556,0.11497326203208556,0.11764705882352941,0.11764705882352941,0.12032085561497326,0.12032085561497326,0.12299465240641712,0.12299465240641712,0.12566844919786097,0.12566844919786097,0.12834224598930483,0.12834224598930483,0.13101604278074866,0.13101604278074866,0.13368983957219252,0.13368983957219252,0.13636363636363635,0.13636363636363635,0.13903743315508021,0.13903743315508021,0.1443850267379679,0.1443850267379679,0.14705882352941177,0.14705882352941177,0.1497326203208556,0.1497326203208556,0.15240641711229946,0.15240641711229946,0.15508021390374332,0.15508021390374332,0.15775401069518716,0.15775401069518716,0.16042780748663102,0.16042780748663102,0.16844919786096257,0.16844919786096257,0.1711229946524064,0.1711229946524064,0.17379679144385027,0.17379679144385027,0.17914438502673796,0.17914438502673796,0.18181818181818182,0.18181818181818182,0.18449197860962566,0.18449197860962566,0.18983957219251338,0.18983957219251338,0.1925133689839572,0.1925133689839572,0.19518716577540107,0.19518716577540107,0.19786096256684493,0.19786096256684493,0.20053475935828877,0.20053475935828877,0.20320855614973263,0.20320855614973263,0.20855614973262032,0.20855614973262032,0.21122994652406418,0.21122994652406418,0.21657754010695188,0.21657754010695188,0.2192513368983957,0.2192513368983957,0.22192513368983957,0.22192513368983957,0.22727272727272727,0.22727272727272727,0.22994652406417113,0.22994652406417113,0.23529411764705882,0.23529411764705882,0.24331550802139038,0.24331550802139038,0.24866310160427807,0.24866310160427807,0.25133689839572193,0.25133689839572193,0.2540106951871658,0.2540106951871658,0.25668449197860965,0.25668449197860965,0.25935828877005346,0.25935828877005346,0.26737967914438504,0.26737967914438504,0.2727272727272727,0.2727272727272727,0.27540106951871657,0.27540106951871657,0.27807486631016043,0.27807486631016043,0.28342245989304815,0.28342245989304815,0.28609625668449196,0.28609625668449196,0.2887700534759358,0.2887700534759358,0.2914438502673797,0.2914438502673797,0.29411764705882354,0.29411764705882354,0.2967914438502674,0.2967914438502674,0.3074866310160428,0.3074866310160428,0.31016042780748665,0.31016042780748665,0.3155080213903743,0.3155080213903743,0.3235294117647059,0.3235294117647059,0.32620320855614976,0.32620320855614976,0.32887700534759357,0.32887700534759357,0.3315508021390374,0.3315508021390374,0.339572192513369,0.339572192513369,0.3422459893048128,0.3422459893048128,0.3609625668449198,0.3609625668449198,0.36363636363636365,0.36363636363636365,0.3663101604278075,0.3663101604278075,0.3689839572192513,0.3689839572192513,0.3716577540106952,0.3716577540106952,0.3770053475935829,0.3770053475935829,0.38235294117647056,0.38235294117647056,0.39037433155080214,0.39037433155080214,0.39572192513368987,0.39572192513368987,0.40106951871657753,0.40106951871657753,0.40641711229946526,0.40641711229946526,0.4090909090909091,0.4090909090909091,0.4117647058823529,0.4117647058823529,0.4144385026737968,0.4144385026737968,0.42513368983957217,0.42513368983957217,0.4358288770053476,0.4358288770053476,0.4385026737967914,0.4385026737967914,0.4411764705882353,0.4411764705882353,0.44919786096256686,0.44919786096256686,0.45187165775401067,0.45187165775401067,0.4572192513368984,0.4572192513368984,0.47058823529411764,0.47058823529411764,0.4732620320855615,0.4732620320855615,0.47593582887700536,0.47593582887700536,0.48663101604278075,0.48663101604278075,0.4893048128342246,0.4893048128342246,0.4946524064171123,0.4946524064171123,0.5026737967914439,0.5026737967914439,0.5080213903743316,0.5080213903743316,0.516042780748663,0.516042780748663,0.5294117647058824,0.5294117647058824,0.5347593582887701,0.5347593582887701,0.5481283422459893,0.5481283422459893,0.5508021390374331,0.5508021390374331,0.553475935828877,0.553475935828877,0.5588235294117647,0.5588235294117647,0.5641711229946524,0.5641711229946524,0.5721925133689839,0.5721925133689839,0.5802139037433155,0.5802139037433155,0.5909090909090909,0.5909090909090909,0.6096256684491979,0.6096256684491979,0.6149732620320856,0.6149732620320856,0.6176470588235294,0.6176470588235294,0.6203208556149733,0.6203208556149733,0.6229946524064172,0.6229946524064172,0.6283422459893048,0.6283422459893048,0.6550802139037433,0.6550802139037433,0.6684491978609626,0.6684491978609626,0.6818181818181818,0.6818181818181818,0.6898395721925134,0.6898395721925134,0.6951871657754011,0.6951871657754011,0.7005347593582888,0.7005347593582888,0.7032085561497327,0.7032085561497327,0.7058823529411765,0.7058823529411765,0.7085561497326203,0.7085561497326203,0.7406417112299465,0.7406417112299465,0.7513368983957219,0.7513368983957219,0.7727272727272727,0.7727272727272727,0.7941176470588235,0.7941176470588235,0.7967914438502673,0.7967914438502673,0.8021390374331551,0.8021390374331551,0.8048128342245989,0.8048128342245989,0.8315508021390374,0.8315508021390374,0.839572192513369,0.839572192513369,0.8663101604278075,0.8663101604278075,0.8716577540106952,0.8716577540106952,0.8850267379679144,0.8850267379679144,0.8903743315508021,0.8903743315508021,0.8957219251336899,0.8957219251336899,0.9385026737967914,0.9385026737967914,0.9438502673796791,0.9438502673796791,0.983957219251337,0.983957219251337,1.0],\"xaxis\":\"x\",\"y\":[0.0,0.000968054211035818,0.0377541142303969,0.0377541142303969,0.06485963213939981,0.06485963213939981,0.08518877057115198,0.08518877057115198,0.11713455953533398,0.11713455953533398,0.12681510164569215,0.12681510164569215,0.17521781219748306,0.17521781219748306,0.1994191674733785,0.1994191674733785,0.2362052274927396,0.2362052274927396,0.2681510164569216,0.2681510164569216,0.2962245885769603,0.2962245885769603,0.30880929332042595,0.30880929332042595,0.32720232333010646,0.32720232333010646,0.33494675701839305,0.33494675701839305,0.34172313649564373,0.34172313649564373,0.3543078412391094,0.3543078412391094,0.35721200387221685,0.35721200387221685,0.3727008712487899,0.3727008712487899,0.3862536302032914,0.3862536302032914,0.4027105517909003,0.4027105517909003,0.40658276863504356,0.40658276863504356,0.46079380445304935,0.46079380445304935,0.4927395934172314,0.4927395934172314,0.4995159728944821,0.4995159728944821,0.5140367860600193,0.5140367860600193,0.5217812197483059,0.5217812197483059,0.558567279767667,0.558567279767667,0.5605033881897387,0.5682478218780251,0.5682478218780251,0.57405614714424,0.57405614714424,0.5779283639883833,0.5779283639883833,0.5818005808325266,0.5818005808325266,0.5895450145208132,0.5895450145208132,0.5924491771539206,0.5924491771539206,0.5934172313649564,0.5934172313649564,0.5963213939980639,0.5963213939980639,0.6040658276863504,0.6040658276863504,0.6205227492739593,0.6205227492739593,0.6224588576960309,0.6224588576960309,0.6302032913843175,0.6302032913843175,0.6311713455953534,0.6311713455953534,0.6360116166505324,0.6360116166505324,0.6398838334946757,0.6398838334946757,0.6505324298160697,0.6505324298160697,0.6563407550822846,0.6563407550822846,0.6582768635043562,0.6582768635043562,0.6592449177153921,0.6592449177153921,0.6689254598257502,0.6689254598257502,0.675701839303001,0.675701839303001,0.6776379477250726,0.6776379477250726,0.6786060019361084,0.6786060019361084,0.6853823814133592,0.6853823814133592,0.6873184898354308,0.6873184898354308,0.6940948693126815,0.6940948693126815,0.6989351403678606,0.6989351403678606,0.7037754114230397,0.7037754114230397,0.712487899322362,0.712487899322362,0.7173281703775412,0.7173281703775412,0.7212003872216844,0.7212003872216844,0.7221684414327202,0.7221684414327202,0.7279767666989352,0.7279767666989352,0.7318489835430784,0.7318489835430784,0.7386253630203291,0.7386253630203291,0.7424975798644724,0.7424975798644724,0.7434656340755083,0.7434656340755083,0.750242013552759,0.750242013552759,0.7512100677637947,0.7512100677637947,0.7608906098741529,0.7608906098741529,0.7618586640851888,0.7618586640851888,0.7637947725072604,0.7637947725072604,0.7696030977734754,0.7696030977734754,0.7734753146176185,0.7734753146176185,0.7754114230396902,0.7754114230396902,0.782187802516941,0.782187802516941,0.7841239109390126,0.7841239109390126,0.7850919651500484,0.7850919651500484,0.7918683446272992,0.7918683446272992,0.7947725072604066,0.7947725072604066,0.7967086156824782,0.7967086156824782,0.7986447241045499,0.7986447241045499,0.8015488867376573,0.8015488867376573,0.8054211035818006,0.8054211035818006,0.8112294288480155,0.8112294288480155,0.814133591481123,0.814133591481123,0.8170377541142304,0.8170377541142304,0.8180058083252663,0.8180058083252663,0.8199419167473379,0.8199419167473379,0.8228460793804453,0.8228460793804453,0.8247821878025169,0.8247821878025169,0.8267182962245886,0.8267182962245886,0.8334946757018393,0.8334946757018393,0.8363988383349468,0.8363988383349468,0.8373668925459826,0.8373668925459826,0.8383349467570184,0.8383349467570184,0.8402710551790901,0.8402710551790901,0.8431752178121975,0.8431752178121975,0.8441432720232332,0.8441432720232332,0.8451113262342691,0.8451113262342691,0.846079380445305,0.846079380445305,0.8480154888673765,0.8480154888673765,0.8499515972894482,0.8499515972894482,0.850919651500484,0.850919651500484,0.8518877057115198,0.8518877057115198,0.8538238141335914,0.8538238141335914,0.8547918683446273,0.8547918683446273,0.8557599225556631,0.8557599225556631,0.8606001936108422,0.8606001936108422,0.861568247821878,0.861568247821878,0.8673765730880929,0.8673765730880929,0.872216844143272,0.872216844143272,0.8731848983543078,0.8731848983543078,0.8751210067763795,0.8751210067763795,0.8770571151984511,0.8770571151984511,0.8789932236205228,0.8789932236205228,0.8809293320425944,0.8809293320425944,0.8867376573088093,0.8867376573088093,0.888673765730881,0.888673765730881,0.8906098741529526,0.8906098741529526,0.8925459825750242,0.8925459825750242,0.89351403678606,0.89351403678606,0.8964181994191674,0.8964181994191674,0.8973862536302033,0.8973862536302033,0.8993223620522749,0.8993223620522749,0.9002904162633107,0.9002904162633107,0.9099709583736689,0.9099709583736689,0.9186834462729913,0.9186834462729913,0.9196515004840271,0.9196515004840271,0.925459825750242,0.925459825750242,0.9273959341723137,0.9273959341723137,0.9293320425943853,0.9293320425943853,0.9303000968054211,0.9303000968054211,0.9332042594385286,0.9332042594385286,0.936108422071636,0.936108422071636,0.9370764762826719,0.9370764762826719,0.9380445304937076,0.9380445304937076,0.9409486931268151,0.9409486931268151,0.9428848015488868,0.9428848015488868,0.9467570183930301,0.9467570183930301,0.9477250726040658,0.9477250726040658,0.9496611810261375,0.9496611810261375,0.9515972894482091,0.9515972894482091,0.9545014520813165,0.9545014520813165,0.9554695062923524,0.9554695062923524,0.9564375605033882,0.9564375605033882,0.957405614714424,0.957405614714424,0.9593417231364957,0.9593417231364957,0.9603097773475314,0.9603097773475314,0.9612778315585673,0.9612778315585673,0.9622458857696031,0.9622458857696031,0.9632139399806389,0.9632139399806389,0.9641819941916747,0.9641819941916747,0.9651500484027106,0.9651500484027106,0.9670861568247822,0.9670861568247822,0.9690222652468539,0.9690222652468539,0.9709583736689255,0.9709583736689255,0.9719264278799613,0.9719264278799613,0.9748305905130688,0.9748305905130688,0.9777347531461762,0.9777347531461762,0.978702807357212,0.978702807357212,0.9796708615682478,0.9796708615682478,0.9816069699903195,0.9816069699903195,0.9845111326234269,0.9845111326234269,0.9854791868344628,0.9854791868344628,0.9864472410454985,0.9864472410454985,0.9874152952565344,0.9874152952565344,0.9883833494675702,0.9883833494675702,0.989351403678606,0.989351403678606,0.9912875121006777,0.9912875121006777,0.9922555663117134,0.9922555663117134,0.9932236205227493,0.9932236205227493,0.9941916747337851,0.9941916747337851,0.995159728944821,0.995159728944821,0.9961277831558567,0.9961277831558567,0.9970958373668926,0.9970958373668926,0.9980638915779284,0.9980638915779284,0.9990319457889641,0.9990319457889641,1.0,1.0],\"yaxis\":\"y\",\"type\":\"scatter\"}],                        {\"template\":{\"data\":{\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"white\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"white\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"#C8D4E3\",\"linecolor\":\"#C8D4E3\",\"minorgridcolor\":\"#C8D4E3\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"#C8D4E3\",\"linecolor\":\"#C8D4E3\",\"minorgridcolor\":\"#C8D4E3\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"choropleth\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"choropleth\"}],\"contourcarpet\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"contourcarpet\"}],\"contour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"contour\"}],\"heatmapgl\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmapgl\"}],\"heatmap\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmap\"}],\"histogram2dcontour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2dcontour\"}],\"histogram2d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2d\"}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"mesh3d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"mesh3d\"}],\"parcoords\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"parcoords\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}],\"scatter3d\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatter3d\"}],\"scattercarpet\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattercarpet\"}],\"scattergeo\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergeo\"}],\"scattergl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergl\"}],\"scattermapbox\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattermapbox\"}],\"scatterpolargl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolargl\"}],\"scatterpolar\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolar\"}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"scatterternary\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterternary\"}],\"surface\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"surface\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}]},\"layout\":{\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"autotypenumbers\":\"strict\",\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]],\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]},\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"geo\":{\"bgcolor\":\"white\",\"lakecolor\":\"white\",\"landcolor\":\"white\",\"showlakes\":true,\"showland\":true,\"subunitcolor\":\"#C8D4E3\"},\"hoverlabel\":{\"align\":\"left\"},\"hovermode\":\"closest\",\"mapbox\":{\"style\":\"light\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"white\",\"polar\":{\"angularaxis\":{\"gridcolor\":\"#EBF0F8\",\"linecolor\":\"#EBF0F8\",\"ticks\":\"\"},\"bgcolor\":\"white\",\"radialaxis\":{\"gridcolor\":\"#EBF0F8\",\"linecolor\":\"#EBF0F8\",\"ticks\":\"\"}},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"white\",\"gridcolor\":\"#DFE8F3\",\"gridwidth\":2,\"linecolor\":\"#EBF0F8\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"#EBF0F8\"},\"yaxis\":{\"backgroundcolor\":\"white\",\"gridcolor\":\"#DFE8F3\",\"gridwidth\":2,\"linecolor\":\"#EBF0F8\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"#EBF0F8\"},\"zaxis\":{\"backgroundcolor\":\"white\",\"gridcolor\":\"#DFE8F3\",\"gridwidth\":2,\"linecolor\":\"#EBF0F8\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"#EBF0F8\"}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"ternary\":{\"aaxis\":{\"gridcolor\":\"#DFE8F3\",\"linecolor\":\"#A2B1C6\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"#DFE8F3\",\"linecolor\":\"#A2B1C6\",\"ticks\":\"\"},\"bgcolor\":\"white\",\"caxis\":{\"gridcolor\":\"#DFE8F3\",\"linecolor\":\"#A2B1C6\",\"ticks\":\"\"}},\"title\":{\"x\":0.05},\"xaxis\":{\"automargin\":true,\"gridcolor\":\"#EBF0F8\",\"linecolor\":\"#EBF0F8\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"#EBF0F8\",\"zerolinewidth\":2},\"yaxis\":{\"automargin\":true,\"gridcolor\":\"#EBF0F8\",\"linecolor\":\"#EBF0F8\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"#EBF0F8\",\"zerolinewidth\":2}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"False Positive Rate\"},\"constrain\":\"domain\"},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"True Positive Rate\"},\"scaleanchor\":\"x\",\"scaleratio\":1},\"legend\":{\"tracegroupgap\":0},\"title\":{\"text\":\"ROC Curve (AUC=0.8558)\"},\"height\":600,\"width\":800,\"shapes\":[{\"line\":{\"dash\":\"dash\"},\"type\":\"line\",\"x0\":0,\"x1\":1,\"y0\":0,\"y1\":1}],\"showlegend\":false},                        {\"responsive\": true}                    ).then(function(){\n","                            \n","var gd = document.getElementById('b2932fa2-3d5b-4263-ad36-a12878ac0f36');\n","var x = new MutationObserver(function (mutations, observer) {{\n","        var display = window.getComputedStyle(gd).display;\n","        if (!display || display === 'none') {{\n","            console.log([gd, 'removed!']);\n","            Plotly.purge(gd);\n","            observer.disconnect();\n","        }}\n","}});\n","\n","// Listen for the removal of the full notebook cells\n","var notebookContainer = gd.closest('#notebook-container');\n","if (notebookContainer) {{\n","    x.observe(notebookContainer, {childList: true});\n","}}\n","\n","// Listen for the clearing of the current output cell\n","var outputEl = gd.closest('.output');\n","if (outputEl) {{\n","    x.observe(outputEl, {childList: true});\n","}}\n","\n","                        })                };                });            </script>        </div>"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.plotly.v1+json":{"config":{"plotlyServerURL":"https://plot.ly"},"data":[{"fillpattern":{"shape":""},"hovertemplate":"Recall=%{x}<br>Precision=%{y}<extra></extra>","legendgroup":"","line":{"color":"#636efa"},"marker":{"symbol":"circle"},"mode":"lines","name":"","orientation":"v","showlegend":false,"stackgroup":"1","type":"scatter","x":[1,1,1,1,1,1,1,0.9990319457889641,0.9990319457889641,0.9990319457889641,0.9990319457889641,0.9990319457889641,0.9990319457889641,0.9990319457889641,0.9990319457889641,0.9990319457889641,0.9990319457889641,0.9990319457889641,0.9990319457889641,0.9990319457889641,0.9990319457889641,0.9990319457889641,0.9990319457889641,0.9980638915779284,0.9980638915779284,0.9980638915779284,0.9970958373668926,0.9970958373668926,0.9970958373668926,0.9970958373668926,0.9970958373668926,0.9970958373668926,0.9970958373668926,0.9970958373668926,0.9970958373668926,0.9970958373668926,0.9970958373668926,0.9970958373668926,0.9970958373668926,0.9970958373668926,0.9970958373668926,0.9970958373668926,0.9970958373668926,0.9961277831558567,0.9961277831558567,0.9961277831558567,0.995159728944821,0.995159728944821,0.995159728944821,0.9941916747337851,0.9941916747337851,0.9941916747337851,0.9941916747337851,0.9941916747337851,0.9941916747337851,0.9932236205227493,0.9932236205227493,0.9932236205227493,0.9922555663117134,0.9922555663117134,0.9922555663117134,0.9922555663117134,0.9922555663117134,0.9922555663117134,0.9922555663117134,0.9922555663117134,0.9922555663117134,0.9922555663117134,0.9922555663117134,0.9912875121006777,0.9912875121006777,0.9912875121006777,0.9912875121006777,0.9903194578896418,0.989351403678606,0.989351403678606,0.989351403678606,0.989351403678606,0.989351403678606,0.989351403678606,0.989351403678606,0.989351403678606,0.989351403678606,0.989351403678606,0.989351403678606,0.9883833494675702,0.9883833494675702,0.9874152952565344,0.9874152952565344,0.9874152952565344,0.9864472410454985,0.9864472410454985,0.9854791868344628,0.9854791868344628,0.9854791868344628,0.9854791868344628,0.9854791868344628,0.9854791868344628,0.9854791868344628,0.9854791868344628,0.9854791868344628,0.9845111326234269,0.9845111326234269,0.9845111326234269,0.9845111326234269,0.9845111326234269,0.9845111326234269,0.9845111326234269,0.9845111326234269,0.9845111326234269,0.9835430784123911,0.9825750242013552,0.9816069699903195,0.9816069699903195,0.9816069699903195,0.9816069699903195,0.9816069699903195,0.9806389157792836,0.9796708615682478,0.9796708615682478,0.9796708615682478,0.9796708615682478,0.9796708615682478,0.9796708615682478,0.9796708615682478,0.9796708615682478,0.9796708615682478,0.9796708615682478,0.9796708615682478,0.9796708615682478,0.9796708615682478,0.978702807357212,0.978702807357212,0.9777347531461762,0.9777347531461762,0.9767666989351403,0.9757986447241046,0.9748305905130688,0.9748305905130688,0.9738625363020329,0.972894482090997,0.9719264278799613,0.9719264278799613,0.9719264278799613,0.9709583736689255,0.9709583736689255,0.9709583736689255,0.9699903194578896,0.9690222652468539,0.9690222652468539,0.9690222652468539,0.9690222652468539,0.968054211035818,0.9670861568247822,0.9670861568247822,0.9670861568247822,0.9670861568247822,0.9670861568247822,0.9670861568247822,0.9661181026137464,0.9651500484027106,0.9651500484027106,0.9651500484027106,0.9651500484027106,0.9651500484027106,0.9651500484027106,0.9641819941916747,0.9641819941916747,0.9641819941916747,0.9641819941916747,0.9641819941916747,0.9641819941916747,0.9641819941916747,0.9641819941916747,0.9641819941916747,0.9641819941916747,0.9641819941916747,0.9632139399806389,0.9632139399806389,0.9632139399806389,0.9622458857696031,0.9622458857696031,0.9612778315585673,0.9612778315585673,0.9603097773475314,0.9603097773475314,0.9593417231364957,0.9593417231364957,0.9593417231364957,0.9583736689254598,0.957405614714424,0.957405614714424,0.957405614714424,0.957405614714424,0.957405614714424,0.957405614714424,0.957405614714424,0.957405614714424,0.9564375605033882,0.9564375605033882,0.9564375605033882,0.9564375605033882,0.9564375605033882,0.9554695062923524,0.9554695062923524,0.9554695062923524,0.9554695062923524,0.9545014520813165,0.9545014520813165,0.9545014520813165,0.9545014520813165,0.9535333978702807,0.952565343659245,0.9515972894482091,0.9515972894482091,0.9515972894482091,0.9506292352371732,0.9496611810261375,0.9496611810261375,0.9496611810261375,0.9486931268151017,0.9477250726040658,0.9477250726040658,0.9467570183930301,0.9467570183930301,0.9457889641819942,0.9448209099709584,0.9438528557599225,0.9428848015488868,0.9428848015488868,0.9428848015488868,0.9428848015488868,0.9428848015488868,0.9428848015488868,0.9419167473378509,0.9409486931268151,0.9409486931268151,0.9409486931268151,0.9399806389157793,0.9390125847047435,0.9380445304937076,0.9380445304937076,0.9380445304937076,0.9380445304937076,0.9380445304937076,0.9380445304937076,0.9370764762826719,0.9370764762826719,0.9370764762826719,0.9370764762826719,0.936108422071636,0.936108422071636,0.936108422071636,0.9351403678606002,0.9341723136495643,0.9332042594385286,0.9332042594385286,0.9332042594385286,0.9332042594385286,0.9322362052274927,0.9312681510164569,0.9303000968054211,0.9303000968054211,0.9303000968054211,0.9293320425943853,0.9293320425943853,0.9283639883833494,0.9273959341723137,0.9273959341723137,0.9273959341723137,0.9273959341723137,0.9273959341723137,0.9264278799612778,0.925459825750242,0.925459825750242,0.9244917715392061,0.9235237173281704,0.9225556631171346,0.9215876089060987,0.920619554695063,0.9196515004840271,0.9196515004840271,0.9186834462729913,0.9186834462729913,0.9186834462729913,0.9186834462729913,0.9186834462729913,0.9186834462729913,0.9177153920619555,0.9167473378509197,0.9157792836398838,0.914811229428848,0.9138431752178122,0.9128751210067764,0.9119070667957405,0.9109390125847048,0.9099709583736689,0.9099709583736689,0.9099709583736689,0.9090029041626331,0.9080348499515973,0.9070667957405615,0.9060987415295256,0.9051306873184899,0.904162633107454,0.9031945788964182,0.9022265246853823,0.9012584704743466,0.9002904162633107,0.9002904162633107,0.8993223620522749,0.8993223620522749,0.8993223620522749,0.8993223620522749,0.8983543078412392,0.8973862536302033,0.8973862536302033,0.8964181994191674,0.8964181994191674,0.8954501452081317,0.8944820909970959,0.89351403678606,0.89351403678606,0.89351403678606,0.89351403678606,0.89351403678606,0.8925459825750242,0.8925459825750242,0.8925459825750242,0.8925459825750242,0.8925459825750242,0.8915779283639884,0.8906098741529526,0.8906098741529526,0.8896418199419167,0.888673765730881,0.888673765730881,0.8877057115198451,0.8867376573088093,0.8867376573088093,0.8857696030977735,0.8848015488867377,0.8838334946757018,0.882865440464666,0.8818973862536302,0.8809293320425944,0.8809293320425944,0.8809293320425944,0.8799612778315585,0.8789932236205228,0.8789932236205228,0.8789932236205228,0.8780251694094869,0.8770571151984511,0.8770571151984511,0.8770571151984511,0.8760890609874153,0.8751210067763795,0.8751210067763795,0.8751210067763795,0.8751210067763795,0.8741529525653436,0.8731848983543078,0.8731848983543078,0.8731848983543078,0.872216844143272,0.872216844143272,0.872216844143272,0.8712487899322362,0.8702807357212003,0.8693126815101646,0.8683446272991288,0.8673765730880929,0.8673765730880929,0.8664085188770572,0.8654404646660213,0.8644724104549855,0.8635043562439496,0.8625363020329139,0.861568247821878,0.861568247821878,0.8606001936108422,0.8606001936108422,0.8596321393998064,0.8586640851887706,0.8576960309777347,0.856727976766699,0.8557599225556631,0.8557599225556631,0.8547918683446273,0.8547918683446273,0.8547918683446273,0.8547918683446273,0.8547918683446273,0.8547918683446273,0.8547918683446273,0.8547918683446273,0.8538238141335914,0.8538238141335914,0.8528557599225557,0.8518877057115198,0.8518877057115198,0.8518877057115198,0.8518877057115198,0.850919651500484,0.850919651500484,0.8499515972894482,0.8499515972894482,0.8489835430784124,0.8480154888673765,0.8480154888673765,0.8470474346563408,0.846079380445305,0.846079380445305,0.846079380445305,0.846079380445305,0.8451113262342691,0.8451113262342691,0.8451113262342691,0.8441432720232332,0.8441432720232332,0.8431752178121975,0.8431752178121975,0.8431752178121975,0.8431752178121975,0.8431752178121975,0.8422071636011617,0.8412391093901258,0.8402710551790901,0.8402710551790901,0.8393030009680542,0.8383349467570184,0.8383349467570184,0.8373668925459826,0.8373668925459826,0.8363988383349468,0.8363988383349468,0.8354307841239109,0.8344627299128751,0.8334946757018393,0.8334946757018393,0.8325266214908035,0.8315585672797676,0.8305905130687319,0.829622458857696,0.8286544046466602,0.8276863504356244,0.8267182962245886,0.8267182962245886,0.8267182962245886,0.8257502420135527,0.8247821878025169,0.8247821878025169,0.8238141335914811,0.8228460793804453,0.8228460793804453,0.8218780251694094,0.8209099709583737,0.8199419167473379,0.8199419167473379,0.8199419167473379,0.818973862536302,0.8180058083252663,0.8180058083252663,0.8180058083252663,0.8180058083252663,0.8170377541142304,0.8170377541142304,0.8160696999031946,0.8151016456921588,0.814133591481123,0.814133591481123,0.8131655372700871,0.8121974830590513,0.8112294288480155,0.8112294288480155,0.8102613746369797,0.8092933204259438,0.8083252662149081,0.8073572120038722,0.8063891577928364,0.8054211035818006,0.8054211035818006,0.8044530493707648,0.8034849951597289,0.8025169409486931,0.8015488867376573,0.8015488867376573,0.8015488867376573,0.8005808325266215,0.7996127783155856,0.7986447241045499,0.7986447241045499,0.7986447241045499,0.7986447241045499,0.797676669893514,0.7967086156824782,0.7967086156824782,0.7967086156824782,0.7957405614714425,0.7947725072604066,0.7947725072604066,0.7938044530493708,0.7928363988383349,0.7918683446272992,0.7918683446272992,0.7918683446272992,0.7909002904162633,0.7899322362052275,0.7889641819941917,0.7879961277831559,0.78702807357212,0.7860600193610843,0.7850919651500484,0.7850919651500484,0.7841239109390126,0.7841239109390126,0.7831558567279767,0.782187802516941,0.782187802516941,0.782187802516941,0.7812197483059051,0.7802516940948693,0.7792836398838335,0.7783155856727977,0.7773475314617618,0.7763794772507261,0.7754114230396902,0.7754114230396902,0.7744433688286544,0.7734753146176185,0.7734753146176185,0.7734753146176185,0.7725072604065828,0.771539206195547,0.7705711519845111,0.7696030977734754,0.7696030977734754,0.7686350435624395,0.7676669893514037,0.7666989351403679,0.7657308809293321,0.7647628267182962,0.7637947725072604,0.7637947725072604,0.7628267182962246,0.7618586640851888,0.7618586640851888,0.7608906098741529,0.7608906098741529,0.7599225556631172,0.7589545014520813,0.7579864472410455,0.7570183930300097,0.7560503388189739,0.755082284607938,0.7541142303969022,0.7531461761858664,0.7521781219748306,0.7512100677637947,0.7512100677637947,0.750242013552759,0.750242013552759,0.750242013552759,0.7492739593417231,0.7483059051306873,0.7473378509196515,0.7463697967086157,0.7454017424975798,0.744433688286544,0.7434656340755083,0.7434656340755083,0.7424975798644724,0.7424975798644724,0.7415295256534365,0.7405614714424008,0.739593417231365,0.7386253630203291,0.7386253630203291,0.7386253630203291,0.7376573088092934,0.7366892545982575,0.7357212003872217,0.7347531461761858,0.7337850919651501,0.7328170377541142,0.7318489835430784,0.7318489835430784,0.7308809293320426,0.7299128751210068,0.7289448209099709,0.7279767666989352,0.7279767666989352,0.7270087124878993,0.7260406582768635,0.7250726040658277,0.7241045498547919,0.723136495643756,0.7221684414327202,0.7221684414327202,0.7221684414327202,0.7221684414327202,0.7212003872216844,0.7212003872216844,0.7202323330106486,0.7192642787996127,0.718296224588577,0.7173281703775412,0.7173281703775412,0.7163601161665053,0.7153920619554696,0.7144240077444337,0.7134559535333979,0.712487899322362,0.712487899322362,0.7115198451113263,0.7105517909002904,0.7095837366892546,0.7086156824782188,0.707647628267183,0.7066795740561471,0.7057115198451114,0.7047434656340755,0.7037754114230397,0.7037754114230397,0.7028073572120038,0.7018393030009681,0.7008712487899322,0.6999031945788964,0.6989351403678606,0.6989351403678606,0.6979670861568248,0.6969990319457889,0.6960309777347532,0.6950629235237173,0.6940948693126815,0.6940948693126815,0.6931268151016456,0.6921587608906099,0.691190706679574,0.6902226524685382,0.6892545982575025,0.6882865440464666,0.6873184898354308,0.6873184898354308,0.6873184898354308,0.686350435624395,0.6853823814133592,0.6853823814133592,0.6844143272023233,0.6834462729912875,0.6824782187802517,0.6815101645692159,0.68054211035818,0.6795740561471443,0.6786060019361084,0.6786060019361084,0.6776379477250726,0.6776379477250726,0.6766698935140368,0.675701839303001,0.675701839303001,0.6747337850919651,0.6737657308809293,0.6727976766698935,0.6718296224588577,0.6708615682478218,0.6698935140367861,0.6689254598257502,0.6689254598257502,0.6679574056147144,0.6669893514036787,0.6660212971926428,0.665053242981607,0.6640851887705711,0.6631171345595354,0.6621490803484995,0.6611810261374637,0.6602129719264279,0.6592449177153921,0.6592449177153921,0.6582768635043562,0.6582768635043562,0.6573088092933205,0.6563407550822846,0.6563407550822846,0.6553727008712488,0.6544046466602129,0.6534365924491772,0.6524685382381413,0.6515004840271055,0.6505324298160697,0.6505324298160697,0.6495643756050339,0.648596321393998,0.6476282671829623,0.6466602129719264,0.6456921587608906,0.6447241045498547,0.643756050338819,0.6427879961277831,0.6418199419167473,0.6408518877057116,0.6398838334946757,0.6398838334946757,0.6389157792836399,0.6379477250726041,0.6369796708615683,0.6360116166505324,0.6360116166505324,0.6350435624394967,0.6340755082284608,0.633107454017425,0.6321393998063891,0.6311713455953534,0.6311713455953534,0.6302032913843175,0.6302032913843175,0.6302032913843175,0.6292352371732817,0.6282671829622459,0.6272991287512101,0.6263310745401742,0.6253630203291385,0.6243949661181026,0.6234269119070668,0.6224588576960309,0.6224588576960309,0.6214908034849952,0.6205227492739593,0.6205227492739593,0.6205227492739593,0.6195546950629235,0.6185866408518877,0.6176185866408519,0.616650532429816,0.6156824782187803,0.6147144240077445,0.6137463697967086,0.6127783155856728,0.611810261374637,0.6108422071636012,0.6098741529525653,0.6089060987415296,0.6079380445304937,0.6069699903194579,0.6060019361084221,0.6050338818973863,0.6040658276863504,0.6040658276863504,0.6030977734753146,0.6021297192642788,0.601161665053243,0.6001936108422071,0.5992255566311714,0.5982575024201355,0.5972894482090997,0.5963213939980639,0.5963213939980639,0.5953533397870281,0.5943852855759922,0.5934172313649564,0.5934172313649564,0.5924491771539206,0.5924491771539206,0.5914811229428848,0.590513068731849,0.5895450145208132,0.5895450145208132,0.5885769603097774,0.5876089060987415,0.5866408518877058,0.5856727976766699,0.5847047434656341,0.5837366892545982,0.5827686350435625,0.5818005808325266,0.5818005808325266,0.5808325266214908,0.579864472410455,0.5788964181994192,0.5779283639883833,0.5779283639883833,0.5769603097773476,0.5759922555663117,0.5750242013552759,0.57405614714424,0.57405614714424,0.5730880929332043,0.5721200387221684,0.5711519845111326,0.5701839303000968,0.569215876089061,0.5682478218780251,0.5682478218780251,0.5672797676669894,0.5663117134559535,0.5653436592449177,0.5643756050338818,0.5634075508228461,0.5624394966118103,0.5614714424007744,0.5605033881897387,0.558567279767667,0.558567279767667,0.5575992255566312,0.5566311713455954,0.5556631171345595,0.5546950629235237,0.5537270087124879,0.5527589545014521,0.5517909002904162,0.5508228460793805,0.5498547918683446,0.5488867376573088,0.547918683446273,0.5469506292352372,0.5459825750242013,0.5450145208131656,0.5440464666021297,0.5430784123910939,0.542110358180058,0.5411423039690223,0.5401742497579864,0.5392061955469506,0.5382381413359149,0.537270087124879,0.5363020329138432,0.5353339787028074,0.5343659244917716,0.5333978702807357,0.5324298160696999,0.5314617618586641,0.5304937076476283,0.5295256534365924,0.5285575992255567,0.5275895450145208,0.526621490803485,0.5256534365924492,0.5246853823814134,0.5237173281703775,0.5227492739593417,0.5217812197483059,0.5217812197483059,0.5208131655372701,0.5198451113262342,0.5188770571151985,0.5179090029041626,0.5169409486931268,0.515972894482091,0.5150048402710552,0.5140367860600193,0.5140367860600193,0.5130687318489835,0.5121006776379478,0.5111326234269119,0.510164569215876,0.5091965150048403,0.5082284607938045,0.5072604065827686,0.5062923523717329,0.505324298160697,0.5043562439496612,0.5033881897386253,0.5024201355275896,0.5014520813165537,0.5004840271055179,0.4995159728944821,0.4995159728944821,0.4985479186834463,0.4975798644724105,0.49661181026137463,0.49564375605033884,0.494675701839303,0.4937076476282672,0.4927395934172314,0.4927395934172314,0.49177153920619554,0.49080348499515974,0.4898354307841239,0.4888673765730881,0.4878993223620523,0.48693126815101645,0.48596321393998065,0.4849951597289448,0.484027105517909,0.4830590513068732,0.48209099709583736,0.48112294288480156,0.4801548886737657,0.4791868344627299,0.4782187802516941,0.47725072604065827,0.4762826718296225,0.4753146176185866,0.4743465634075508,0.47337850919651503,0.4724104549854792,0.4714424007744434,0.47047434656340753,0.46950629235237173,0.46853823814133594,0.4675701839303001,0.4666021297192643,0.46563407550822844,0.46466602129719264,0.46369796708615685,0.462729912875121,0.4617618586640852,0.46079380445304935,0.46079380445304935,0.45982575024201355,0.45885769603097776,0.4578896418199419,0.4569215876089061,0.45595353339787026,0.45498547918683446,0.45401742497579867,0.4530493707647628,0.452081316553727,0.45111326234269117,0.45014520813165537,0.4491771539206196,0.4482090997095837,0.44724104549854793,0.4462729912875121,0.4453049370764763,0.4443368828654405,0.44336882865440463,0.44240077444336884,0.441432720232333,0.4404646660212972,0.4394966118102614,0.43852855759922554,0.43756050338818975,0.4365924491771539,0.4356243949661181,0.4346563407550823,0.43368828654404645,0.43272023233301066,0.4317521781219748,0.430784123910939,0.4298160696999032,0.42884801548886736,0.42787996127783157,0.4269119070667957,0.4259438528557599,0.4249757986447241,0.42400774443368827,0.4230396902226525,0.4220716360116166,0.42110358180058083,0.42013552758954503,0.4191674733785092,0.4181994191674734,0.41723136495643753,0.41626331074540174,0.41529525653436594,0.4143272023233301,0.4133591481122943,0.41239109390125844,0.41142303969022265,0.41045498547918685,0.409486931268151,0.4085188770571152,0.4075508228460794,0.40658276863504356,0.40658276863504356,0.40561471442400776,0.4046466602129719,0.4036786060019361,0.4027105517909003,0.4027105517909003,0.40174249757986447,0.40077444336882867,0.3998063891577928,0.398838334946757,0.3978702807357212,0.3969022265246854,0.3959341723136496,0.39496611810261373,0.39399806389157793,0.39303000968054214,0.3920619554695063,0.3910939012584705,0.39012584704743464,0.38915779283639884,0.38818973862536305,0.3872216844143272,0.3862536302032914,0.3862536302032914,0.38528557599225555,0.38431752178121975,0.38334946757018395,0.3823814133591481,0.3814133591481123,0.38044530493707646,0.37947725072604066,0.37850919651500486,0.377541142303969,0.3765730880929332,0.37560503388189737,0.37463697967086157,0.3736689254598258,0.3727008712487899,0.3727008712487899,0.3717328170377541,0.3707647628267183,0.3697967086156825,0.3688286544046467,0.36786060019361083,0.36689254598257504,0.3659244917715392,0.3649564375605034,0.3639883833494676,0.36302032913843174,0.36205227492739595,0.3610842207163601,0.3601161665053243,0.3591481122942885,0.35818005808325265,0.35721200387221685,0.35721200387221685,0.356243949661181,0.3552758954501452,0.3543078412391094,0.3543078412391094,0.35333978702807356,0.35237173281703776,0.3514036786060019,0.3504356243949661,0.3494675701839303,0.34849951597289447,0.3475314617618587,0.3465634075508228,0.345595353339787,0.34462729912875123,0.3436592449177154,0.3426911907066796,0.34172313649564373,0.34172313649564373,0.34075508228460794,0.33978702807357214,0.3388189738625363,0.3378509196515005,0.33688286544046464,0.33591481122942884,0.33494675701839305,0.33494675701839305,0.3339787028073572,0.3330106485963214,0.33204259438528555,0.33107454017424975,0.33010648596321396,0.3291384317521781,0.3281703775411423,0.32720232333010646,0.32720232333010646,0.32623426911907066,0.32526621490803487,0.324298160696999,0.3233301064859632,0.32236205227492737,0.3213939980638916,0.3204259438528558,0.3194578896418199,0.31848983543078413,0.31752178121974833,0.3165537270087125,0.3155856727976767,0.31461761858664083,0.31364956437560504,0.31268151016456924,0.3117134559535334,0.3107454017424976,0.30977734753146174,0.30880929332042595,0.30880929332042595,0.30784123910939015,0.3068731848983543,0.3059051306873185,0.30493707647628265,0.30396902226524686,0.30300096805421106,0.3020329138431752,0.3010648596321394,0.30009680542110356,0.29912875121006777,0.29816069699903197,0.2971926427879961,0.2962245885769603,0.2962245885769603,0.2952565343659245,0.2942884801548887,0.2933204259438529,0.29235237173281703,0.29138431752178123,0.2904162633107454,0.2894482090997096,0.2884801548886738,0.28751210067763794,0.28654404646660214,0.2855759922555663,0.2846079380445305,0.2836398838334947,0.28267182962245885,0.28170377541142305,0.2807357212003872,0.2797676669893514,0.2787996127783156,0.27783155856727976,0.27686350435624396,0.2758954501452081,0.2749273959341723,0.2739593417231365,0.27299128751210067,0.27202323330106487,0.271055179090029,0.2700871248789932,0.2691190706679574,0.2681510164569216,0.2681510164569216,0.2671829622458858,0.26621490803484993,0.26524685382381413,0.26427879961277834,0.2633107454017425,0.2623426911907067,0.26137463697967084,0.26040658276863504,0.25943852855759925,0.2584704743465634,0.2575024201355276,0.25653436592449175,0.25556631171345595,0.25459825750242016,0.2536302032913843,0.2526621490803485,0.25169409486931266,0.25072604065827686,0.24975798644724104,0.24878993223620524,0.24782187802516942,0.2468538238141336,0.24588576960309777,0.24491771539206195,0.24394966118102615,0.24298160696999033,0.2420135527589545,0.24104549854791868,0.24007744433688286,0.23910939012584706,0.23814133591481124,0.2371732817037754,0.2362052274927396,0.2362052274927396,0.23523717328170377,0.23426911907066797,0.23330106485963215,0.23233301064859632,0.2313649564375605,0.23039690222652467,0.22942884801548888,0.22846079380445306,0.22749273959341723,0.2265246853823814,0.22555663117134558,0.2245885769603098,0.22362052274927396,0.22265246853823814,0.22168441432720232,0.2207163601161665,0.2197483059051307,0.21878025169409487,0.21781219748305905,0.21684414327202323,0.2158760890609874,0.2149080348499516,0.21393998063891578,0.21297192642787996,0.21200387221684414,0.2110358180058083,0.21006776379477252,0.2090997095837367,0.20813165537270087,0.20716360116166505,0.20619554695062922,0.20522749273959343,0.2042594385285576,0.20329138431752178,0.20232333010648595,0.20135527589545016,0.20038722168441434,0.1994191674733785,0.1994191674733785,0.1984511132623427,0.19748305905130686,0.19651500484027107,0.19554695062923524,0.19457889641819942,0.1936108422071636,0.19264278799612777,0.19167473378509198,0.19070667957405615,0.18973862536302033,0.1887705711519845,0.18780251694094868,0.1868344627299129,0.18586640851887706,0.18489835430784124,0.18393030009680542,0.1829622458857696,0.1819941916747338,0.18102613746369797,0.18005808325266215,0.17909002904162633,0.1781219748305905,0.1771539206195547,0.17618586640851888,0.17521781219748306,0.17521781219748306,0.17424975798644723,0.1732817037754114,0.17231364956437561,0.1713455953533398,0.17037754114230397,0.16940948693126814,0.16844143272023232,0.16747337850919652,0.1665053242981607,0.16553727008712488,0.16456921587608905,0.16360116166505323,0.16263310745401743,0.1616650532429816,0.1606969990319458,0.15972894482090996,0.15876089060987417,0.15779283639883834,0.15682478218780252,0.1558567279767667,0.15488867376573087,0.15392061955469508,0.15295256534365925,0.15198451113262343,0.1510164569215876,0.15004840271055178,0.14908034849951599,0.14811229428848016,0.14714424007744434,0.14617618586640851,0.1452081316553727,0.1442400774443369,0.14327202323330107,0.14230396902226525,0.14133591481122942,0.1403678606001936,0.1393998063891578,0.13843175217812198,0.13746369796708616,0.13649564375605033,0.1355275895450145,0.1345595353339787,0.1335914811229429,0.13262342691190707,0.13165537270087124,0.13068731848983542,0.12971926427879962,0.1287512100677638,0.12778315585672798,0.12681510164569215,0.12681510164569215,0.12584704743465633,0.12487899322362052,0.12391093901258471,0.12294288480154889,0.12197483059051308,0.12100677637947725,0.12003872216844143,0.11907066795740562,0.1181026137463698,0.11713455953533398,0.11713455953533398,0.11616650532429816,0.11519845111326234,0.11423039690222653,0.1132623426911907,0.1122942884801549,0.11132623426911907,0.11035818005808325,0.10939012584704744,0.10842207163601161,0.1074540174249758,0.10648596321393998,0.10551790900290416,0.10454985479186835,0.10358180058083252,0.10261374636979671,0.10164569215876089,0.10067763794772508,0.09970958373668926,0.09874152952565343,0.09777347531461762,0.0968054211035818,0.09583736689254599,0.09486931268151017,0.09390125847047434,0.09293320425943853,0.09196515004840271,0.0909970958373669,0.09002904162633107,0.08906098741529525,0.08809293320425944,0.08712487899322362,0.08615682478218781,0.08518877057115198,0.08518877057115198,0.08422071636011616,0.08325266214908035,0.08228460793804453,0.08131655372700872,0.0803484995159729,0.07938044530493708,0.07841239109390126,0.07744433688286544,0.07647628267182963,0.0755082284607938,0.07454017424975799,0.07357212003872217,0.07260406582768635,0.07163601161665054,0.07066795740561471,0.0696999031945789,0.06873184898354308,0.06776379477250725,0.06679574056147145,0.06582768635043562,0.06485963213939981,0.06485963213939981,0.06389157792836399,0.06292352371732816,0.061955469506292354,0.06098741529525654,0.060019361084220714,0.0590513068731849,0.05808325266214908,0.057115198451113264,0.05614714424007745,0.05517909002904162,0.05421103581800581,0.05324298160696999,0.05227492739593417,0.051306873184898356,0.05033881897386254,0.049370764762826716,0.0484027105517909,0.04743465634075508,0.046466602129719266,0.04549854791868345,0.044530493707647625,0.04356243949661181,0.04259438528557599,0.041626331074540175,0.04065827686350436,0.03969022265246854,0.03872216844143272,0.0377541142303969,0.0377541142303969,0.036786060019361085,0.03581800580832527,0.03484995159728945,0.03388189738625363,0.03291384317521781,0.031945788964181994,0.030977734753146177,0.030009680542110357,0.02904162633107454,0.028073572120038724,0.027105517909002903,0.026137463697967087,0.02516940948693127,0.02420135527589545,0.023233301064859633,0.022265246853823813,0.021297192642787996,0.02032913843175218,0.01936108422071636,0.018393030009680542,0.017424975798644726,0.016456921587608905,0.015488867376573089,0.01452081316553727,0.013552758954501452,0.012584704743465635,0.011616650532429816,0.010648596321393998,0.00968054211035818,0.008712487899322363,0.007744433688286544,0.006776379477250726,0.005808325266214908,0.00484027105517909,0.003872216844143272,0.002904162633107454,0.001936108422071636,0.000968054211035818,0],"xaxis":"x","y":[0.7341862117981521,0.7347083926031295,0.7352313167259786,0.7357549857549858,0.7362794012829651,0.7368045649072753,0.7373304782298359,0.7371428571428571,0.7376697641172266,0.7381974248927039,0.7387258410880458,0.7392550143266475,0.7397849462365591,0.7403156384505022,0.7408470926058865,0.7413793103448276,0.7419122933141624,0.7424460431654676,0.7429805615550756,0.7435158501440923,0.7440519105984138,0.7445887445887446,0.7451263537906138,0.744942196531792,0.7454808387563269,0.7460202604920405,0.7458363504706734,0.7463768115942029,0.7469180565627266,0.7474600870827286,0.7480029048656499,0.748546511627907,0.7490909090909091,0.7496360989810772,0.7501820830298617,0.750728862973761,0.75127644055434,0.7518248175182481,0.7523739956172388,0.7529239766081871,0.7534747622531089,0.7540263543191801,0.7545787545787546,0.7543988269794721,0.7549523110785032,0.7555066079295154,0.7553269654665687,0.7558823529411764,0.7564385577630611,0.7562592047128129,0.7568165070007369,0.7573746312684366,0.7579335793357933,0.758493353028065,0.7590539541759054,0.7588757396449705,0.7594374537379719,0.76,0.759822090437361,0.7603857566765578,0.7609502598366741,0.7615156017830609,0.7620817843866171,0.7626488095238095,0.763216679076694,0.763785394932936,0.7643549589858315,0.7649253731343284,0.765496639283047,0.7653213751868461,0.7658937920718025,0.7664670658682635,0.7670411985018727,0.7668665667166417,0.7666916729182296,0.7672672672672672,0.7678437265214124,0.7684210526315789,0.7689992475545523,0.7695783132530121,0.7701582516955539,0.770739064856712,0.7713207547169811,0.7719033232628398,0.7724867724867724,0.7723146747352496,0.7728993186979561,0.7727272727272727,0.7733131159969674,0.7738998482549317,0.7737281700835231,0.7743161094224924,0.7741444866920152,0.7747336377473364,0.7753236862147753,0.7759146341463414,0.7765064836003052,0.7770992366412214,0.7776928953399541,0.7782874617737003,0.7788829380260138,0.7787136294027565,0.7793103448275862,0.7799079754601227,0.7805065234075211,0.7811059907834101,0.781706379707917,0.7823076923076923,0.7829099307159353,0.7835130970724191,0.7833461835003855,0.783179012345679,0.783011583011583,0.7836166924265843,0.7842227378190255,0.7848297213622291,0.7854376452362509,0.7852713178294574,0.7851047323506595,0.7857142857142857,0.7863247863247863,0.7869362363919129,0.7875486381322957,0.7881619937694704,0.7887763055339049,0.7893915756630265,0.790007806401249,0.790625,0.7912431587177482,0.7918622848200313,0.7924823805794832,0.792319749216301,0.7929411764705883,0.792778649921507,0.793401413982718,0.7932389937106918,0.7930763178599528,0.7929133858267716,0.793538219070134,0.7933753943217665,0.7932123125493291,0.7930489731437599,0.7936758893280632,0.7943037974683544,0.7941409342834521,0.794770206022187,0.7954004758128469,0.7952380952380952,0.7950754567116759,0.7957074721780604,0.7963404932378679,0.7969745222929936,0.796812749003984,0.7966507177033493,0.7972865123703112,0.797923322683706,0.7985611510791367,0.7992,0.799839871897518,0.7996794871794872,0.7995188452285485,0.8001605136436597,0.8008032128514057,0.8014469453376206,0.8020917135961384,0.8027375201288245,0.8025785656728445,0.8032258064516129,0.8038740920096852,0.8045234248788369,0.8051738075990299,0.8058252427184466,0.8064777327935223,0.807131280388979,0.8077858880778589,0.8084415584415584,0.8090982940698619,0.8089430894308943,0.8096013018714402,0.8102605863192183,0.8101059494702526,0.8107667210440457,0.8106122448979591,0.8112745098039216,0.8111201962387572,0.8117839607201309,0.8116298116298116,0.8122950819672131,0.8129614438063987,0.812807881773399,0.8126540673788003,0.8133223684210527,0.8139917695473251,0.814662273476112,0.8153338829348722,0.816006600660066,0.8166804293971924,0.8173553719008264,0.8172043010752689,0.8178807947019867,0.8185584092792046,0.8192371475953566,0.8199170124481328,0.8197674418604651,0.8204488778054863,0.8211314475873545,0.8218151540383014,0.8216666666666667,0.8223519599666389,0.8230383973288815,0.8237259816207184,0.8235785953177257,0.8234309623430962,0.8232830820770519,0.8239731768650461,0.8246644295302014,0.8245172124265323,0.8243697478991596,0.8250630782169891,0.8257575757575758,0.8256107834877844,0.8254637436762225,0.8261603375527427,0.8260135135135135,0.8267117497886729,0.8265651438240271,0.8264182895850973,0.826271186440678,0.8261238337574215,0.8268251273344652,0.8275276125743416,0.8282312925170068,0.828936170212766,0.8296422487223168,0.8294970161977835,0.8293515358361775,0.8300597779675492,0.8307692307692308,0.8306244653550042,0.8304794520547946,0.8303341902313625,0.8310463121783876,0.8317596566523605,0.8324742268041238,0.8331900257953568,0.8339070567986231,0.8337639965546942,0.8344827586206897,0.8352027610008628,0.8359240069084629,0.8357821953327571,0.8365051903114187,0.8372294372294372,0.8370883882149047,0.836947094535993,0.8368055555555556,0.8375325803649001,0.8382608695652174,0.8389904264577894,0.8388501742160279,0.8387096774193549,0.8385689354275742,0.8393013100436681,0.840034965034965,0.8398950131233596,0.840630472854641,0.8404907975460123,0.8403508771929824,0.8410886742756805,0.8418277680140598,0.8425681618293756,0.8433098591549296,0.8431718061674008,0.8430335097001763,0.8437775816416593,0.8436395759717314,0.843501326259947,0.8433628318584071,0.8432240921169176,0.8430851063829787,0.8429458740017747,0.8436944937833037,0.8435555555555555,0.844306049822064,0.8450578806767587,0.8458110516934046,0.8465655664585192,0.8473214285714286,0.8471849865951743,0.8470483005366727,0.846911369740376,0.8467741935483871,0.8466367713004485,0.8464991023339318,0.8463611859838275,0.8462230215827338,0.8460846084608461,0.8468468468468469,0.8476104598737602,0.8474729241877257,0.8473351400180669,0.8471971066907775,0.8470588235294118,0.8469202898550725,0.8467815049864007,0.8466424682395645,0.846503178928247,0.8463636363636363,0.8462238398544131,0.8469945355191257,0.8468550592525068,0.8476277372262774,0.8484018264840183,0.8491773308957953,0.8490393412625801,0.8489010989010989,0.84967919340055,0.8495412844036697,0.8503213957759412,0.8501838235294118,0.8500459981600736,0.8499079189686924,0.8506912442396314,0.8514760147601476,0.8522622345337026,0.8530499075785583,0.8529139685476411,0.8537037037037037,0.8544949026876738,0.8552875695732839,0.8560817084493965,0.8559479553903345,0.8558139534883721,0.8566108007448789,0.8564771668219944,0.8563432835820896,0.8571428571428571,0.8570093457943925,0.8568755846585594,0.8576779026217228,0.8575445173383318,0.8574108818011257,0.8572769953051643,0.8571428571428571,0.8570084666039511,0.8568738229755178,0.8576814326107446,0.8584905660377359,0.8583569405099151,0.8582230623818525,0.859035004730369,0.8598484848484849,0.8597156398104265,0.8595825426944972,0.8603988603988604,0.8612167300380228,0.8610846812559467,0.8609523809523809,0.8617731172545281,0.8625954198473282,0.8634192932187201,0.8632887189292543,0.8631578947368421,0.8639846743295019,0.8648130393096836,0.8646833013435701,0.8655139289145053,0.8663461538461539,0.8662175168431184,0.8660886319845857,0.8659594985535197,0.8658301158301158,0.8657004830917875,0.8665377176015474,0.8664085188770572,0.8662790697674418,0.8661493695441319,0.8660194174757282,0.8658892128279884,0.8657587548638133,0.866601752677702,0.8664717348927875,0.8673170731707317,0.8671875,0.8670576735092864,0.8669275929549902,0.8667972575905974,0.8666666666666667,0.8675171736997056,0.8673870333988212,0.8682399213372665,0.8690944881889764,0.8699507389162562,0.8708086785009862,0.8716683119447186,0.8725296442687747,0.8733926805143423,0.8732673267326733,0.8741328047571854,0.8740079365079365,0.8738828202581926,0.8747514910536779,0.8756218905472637,0.8764940239043825,0.8763708873379861,0.8772455089820359,0.8771228771228772,0.878,0.8778778778778779,0.8777555110220441,0.8786359077231695,0.8785140562248996,0.878391959798995,0.8792756539235412,0.8801611278952669,0.8810483870967742,0.8809283551967709,0.8818181818181818,0.8827098078867543,0.8825910931174089,0.8834853090172239,0.8833671399594321,0.884263959390863,0.8851626016260162,0.8860630722278738,0.8869653767820774,0.8868501529051988,0.886734693877551,0.8866189989785496,0.8875255623721882,0.887410440122825,0.8872950819672131,0.8882051282051282,0.8880903490759754,0.8890030832476875,0.8888888888888888,0.8898043254376931,0.8896907216494845,0.8895768833849329,0.8894628099173554,0.890382626680455,0.8902691511387164,0.8901554404145078,0.8900414937759336,0.8899273104880582,0.8898128898128899,0.8896982310093653,0.8895833333333333,0.8905109489051095,0.8914405010438413,0.8913270637408568,0.891213389121339,0.8921465968586387,0.8920335429769392,0.8919202518363064,0.8928571428571429,0.8927444794952681,0.8926315789473684,0.8925184404636459,0.8934599156118144,0.8944033790918691,0.8942917547568711,0.8941798941798942,0.8951271186440678,0.8960763520678685,0.8970276008492569,0.8969181721572795,0.8978723404255319,0.8977635782747604,0.8976545842217484,0.8975453575240128,0.8985042735042735,0.8983957219251337,0.8982869379014989,0.8981779206859593,0.8991416309012875,0.8990332975295381,0.8989247311827957,0.898815931108719,0.8987068965517241,0.8985976267529665,0.8984881209503239,0.8994594594594595,0.8993506493506493,0.8992416034669556,0.8991323210412148,0.8990228013029316,0.9,0.9009793253536452,0.900871459694989,0.9007633587786259,0.9006550218340611,0.9016393442622951,0.9026258205689278,0.9036144578313253,0.9035087719298246,0.9034028540065862,0.9043956043956044,0.9053905390539054,0.9052863436123348,0.9051819184123484,0.9061810154525386,0.9060773480662984,0.9059734513274337,0.9058693244739756,0.9068736141906873,0.9078801331853497,0.9077777777777778,0.9076751946607341,0.9075723830734966,0.907469342251951,0.9073660714285714,0.9072625698324023,0.9071588366890381,0.9081746920492721,0.9080717488789237,0.9090909090909091,0.9089887640449438,0.9088863892013498,0.9099099099099099,0.910935738444194,0.9108352144469526,0.9107344632768362,0.9106334841628959,0.9105322763306908,0.9104308390022676,0.9103291713961408,0.9102272727272728,0.9112627986348123,0.9111617312072893,0.9110604332953249,0.9121004566210046,0.9131428571428571,0.9130434782608695,0.9129438717067583,0.9128440366972477,0.912743972445465,0.9137931034482759,0.9136939010356732,0.913594470046083,0.9134948096885813,0.9133949191685913,0.9132947976878613,0.9131944444444444,0.9142526071842411,0.91415313225058,0.9140534262485482,0.9151162790697674,0.9150174621653085,0.916083916083916,0.9159859976662778,0.9158878504672897,0.9157894736842105,0.9156908665105387,0.9155920281359906,0.9154929577464789,0.9153936545240893,0.9152941176470588,0.9151943462897526,0.9150943396226415,0.9161747343565525,0.9160756501182034,0.9171597633136095,0.9182464454976303,0.9181494661921709,0.9180522565320665,0.9179548156956004,0.9178571428571428,0.9177592371871275,0.9176610978520287,0.9175627240143369,0.9186602870813397,0.918562874251497,0.919664268585132,0.9195678271308524,0.9194711538461539,0.9193742478941035,0.9192771084337349,0.9203860072376358,0.9214975845410628,0.9214026602176542,0.9213075060532687,0.9212121212121213,0.9211165048543689,0.9210206561360875,0.9209245742092458,0.9208282582216809,0.9219512195121952,0.9218559218559218,0.921760391198044,0.9216646266829865,0.9215686274509803,0.9226993865030675,0.9226044226044227,0.922509225092251,0.9224137931034483,0.9223181257706535,0.9222222222222223,0.9221260815822002,0.9232673267326733,0.9244114002478315,0.9255583126550868,0.9254658385093167,0.9266169154228856,0.9265255292652553,0.92643391521197,0.9263420724094882,0.92625,0.9274092615769712,0.9273182957393483,0.9272271016311167,0.9271356783919598,0.9270440251572327,0.9269521410579346,0.9281210592686002,0.928030303030303,0.9279393173198482,0.9278481012658227,0.9277566539923955,0.9276649746192893,0.9275730622617535,0.9274809160305344,0.9273885350318471,0.9272959183673469,0.9284802043422733,0.928388746803069,0.9282970550576184,0.9282051282051282,0.9281129653401797,0.9280205655526992,0.9292149292149292,0.9291237113402062,0.9290322580645162,0.9289405684754521,0.9288486416558862,0.9287564766839378,0.9299610894941635,0.9298701298701298,0.929778933680104,0.9296875,0.9295958279009127,0.9295039164490861,0.9294117647058824,0.9293193717277487,0.9305373525557011,0.931758530183727,0.9316688567674113,0.9315789473684211,0.932806324110672,0.9327176781002638,0.9326287978863936,0.9325396825396826,0.9324503311258279,0.9323607427055703,0.9322709163346613,0.9321808510638298,0.933422103861518,0.9333333333333333,0.9345794392523364,0.9344919786096256,0.9344042838018741,0.935656836461126,0.9355704697986578,0.9354838709677419,0.9353970390309556,0.9353099730458221,0.9352226720647774,0.9351351351351351,0.9350473612990527,0.9363143631436315,0.9362279511533242,0.936141304347826,0.9360544217687075,0.9359673024523161,0.9358799454297408,0.9357923497267759,0.9357045143638851,0.9356164383561644,0.9355281207133059,0.9354395604395604,0.936726272352132,0.9366391184573003,0.9379310344827586,0.9378453038674033,0.9377593360995851,0.9390581717451524,0.9389736477115118,0.9388888888888889,0.9388038942976356,0.9387186629526463,0.9386331938633193,0.9385474860335196,0.9398601398601398,0.9397759103641457,0.9396914446002805,0.9396067415730337,0.939521800281294,0.9394366197183098,0.9393511988716502,0.9392655367231638,0.9391796322489392,0.9390934844192634,0.9390070921985816,0.9389204545454546,0.9402560455192034,0.9401709401709402,0.9400855920114123,0.94,0.9399141630901288,0.9412607449856734,0.9411764705882353,0.9410919540229885,0.9410071942446043,0.9409221902017291,0.9408369408369408,0.9421965317919075,0.9421128798842258,0.9434782608695652,0.9448476052249637,0.9447674418604651,0.9446870451237264,0.9446064139941691,0.9445255474452555,0.9444444444444444,0.9443631039531479,0.9442815249266863,0.9441997063142438,0.9455882352941176,0.9455081001472754,0.9454277286135693,0.946824224519941,0.9482248520710059,0.9481481481481482,0.9480712166172107,0.9479940564635958,0.9479166666666666,0.9478390461997019,0.9477611940298507,0.9476831091180867,0.9476047904191617,0.9475262368815592,0.9474474474474475,0.9473684210526315,0.947289156626506,0.947209653092006,0.947129909365559,0.9470499243570348,0.946969696969697,0.9468892261001517,0.9483282674772037,0.9482496194824962,0.948170731707317,0.9480916030534351,0.9480122324159022,0.9479326186830015,0.9478527607361963,0.9477726574500768,0.9476923076923077,0.9491525423728814,0.9490740740740741,0.9489953632148377,0.9489164086687306,0.9503875968992248,0.9503105590062112,0.9517884914463453,0.9517133956386293,0.9516380655226209,0.9515625,0.9530516431924883,0.9529780564263323,0.9529042386185244,0.9528301886792453,0.952755905511811,0.9526813880126183,0.95260663507109,0.9525316455696202,0.9524564183835182,0.953968253968254,0.9538950715421304,0.9538216560509554,0.9537480063795853,0.9536741214057508,0.9552,0.9551282051282052,0.9550561797752809,0.954983922829582,0.9549114331723028,0.9564516129032258,0.9563812600969306,0.9563106796116505,0.9562398703403565,0.9561688311688312,0.9560975609756097,0.9560260586319218,0.9575856443719413,0.9575163398692811,0.9574468085106383,0.9573770491803278,0.9573070607553367,0.9572368421052632,0.957166392092257,0.9570957095709571,0.9570247933884297,0.956882255389718,0.9584717607973422,0.9584026622296173,0.9583333333333334,0.9582637729549248,0.9581939799331104,0.9581239530988275,0.9580536912751678,0.957983193277311,0.9579124579124579,0.9578414839797639,0.9577702702702703,0.9576988155668359,0.9576271186440678,0.9575551782682513,0.9574829931972789,0.9574105621805792,0.9573378839590444,0.9572649572649573,0.9571917808219178,0.9571183533447685,0.9570446735395189,0.9569707401032702,0.9568965517241379,0.9568221070811744,0.9567474048442907,0.9566724436741768,0.9565972222222222,0.9565217391304348,0.9564459930313589,0.956369982547993,0.9562937062937062,0.9562171628721541,0.956140350877193,0.9560632688927944,0.9559859154929577,0.9559082892416225,0.9558303886925795,0.9557522123893806,0.9556737588652482,0.9573712255772646,0.9572953736654805,0.9572192513368984,0.9571428571428572,0.9570661896243292,0.956989247311828,0.9569120287253142,0.9568345323741008,0.9567567567567568,0.9584837545126353,0.9584086799276673,0.9583333333333334,0.9582577132486388,0.9581818181818181,0.9581056466302368,0.958029197080292,0.9579524680073126,0.9578754578754579,0.9577981651376147,0.9577205882352942,0.9576427255985267,0.9575645756457565,0.9574861367837338,0.9574074074074074,0.9573283858998145,0.9591078066914498,0.9590316573556797,0.9589552238805971,0.9588785046728971,0.9588014981273408,0.9587242026266416,0.9586466165413534,0.9585687382297552,0.960377358490566,0.9603024574669187,0.9602272727272727,0.9601518026565465,0.9600760456273765,0.96,0.9599236641221374,0.9598470363288719,0.9597701149425287,0.9596928982725528,0.9596153846153846,0.9595375722543352,0.9594594594594594,0.9593810444874274,0.9593023255813954,0.9592233009708738,0.9591439688715954,0.9590643274853801,0.958984375,0.958904109589041,0.9588235294117647,0.9587426326129665,0.9586614173228346,0.9585798816568047,0.958498023715415,0.9584158415841584,0.9583333333333334,0.9582504970178927,0.9581673306772909,0.9580838323353293,0.958,0.9579158316633266,0.9578313253012049,0.9577464788732394,0.9596774193548387,0.9595959595959596,0.9595141700404858,0.9594320486815415,0.959349593495935,0.9592668024439919,0.9591836734693877,0.9591002044989775,0.9590163934426229,0.9589322381930184,0.9588477366255144,0.9587628865979382,0.9586776859504132,0.9585921325051759,0.9585062240663901,0.9584199584199584,0.9583333333333334,0.9582463465553236,0.9581589958158996,0.9580712788259959,0.957983193277311,0.9578947368421052,0.9578059071729957,0.9577167019027484,0.9576271186440678,0.9575371549893843,0.9574468085106383,0.9573560767590619,0.9572649572649573,0.9571734475374732,0.9570815450643777,0.956989247311828,0.9568965517241379,0.9568034557235421,0.9567099567099567,0.9566160520607375,0.9565217391304348,0.9564270152505446,0.9563318777292577,0.9562363238512035,0.956140350877193,0.9560439560439561,0.9559471365638766,0.9558498896247241,0.9557522123893806,0.9556541019955654,0.9555555555555556,0.955456570155902,0.9553571428571429,0.9552572706935123,0.9551569506726457,0.9550561797752809,0.954954954954955,0.9548532731376975,0.9547511312217195,0.9546485260770975,0.9545454545454546,0.9567198177676538,0.95662100456621,0.9565217391304348,0.9564220183486238,0.9563218390804598,0.9585253456221198,0.9584295612009238,0.9583333333333334,0.9582366589327146,0.958139534883721,0.958041958041958,0.9579439252336449,0.9578454332552693,0.9577464788732394,0.9576470588235294,0.9575471698113207,0.9574468085106383,0.957345971563981,0.9572446555819477,0.9571428571428572,0.9570405727923628,0.9569377990430622,0.9568345323741008,0.9591346153846154,0.9590361445783132,0.9589371980676329,0.9588377723970944,0.9587378640776699,0.9586374695863747,0.9585365853658536,0.9584352078239609,0.9583333333333334,0.9582309582309583,0.958128078817734,0.9580246913580247,0.9579207920792079,0.9578163771712159,0.9577114427860697,0.9600997506234414,0.96,0.9598997493734336,0.9597989949748744,0.9596977329974811,0.9595959595959596,0.959493670886076,0.9593908629441624,0.9592875318066157,0.9591836734693877,0.959079283887468,0.958974358974359,0.9588688946015425,0.9587628865979382,0.958656330749354,0.9585492227979274,0.9584415584415584,0.9609375,0.9608355091383812,0.9607329842931938,0.9606299212598425,0.9631578947368421,0.9630606860158312,0.9629629629629629,0.9628647214854111,0.9627659574468085,0.9626666666666667,0.9625668449197861,0.9624664879356568,0.9623655913978495,0.9622641509433962,0.9621621621621622,0.962059620596206,0.9619565217391305,0.9618528610354223,0.9644808743169399,0.9643835616438357,0.9642857142857143,0.9641873278236914,0.9640883977900553,0.96398891966759,0.9638888888888889,0.9637883008356546,0.9664804469273743,0.9663865546218487,0.9662921348314607,0.9661971830985916,0.9661016949152542,0.9660056657223796,0.9659090909090909,0.9658119658119658,0.9657142857142857,0.9684813753581661,0.9683908045977011,0.968299711815562,0.9682080924855492,0.9681159420289855,0.9680232558139535,0.967930029154519,0.9678362573099415,0.967741935483871,0.9676470588235294,0.967551622418879,0.9674556213017751,0.9673590504451038,0.9672619047619048,0.9671641791044776,0.9670658682634731,0.9669669669669669,0.9668674698795181,0.9667673716012085,0.9666666666666667,0.9696048632218845,0.9695121951219512,0.9694189602446484,0.9693251533742331,0.9692307692307692,0.9691358024691358,0.9690402476780186,0.968944099378882,0.9688473520249221,0.96875,0.9686520376175548,0.9685534591194969,0.9684542586750788,0.9683544303797469,0.9714285714285714,0.9713375796178344,0.9712460063897763,0.9711538461538461,0.9710610932475884,0.9709677419354839,0.970873786407767,0.9707792207792207,0.9706840390879479,0.9705882352941176,0.9704918032786886,0.9703947368421053,0.9702970297029703,0.9701986754966887,0.9700996677740864,0.97,0.9698996655518395,0.9697986577181208,0.9696969696969697,0.9695945945945946,0.9694915254237289,0.9693877551020408,0.9692832764505119,0.9691780821917808,0.9690721649484536,0.9689655172413794,0.9688581314878892,0.96875,0.9686411149825784,0.9685314685314685,0.9719298245614035,0.971830985915493,0.9717314487632509,0.9716312056737588,0.9715302491103203,0.9714285714285714,0.9713261648745519,0.9712230215827338,0.9711191335740073,0.9710144927536232,0.9709090909090909,0.9708029197080292,0.9706959706959707,0.9705882352941176,0.9704797047970479,0.9703703703703703,0.9702602230483272,0.9701492537313433,0.9700374531835206,0.9699248120300752,0.969811320754717,0.9696969696969697,0.9695817490494296,0.9694656488549618,0.9693486590038314,0.9692307692307692,0.9691119691119691,0.9689922480620154,0.9688715953307393,0.96875,0.9686274509803922,0.968503937007874,0.9683794466403162,0.9682539682539683,0.9721115537848606,0.972,0.9718875502008032,0.9717741935483871,0.97165991902834,0.9715447154471545,0.9714285714285714,0.9713114754098361,0.9711934156378601,0.9710743801652892,0.970954356846473,0.9708333333333333,0.9707112970711297,0.9705882352941176,0.9704641350210971,0.9703389830508474,0.9702127659574468,0.9700854700854701,0.9699570815450643,0.9698275862068966,0.9696969696969697,0.9695652173913043,0.9694323144104804,0.9692982456140351,0.9691629955947136,0.9690265486725663,0.9688888888888889,0.96875,0.968609865470852,0.9684684684684685,0.9683257918552036,0.9681818181818181,0.9680365296803652,0.9678899082568807,0.967741935483871,0.9675925925925926,0.9674418604651163,0.9672897196261683,0.9671361502347418,0.9716981132075472,0.9715639810426541,0.9714285714285714,0.9712918660287081,0.9711538461538461,0.9710144927536232,0.970873786407767,0.9707317073170731,0.9705882352941176,0.9704433497536946,0.9702970297029703,0.9701492537313433,0.97,0.9698492462311558,0.9696969696969697,0.9695431472081218,0.9693877551020408,0.9692307692307692,0.9690721649484536,0.9689119170984456,0.96875,0.9685863874345549,0.968421052631579,0.9682539682539683,0.9680851063829787,0.9679144385026738,0.9731182795698925,0.972972972972973,0.9728260869565217,0.9726775956284153,0.9725274725274725,0.9723756906077348,0.9722222222222222,0.9720670391061452,0.9719101123595506,0.9717514124293786,0.9715909090909091,0.9714285714285714,0.9712643678160919,0.9710982658959537,0.9709302325581395,0.9707602339181286,0.9705882352941176,0.9704142011834319,0.9702380952380952,0.9700598802395209,0.9698795180722891,0.9696969696969697,0.9695121951219512,0.9693251533742331,0.9691358024691358,0.968944099378882,0.96875,0.9685534591194969,0.9683544303797469,0.9681528662420382,0.967948717948718,0.967741935483871,0.9675324675324676,0.9673202614379085,0.9671052631578947,0.9668874172185431,0.9666666666666667,0.9664429530201343,0.9662162162162162,0.9659863945578231,0.9657534246575342,0.9655172413793104,0.9652777777777778,0.965034965034965,0.9647887323943662,0.9645390070921985,0.9642857142857143,0.9640287769784173,0.9637681159420289,0.9635036496350365,0.9632352941176471,0.9703703703703703,0.9701492537313433,0.9699248120300752,0.9696969696969697,0.9694656488549618,0.9692307692307692,0.9689922480620154,0.96875,0.968503937007874,0.9682539682539683,0.968,0.9758064516129032,0.975609756097561,0.9754098360655737,0.9752066115702479,0.975,0.9747899159663865,0.9745762711864406,0.9743589743589743,0.9741379310344828,0.9739130434782609,0.9736842105263158,0.9734513274336283,0.9732142857142857,0.972972972972973,0.9727272727272728,0.9724770642201835,0.9722222222222222,0.9719626168224299,0.9716981132075472,0.9714285714285714,0.9711538461538461,0.970873786407767,0.9705882352941176,0.9702970297029703,0.97,0.9696969696969697,0.9693877551020408,0.9690721649484536,0.96875,0.968421052631579,0.9680851063829787,0.967741935483871,0.967391304347826,0.967032967032967,0.9777777777777777,0.9775280898876404,0.9772727272727273,0.9770114942528736,0.9767441860465116,0.9764705882352941,0.9761904761904762,0.9759036144578314,0.975609756097561,0.9753086419753086,0.975,0.9746835443037974,0.9743589743589743,0.974025974025974,0.9736842105263158,0.9733333333333334,0.972972972972973,0.9726027397260274,0.9722222222222222,0.971830985915493,0.9714285714285714,0.9710144927536232,0.9852941176470589,0.9850746268656716,0.9848484848484849,0.9846153846153847,0.984375,0.9841269841269841,0.9838709677419355,0.9836065573770492,0.9833333333333333,0.9830508474576272,0.9827586206896551,0.9824561403508771,0.9821428571428571,0.9818181818181818,0.9814814814814815,0.9811320754716981,0.9807692307692307,0.9803921568627451,0.98,0.9795918367346939,0.9791666666666666,0.9787234042553191,0.9782608695652174,0.9777777777777777,0.9772727272727273,0.9767441860465116,0.9761904761904762,0.975609756097561,0.975,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1],"yaxis":"y"}],"layout":{"height":600,"legend":{"tracegroupgap":0},"shapes":[{"line":{"dash":"dash"},"type":"line","x0":0,"x1":1,"y0":1,"y1":0}],"showlegend":false,"template":{"data":{"bar":[{"error_x":{"color":"#2a3f5f"},"error_y":{"color":"#2a3f5f"},"marker":{"line":{"color":"white","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"barpolar":[{"marker":{"line":{"color":"white","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"carpet":[{"aaxis":{"endlinecolor":"#2a3f5f","gridcolor":"#C8D4E3","linecolor":"#C8D4E3","minorgridcolor":"#C8D4E3","startlinecolor":"#2a3f5f"},"baxis":{"endlinecolor":"#2a3f5f","gridcolor":"#C8D4E3","linecolor":"#C8D4E3","minorgridcolor":"#C8D4E3","startlinecolor":"#2a3f5f"},"type":"carpet"}],"choropleth":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"choropleth"}],"contour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"contour"}],"contourcarpet":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"contourcarpet"}],"heatmap":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmap"}],"heatmapgl":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmapgl"}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"histogram2d":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2d"}],"histogram2dcontour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2dcontour"}],"mesh3d":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"mesh3d"}],"parcoords":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"parcoords"}],"pie":[{"automargin":true,"type":"pie"}],"scatter":[{"fillpattern":{"fillmode":"overlay","size":10,"solidity":0.2},"type":"scatter"}],"scatter3d":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter3d"}],"scattercarpet":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattercarpet"}],"scattergeo":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergeo"}],"scattergl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergl"}],"scattermapbox":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattermapbox"}],"scatterpolar":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolar"}],"scatterpolargl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolargl"}],"scatterternary":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterternary"}],"surface":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"surface"}],"table":[{"cells":{"fill":{"color":"#EBF0F8"},"line":{"color":"white"}},"header":{"fill":{"color":"#C8D4E3"},"line":{"color":"white"}},"type":"table"}]},"layout":{"annotationdefaults":{"arrowcolor":"#2a3f5f","arrowhead":0,"arrowwidth":1},"autotypenumbers":"strict","coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]],"sequential":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"sequentialminus":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]]},"colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#2a3f5f"},"geo":{"bgcolor":"white","lakecolor":"white","landcolor":"white","showlakes":true,"showland":true,"subunitcolor":"#C8D4E3"},"hoverlabel":{"align":"left"},"hovermode":"closest","mapbox":{"style":"light"},"paper_bgcolor":"white","plot_bgcolor":"white","polar":{"angularaxis":{"gridcolor":"#EBF0F8","linecolor":"#EBF0F8","ticks":""},"bgcolor":"white","radialaxis":{"gridcolor":"#EBF0F8","linecolor":"#EBF0F8","ticks":""}},"scene":{"xaxis":{"backgroundcolor":"white","gridcolor":"#DFE8F3","gridwidth":2,"linecolor":"#EBF0F8","showbackground":true,"ticks":"","zerolinecolor":"#EBF0F8"},"yaxis":{"backgroundcolor":"white","gridcolor":"#DFE8F3","gridwidth":2,"linecolor":"#EBF0F8","showbackground":true,"ticks":"","zerolinecolor":"#EBF0F8"},"zaxis":{"backgroundcolor":"white","gridcolor":"#DFE8F3","gridwidth":2,"linecolor":"#EBF0F8","showbackground":true,"ticks":"","zerolinecolor":"#EBF0F8"}},"shapedefaults":{"line":{"color":"#2a3f5f"}},"ternary":{"aaxis":{"gridcolor":"#DFE8F3","linecolor":"#A2B1C6","ticks":""},"baxis":{"gridcolor":"#DFE8F3","linecolor":"#A2B1C6","ticks":""},"bgcolor":"white","caxis":{"gridcolor":"#DFE8F3","linecolor":"#A2B1C6","ticks":""}},"title":{"x":0.05},"xaxis":{"automargin":true,"gridcolor":"#EBF0F8","linecolor":"#EBF0F8","ticks":"","title":{"standoff":15},"zerolinecolor":"#EBF0F8","zerolinewidth":2},"yaxis":{"automargin":true,"gridcolor":"#EBF0F8","linecolor":"#EBF0F8","ticks":"","title":{"standoff":15},"zerolinecolor":"#EBF0F8","zerolinewidth":2}}},"title":{"text":"Precision-Recall Curve (AUC=0.8558)"},"width":800,"xaxis":{"anchor":"y","constrain":"domain","domain":[0,1],"title":{"text":"Recall"}},"yaxis":{"anchor":"x","domain":[0,1],"scaleanchor":"x","scaleratio":1,"title":{"text":"Precision"}}}},"text/html":["<div>                            <div id=\"1e149bf0-bc6a-4179-a047-3b046fd1f79c\" class=\"plotly-graph-div\" style=\"height:600px; width:800px;\"></div>            <script type=\"text/javascript\">                require([\"plotly\"], function(Plotly) {                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"1e149bf0-bc6a-4179-a047-3b046fd1f79c\")) {                    Plotly.newPlot(                        \"1e149bf0-bc6a-4179-a047-3b046fd1f79c\",                        [{\"fillpattern\":{\"shape\":\"\"},\"hovertemplate\":\"Recall=%{x}\\u003cbr\\u003ePrecision=%{y}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"\",\"line\":{\"color\":\"#636efa\"},\"marker\":{\"symbol\":\"circle\"},\"mode\":\"lines\",\"name\":\"\",\"orientation\":\"v\",\"showlegend\":false,\"stackgroup\":\"1\",\"x\":[1.0,1.0,1.0,1.0,1.0,1.0,1.0,0.9990319457889641,0.9990319457889641,0.9990319457889641,0.9990319457889641,0.9990319457889641,0.9990319457889641,0.9990319457889641,0.9990319457889641,0.9990319457889641,0.9990319457889641,0.9990319457889641,0.9990319457889641,0.9990319457889641,0.9990319457889641,0.9990319457889641,0.9990319457889641,0.9980638915779284,0.9980638915779284,0.9980638915779284,0.9970958373668926,0.9970958373668926,0.9970958373668926,0.9970958373668926,0.9970958373668926,0.9970958373668926,0.9970958373668926,0.9970958373668926,0.9970958373668926,0.9970958373668926,0.9970958373668926,0.9970958373668926,0.9970958373668926,0.9970958373668926,0.9970958373668926,0.9970958373668926,0.9970958373668926,0.9961277831558567,0.9961277831558567,0.9961277831558567,0.995159728944821,0.995159728944821,0.995159728944821,0.9941916747337851,0.9941916747337851,0.9941916747337851,0.9941916747337851,0.9941916747337851,0.9941916747337851,0.9932236205227493,0.9932236205227493,0.9932236205227493,0.9922555663117134,0.9922555663117134,0.9922555663117134,0.9922555663117134,0.9922555663117134,0.9922555663117134,0.9922555663117134,0.9922555663117134,0.9922555663117134,0.9922555663117134,0.9922555663117134,0.9912875121006777,0.9912875121006777,0.9912875121006777,0.9912875121006777,0.9903194578896418,0.989351403678606,0.989351403678606,0.989351403678606,0.989351403678606,0.989351403678606,0.989351403678606,0.989351403678606,0.989351403678606,0.989351403678606,0.989351403678606,0.989351403678606,0.9883833494675702,0.9883833494675702,0.9874152952565344,0.9874152952565344,0.9874152952565344,0.9864472410454985,0.9864472410454985,0.9854791868344628,0.9854791868344628,0.9854791868344628,0.9854791868344628,0.9854791868344628,0.9854791868344628,0.9854791868344628,0.9854791868344628,0.9854791868344628,0.9845111326234269,0.9845111326234269,0.9845111326234269,0.9845111326234269,0.9845111326234269,0.9845111326234269,0.9845111326234269,0.9845111326234269,0.9845111326234269,0.9835430784123911,0.9825750242013552,0.9816069699903195,0.9816069699903195,0.9816069699903195,0.9816069699903195,0.9816069699903195,0.9806389157792836,0.9796708615682478,0.9796708615682478,0.9796708615682478,0.9796708615682478,0.9796708615682478,0.9796708615682478,0.9796708615682478,0.9796708615682478,0.9796708615682478,0.9796708615682478,0.9796708615682478,0.9796708615682478,0.9796708615682478,0.978702807357212,0.978702807357212,0.9777347531461762,0.9777347531461762,0.9767666989351403,0.9757986447241046,0.9748305905130688,0.9748305905130688,0.9738625363020329,0.972894482090997,0.9719264278799613,0.9719264278799613,0.9719264278799613,0.9709583736689255,0.9709583736689255,0.9709583736689255,0.9699903194578896,0.9690222652468539,0.9690222652468539,0.9690222652468539,0.9690222652468539,0.968054211035818,0.9670861568247822,0.9670861568247822,0.9670861568247822,0.9670861568247822,0.9670861568247822,0.9670861568247822,0.9661181026137464,0.9651500484027106,0.9651500484027106,0.9651500484027106,0.9651500484027106,0.9651500484027106,0.9651500484027106,0.9641819941916747,0.9641819941916747,0.9641819941916747,0.9641819941916747,0.9641819941916747,0.9641819941916747,0.9641819941916747,0.9641819941916747,0.9641819941916747,0.9641819941916747,0.9641819941916747,0.9632139399806389,0.9632139399806389,0.9632139399806389,0.9622458857696031,0.9622458857696031,0.9612778315585673,0.9612778315585673,0.9603097773475314,0.9603097773475314,0.9593417231364957,0.9593417231364957,0.9593417231364957,0.9583736689254598,0.957405614714424,0.957405614714424,0.957405614714424,0.957405614714424,0.957405614714424,0.957405614714424,0.957405614714424,0.957405614714424,0.9564375605033882,0.9564375605033882,0.9564375605033882,0.9564375605033882,0.9564375605033882,0.9554695062923524,0.9554695062923524,0.9554695062923524,0.9554695062923524,0.9545014520813165,0.9545014520813165,0.9545014520813165,0.9545014520813165,0.9535333978702807,0.952565343659245,0.9515972894482091,0.9515972894482091,0.9515972894482091,0.9506292352371732,0.9496611810261375,0.9496611810261375,0.9496611810261375,0.9486931268151017,0.9477250726040658,0.9477250726040658,0.9467570183930301,0.9467570183930301,0.9457889641819942,0.9448209099709584,0.9438528557599225,0.9428848015488868,0.9428848015488868,0.9428848015488868,0.9428848015488868,0.9428848015488868,0.9428848015488868,0.9419167473378509,0.9409486931268151,0.9409486931268151,0.9409486931268151,0.9399806389157793,0.9390125847047435,0.9380445304937076,0.9380445304937076,0.9380445304937076,0.9380445304937076,0.9380445304937076,0.9380445304937076,0.9370764762826719,0.9370764762826719,0.9370764762826719,0.9370764762826719,0.936108422071636,0.936108422071636,0.936108422071636,0.9351403678606002,0.9341723136495643,0.9332042594385286,0.9332042594385286,0.9332042594385286,0.9332042594385286,0.9322362052274927,0.9312681510164569,0.9303000968054211,0.9303000968054211,0.9303000968054211,0.9293320425943853,0.9293320425943853,0.9283639883833494,0.9273959341723137,0.9273959341723137,0.9273959341723137,0.9273959341723137,0.9273959341723137,0.9264278799612778,0.925459825750242,0.925459825750242,0.9244917715392061,0.9235237173281704,0.9225556631171346,0.9215876089060987,0.920619554695063,0.9196515004840271,0.9196515004840271,0.9186834462729913,0.9186834462729913,0.9186834462729913,0.9186834462729913,0.9186834462729913,0.9186834462729913,0.9177153920619555,0.9167473378509197,0.9157792836398838,0.914811229428848,0.9138431752178122,0.9128751210067764,0.9119070667957405,0.9109390125847048,0.9099709583736689,0.9099709583736689,0.9099709583736689,0.9090029041626331,0.9080348499515973,0.9070667957405615,0.9060987415295256,0.9051306873184899,0.904162633107454,0.9031945788964182,0.9022265246853823,0.9012584704743466,0.9002904162633107,0.9002904162633107,0.8993223620522749,0.8993223620522749,0.8993223620522749,0.8993223620522749,0.8983543078412392,0.8973862536302033,0.8973862536302033,0.8964181994191674,0.8964181994191674,0.8954501452081317,0.8944820909970959,0.89351403678606,0.89351403678606,0.89351403678606,0.89351403678606,0.89351403678606,0.8925459825750242,0.8925459825750242,0.8925459825750242,0.8925459825750242,0.8925459825750242,0.8915779283639884,0.8906098741529526,0.8906098741529526,0.8896418199419167,0.888673765730881,0.888673765730881,0.8877057115198451,0.8867376573088093,0.8867376573088093,0.8857696030977735,0.8848015488867377,0.8838334946757018,0.882865440464666,0.8818973862536302,0.8809293320425944,0.8809293320425944,0.8809293320425944,0.8799612778315585,0.8789932236205228,0.8789932236205228,0.8789932236205228,0.8780251694094869,0.8770571151984511,0.8770571151984511,0.8770571151984511,0.8760890609874153,0.8751210067763795,0.8751210067763795,0.8751210067763795,0.8751210067763795,0.8741529525653436,0.8731848983543078,0.8731848983543078,0.8731848983543078,0.872216844143272,0.872216844143272,0.872216844143272,0.8712487899322362,0.8702807357212003,0.8693126815101646,0.8683446272991288,0.8673765730880929,0.8673765730880929,0.8664085188770572,0.8654404646660213,0.8644724104549855,0.8635043562439496,0.8625363020329139,0.861568247821878,0.861568247821878,0.8606001936108422,0.8606001936108422,0.8596321393998064,0.8586640851887706,0.8576960309777347,0.856727976766699,0.8557599225556631,0.8557599225556631,0.8547918683446273,0.8547918683446273,0.8547918683446273,0.8547918683446273,0.8547918683446273,0.8547918683446273,0.8547918683446273,0.8547918683446273,0.8538238141335914,0.8538238141335914,0.8528557599225557,0.8518877057115198,0.8518877057115198,0.8518877057115198,0.8518877057115198,0.850919651500484,0.850919651500484,0.8499515972894482,0.8499515972894482,0.8489835430784124,0.8480154888673765,0.8480154888673765,0.8470474346563408,0.846079380445305,0.846079380445305,0.846079380445305,0.846079380445305,0.8451113262342691,0.8451113262342691,0.8451113262342691,0.8441432720232332,0.8441432720232332,0.8431752178121975,0.8431752178121975,0.8431752178121975,0.8431752178121975,0.8431752178121975,0.8422071636011617,0.8412391093901258,0.8402710551790901,0.8402710551790901,0.8393030009680542,0.8383349467570184,0.8383349467570184,0.8373668925459826,0.8373668925459826,0.8363988383349468,0.8363988383349468,0.8354307841239109,0.8344627299128751,0.8334946757018393,0.8334946757018393,0.8325266214908035,0.8315585672797676,0.8305905130687319,0.829622458857696,0.8286544046466602,0.8276863504356244,0.8267182962245886,0.8267182962245886,0.8267182962245886,0.8257502420135527,0.8247821878025169,0.8247821878025169,0.8238141335914811,0.8228460793804453,0.8228460793804453,0.8218780251694094,0.8209099709583737,0.8199419167473379,0.8199419167473379,0.8199419167473379,0.818973862536302,0.8180058083252663,0.8180058083252663,0.8180058083252663,0.8180058083252663,0.8170377541142304,0.8170377541142304,0.8160696999031946,0.8151016456921588,0.814133591481123,0.814133591481123,0.8131655372700871,0.8121974830590513,0.8112294288480155,0.8112294288480155,0.8102613746369797,0.8092933204259438,0.8083252662149081,0.8073572120038722,0.8063891577928364,0.8054211035818006,0.8054211035818006,0.8044530493707648,0.8034849951597289,0.8025169409486931,0.8015488867376573,0.8015488867376573,0.8015488867376573,0.8005808325266215,0.7996127783155856,0.7986447241045499,0.7986447241045499,0.7986447241045499,0.7986447241045499,0.797676669893514,0.7967086156824782,0.7967086156824782,0.7967086156824782,0.7957405614714425,0.7947725072604066,0.7947725072604066,0.7938044530493708,0.7928363988383349,0.7918683446272992,0.7918683446272992,0.7918683446272992,0.7909002904162633,0.7899322362052275,0.7889641819941917,0.7879961277831559,0.78702807357212,0.7860600193610843,0.7850919651500484,0.7850919651500484,0.7841239109390126,0.7841239109390126,0.7831558567279767,0.782187802516941,0.782187802516941,0.782187802516941,0.7812197483059051,0.7802516940948693,0.7792836398838335,0.7783155856727977,0.7773475314617618,0.7763794772507261,0.7754114230396902,0.7754114230396902,0.7744433688286544,0.7734753146176185,0.7734753146176185,0.7734753146176185,0.7725072604065828,0.771539206195547,0.7705711519845111,0.7696030977734754,0.7696030977734754,0.7686350435624395,0.7676669893514037,0.7666989351403679,0.7657308809293321,0.7647628267182962,0.7637947725072604,0.7637947725072604,0.7628267182962246,0.7618586640851888,0.7618586640851888,0.7608906098741529,0.7608906098741529,0.7599225556631172,0.7589545014520813,0.7579864472410455,0.7570183930300097,0.7560503388189739,0.755082284607938,0.7541142303969022,0.7531461761858664,0.7521781219748306,0.7512100677637947,0.7512100677637947,0.750242013552759,0.750242013552759,0.750242013552759,0.7492739593417231,0.7483059051306873,0.7473378509196515,0.7463697967086157,0.7454017424975798,0.744433688286544,0.7434656340755083,0.7434656340755083,0.7424975798644724,0.7424975798644724,0.7415295256534365,0.7405614714424008,0.739593417231365,0.7386253630203291,0.7386253630203291,0.7386253630203291,0.7376573088092934,0.7366892545982575,0.7357212003872217,0.7347531461761858,0.7337850919651501,0.7328170377541142,0.7318489835430784,0.7318489835430784,0.7308809293320426,0.7299128751210068,0.7289448209099709,0.7279767666989352,0.7279767666989352,0.7270087124878993,0.7260406582768635,0.7250726040658277,0.7241045498547919,0.723136495643756,0.7221684414327202,0.7221684414327202,0.7221684414327202,0.7221684414327202,0.7212003872216844,0.7212003872216844,0.7202323330106486,0.7192642787996127,0.718296224588577,0.7173281703775412,0.7173281703775412,0.7163601161665053,0.7153920619554696,0.7144240077444337,0.7134559535333979,0.712487899322362,0.712487899322362,0.7115198451113263,0.7105517909002904,0.7095837366892546,0.7086156824782188,0.707647628267183,0.7066795740561471,0.7057115198451114,0.7047434656340755,0.7037754114230397,0.7037754114230397,0.7028073572120038,0.7018393030009681,0.7008712487899322,0.6999031945788964,0.6989351403678606,0.6989351403678606,0.6979670861568248,0.6969990319457889,0.6960309777347532,0.6950629235237173,0.6940948693126815,0.6940948693126815,0.6931268151016456,0.6921587608906099,0.691190706679574,0.6902226524685382,0.6892545982575025,0.6882865440464666,0.6873184898354308,0.6873184898354308,0.6873184898354308,0.686350435624395,0.6853823814133592,0.6853823814133592,0.6844143272023233,0.6834462729912875,0.6824782187802517,0.6815101645692159,0.68054211035818,0.6795740561471443,0.6786060019361084,0.6786060019361084,0.6776379477250726,0.6776379477250726,0.6766698935140368,0.675701839303001,0.675701839303001,0.6747337850919651,0.6737657308809293,0.6727976766698935,0.6718296224588577,0.6708615682478218,0.6698935140367861,0.6689254598257502,0.6689254598257502,0.6679574056147144,0.6669893514036787,0.6660212971926428,0.665053242981607,0.6640851887705711,0.6631171345595354,0.6621490803484995,0.6611810261374637,0.6602129719264279,0.6592449177153921,0.6592449177153921,0.6582768635043562,0.6582768635043562,0.6573088092933205,0.6563407550822846,0.6563407550822846,0.6553727008712488,0.6544046466602129,0.6534365924491772,0.6524685382381413,0.6515004840271055,0.6505324298160697,0.6505324298160697,0.6495643756050339,0.648596321393998,0.6476282671829623,0.6466602129719264,0.6456921587608906,0.6447241045498547,0.643756050338819,0.6427879961277831,0.6418199419167473,0.6408518877057116,0.6398838334946757,0.6398838334946757,0.6389157792836399,0.6379477250726041,0.6369796708615683,0.6360116166505324,0.6360116166505324,0.6350435624394967,0.6340755082284608,0.633107454017425,0.6321393998063891,0.6311713455953534,0.6311713455953534,0.6302032913843175,0.6302032913843175,0.6302032913843175,0.6292352371732817,0.6282671829622459,0.6272991287512101,0.6263310745401742,0.6253630203291385,0.6243949661181026,0.6234269119070668,0.6224588576960309,0.6224588576960309,0.6214908034849952,0.6205227492739593,0.6205227492739593,0.6205227492739593,0.6195546950629235,0.6185866408518877,0.6176185866408519,0.616650532429816,0.6156824782187803,0.6147144240077445,0.6137463697967086,0.6127783155856728,0.611810261374637,0.6108422071636012,0.6098741529525653,0.6089060987415296,0.6079380445304937,0.6069699903194579,0.6060019361084221,0.6050338818973863,0.6040658276863504,0.6040658276863504,0.6030977734753146,0.6021297192642788,0.601161665053243,0.6001936108422071,0.5992255566311714,0.5982575024201355,0.5972894482090997,0.5963213939980639,0.5963213939980639,0.5953533397870281,0.5943852855759922,0.5934172313649564,0.5934172313649564,0.5924491771539206,0.5924491771539206,0.5914811229428848,0.590513068731849,0.5895450145208132,0.5895450145208132,0.5885769603097774,0.5876089060987415,0.5866408518877058,0.5856727976766699,0.5847047434656341,0.5837366892545982,0.5827686350435625,0.5818005808325266,0.5818005808325266,0.5808325266214908,0.579864472410455,0.5788964181994192,0.5779283639883833,0.5779283639883833,0.5769603097773476,0.5759922555663117,0.5750242013552759,0.57405614714424,0.57405614714424,0.5730880929332043,0.5721200387221684,0.5711519845111326,0.5701839303000968,0.569215876089061,0.5682478218780251,0.5682478218780251,0.5672797676669894,0.5663117134559535,0.5653436592449177,0.5643756050338818,0.5634075508228461,0.5624394966118103,0.5614714424007744,0.5605033881897387,0.558567279767667,0.558567279767667,0.5575992255566312,0.5566311713455954,0.5556631171345595,0.5546950629235237,0.5537270087124879,0.5527589545014521,0.5517909002904162,0.5508228460793805,0.5498547918683446,0.5488867376573088,0.547918683446273,0.5469506292352372,0.5459825750242013,0.5450145208131656,0.5440464666021297,0.5430784123910939,0.542110358180058,0.5411423039690223,0.5401742497579864,0.5392061955469506,0.5382381413359149,0.537270087124879,0.5363020329138432,0.5353339787028074,0.5343659244917716,0.5333978702807357,0.5324298160696999,0.5314617618586641,0.5304937076476283,0.5295256534365924,0.5285575992255567,0.5275895450145208,0.526621490803485,0.5256534365924492,0.5246853823814134,0.5237173281703775,0.5227492739593417,0.5217812197483059,0.5217812197483059,0.5208131655372701,0.5198451113262342,0.5188770571151985,0.5179090029041626,0.5169409486931268,0.515972894482091,0.5150048402710552,0.5140367860600193,0.5140367860600193,0.5130687318489835,0.5121006776379478,0.5111326234269119,0.510164569215876,0.5091965150048403,0.5082284607938045,0.5072604065827686,0.5062923523717329,0.505324298160697,0.5043562439496612,0.5033881897386253,0.5024201355275896,0.5014520813165537,0.5004840271055179,0.4995159728944821,0.4995159728944821,0.4985479186834463,0.4975798644724105,0.49661181026137463,0.49564375605033884,0.494675701839303,0.4937076476282672,0.4927395934172314,0.4927395934172314,0.49177153920619554,0.49080348499515974,0.4898354307841239,0.4888673765730881,0.4878993223620523,0.48693126815101645,0.48596321393998065,0.4849951597289448,0.484027105517909,0.4830590513068732,0.48209099709583736,0.48112294288480156,0.4801548886737657,0.4791868344627299,0.4782187802516941,0.47725072604065827,0.4762826718296225,0.4753146176185866,0.4743465634075508,0.47337850919651503,0.4724104549854792,0.4714424007744434,0.47047434656340753,0.46950629235237173,0.46853823814133594,0.4675701839303001,0.4666021297192643,0.46563407550822844,0.46466602129719264,0.46369796708615685,0.462729912875121,0.4617618586640852,0.46079380445304935,0.46079380445304935,0.45982575024201355,0.45885769603097776,0.4578896418199419,0.4569215876089061,0.45595353339787026,0.45498547918683446,0.45401742497579867,0.4530493707647628,0.452081316553727,0.45111326234269117,0.45014520813165537,0.4491771539206196,0.4482090997095837,0.44724104549854793,0.4462729912875121,0.4453049370764763,0.4443368828654405,0.44336882865440463,0.44240077444336884,0.441432720232333,0.4404646660212972,0.4394966118102614,0.43852855759922554,0.43756050338818975,0.4365924491771539,0.4356243949661181,0.4346563407550823,0.43368828654404645,0.43272023233301066,0.4317521781219748,0.430784123910939,0.4298160696999032,0.42884801548886736,0.42787996127783157,0.4269119070667957,0.4259438528557599,0.4249757986447241,0.42400774443368827,0.4230396902226525,0.4220716360116166,0.42110358180058083,0.42013552758954503,0.4191674733785092,0.4181994191674734,0.41723136495643753,0.41626331074540174,0.41529525653436594,0.4143272023233301,0.4133591481122943,0.41239109390125844,0.41142303969022265,0.41045498547918685,0.409486931268151,0.4085188770571152,0.4075508228460794,0.40658276863504356,0.40658276863504356,0.40561471442400776,0.4046466602129719,0.4036786060019361,0.4027105517909003,0.4027105517909003,0.40174249757986447,0.40077444336882867,0.3998063891577928,0.398838334946757,0.3978702807357212,0.3969022265246854,0.3959341723136496,0.39496611810261373,0.39399806389157793,0.39303000968054214,0.3920619554695063,0.3910939012584705,0.39012584704743464,0.38915779283639884,0.38818973862536305,0.3872216844143272,0.3862536302032914,0.3862536302032914,0.38528557599225555,0.38431752178121975,0.38334946757018395,0.3823814133591481,0.3814133591481123,0.38044530493707646,0.37947725072604066,0.37850919651500486,0.377541142303969,0.3765730880929332,0.37560503388189737,0.37463697967086157,0.3736689254598258,0.3727008712487899,0.3727008712487899,0.3717328170377541,0.3707647628267183,0.3697967086156825,0.3688286544046467,0.36786060019361083,0.36689254598257504,0.3659244917715392,0.3649564375605034,0.3639883833494676,0.36302032913843174,0.36205227492739595,0.3610842207163601,0.3601161665053243,0.3591481122942885,0.35818005808325265,0.35721200387221685,0.35721200387221685,0.356243949661181,0.3552758954501452,0.3543078412391094,0.3543078412391094,0.35333978702807356,0.35237173281703776,0.3514036786060019,0.3504356243949661,0.3494675701839303,0.34849951597289447,0.3475314617618587,0.3465634075508228,0.345595353339787,0.34462729912875123,0.3436592449177154,0.3426911907066796,0.34172313649564373,0.34172313649564373,0.34075508228460794,0.33978702807357214,0.3388189738625363,0.3378509196515005,0.33688286544046464,0.33591481122942884,0.33494675701839305,0.33494675701839305,0.3339787028073572,0.3330106485963214,0.33204259438528555,0.33107454017424975,0.33010648596321396,0.3291384317521781,0.3281703775411423,0.32720232333010646,0.32720232333010646,0.32623426911907066,0.32526621490803487,0.324298160696999,0.3233301064859632,0.32236205227492737,0.3213939980638916,0.3204259438528558,0.3194578896418199,0.31848983543078413,0.31752178121974833,0.3165537270087125,0.3155856727976767,0.31461761858664083,0.31364956437560504,0.31268151016456924,0.3117134559535334,0.3107454017424976,0.30977734753146174,0.30880929332042595,0.30880929332042595,0.30784123910939015,0.3068731848983543,0.3059051306873185,0.30493707647628265,0.30396902226524686,0.30300096805421106,0.3020329138431752,0.3010648596321394,0.30009680542110356,0.29912875121006777,0.29816069699903197,0.2971926427879961,0.2962245885769603,0.2962245885769603,0.2952565343659245,0.2942884801548887,0.2933204259438529,0.29235237173281703,0.29138431752178123,0.2904162633107454,0.2894482090997096,0.2884801548886738,0.28751210067763794,0.28654404646660214,0.2855759922555663,0.2846079380445305,0.2836398838334947,0.28267182962245885,0.28170377541142305,0.2807357212003872,0.2797676669893514,0.2787996127783156,0.27783155856727976,0.27686350435624396,0.2758954501452081,0.2749273959341723,0.2739593417231365,0.27299128751210067,0.27202323330106487,0.271055179090029,0.2700871248789932,0.2691190706679574,0.2681510164569216,0.2681510164569216,0.2671829622458858,0.26621490803484993,0.26524685382381413,0.26427879961277834,0.2633107454017425,0.2623426911907067,0.26137463697967084,0.26040658276863504,0.25943852855759925,0.2584704743465634,0.2575024201355276,0.25653436592449175,0.25556631171345595,0.25459825750242016,0.2536302032913843,0.2526621490803485,0.25169409486931266,0.25072604065827686,0.24975798644724104,0.24878993223620524,0.24782187802516942,0.2468538238141336,0.24588576960309777,0.24491771539206195,0.24394966118102615,0.24298160696999033,0.2420135527589545,0.24104549854791868,0.24007744433688286,0.23910939012584706,0.23814133591481124,0.2371732817037754,0.2362052274927396,0.2362052274927396,0.23523717328170377,0.23426911907066797,0.23330106485963215,0.23233301064859632,0.2313649564375605,0.23039690222652467,0.22942884801548888,0.22846079380445306,0.22749273959341723,0.2265246853823814,0.22555663117134558,0.2245885769603098,0.22362052274927396,0.22265246853823814,0.22168441432720232,0.2207163601161665,0.2197483059051307,0.21878025169409487,0.21781219748305905,0.21684414327202323,0.2158760890609874,0.2149080348499516,0.21393998063891578,0.21297192642787996,0.21200387221684414,0.2110358180058083,0.21006776379477252,0.2090997095837367,0.20813165537270087,0.20716360116166505,0.20619554695062922,0.20522749273959343,0.2042594385285576,0.20329138431752178,0.20232333010648595,0.20135527589545016,0.20038722168441434,0.1994191674733785,0.1994191674733785,0.1984511132623427,0.19748305905130686,0.19651500484027107,0.19554695062923524,0.19457889641819942,0.1936108422071636,0.19264278799612777,0.19167473378509198,0.19070667957405615,0.18973862536302033,0.1887705711519845,0.18780251694094868,0.1868344627299129,0.18586640851887706,0.18489835430784124,0.18393030009680542,0.1829622458857696,0.1819941916747338,0.18102613746369797,0.18005808325266215,0.17909002904162633,0.1781219748305905,0.1771539206195547,0.17618586640851888,0.17521781219748306,0.17521781219748306,0.17424975798644723,0.1732817037754114,0.17231364956437561,0.1713455953533398,0.17037754114230397,0.16940948693126814,0.16844143272023232,0.16747337850919652,0.1665053242981607,0.16553727008712488,0.16456921587608905,0.16360116166505323,0.16263310745401743,0.1616650532429816,0.1606969990319458,0.15972894482090996,0.15876089060987417,0.15779283639883834,0.15682478218780252,0.1558567279767667,0.15488867376573087,0.15392061955469508,0.15295256534365925,0.15198451113262343,0.1510164569215876,0.15004840271055178,0.14908034849951599,0.14811229428848016,0.14714424007744434,0.14617618586640851,0.1452081316553727,0.1442400774443369,0.14327202323330107,0.14230396902226525,0.14133591481122942,0.1403678606001936,0.1393998063891578,0.13843175217812198,0.13746369796708616,0.13649564375605033,0.1355275895450145,0.1345595353339787,0.1335914811229429,0.13262342691190707,0.13165537270087124,0.13068731848983542,0.12971926427879962,0.1287512100677638,0.12778315585672798,0.12681510164569215,0.12681510164569215,0.12584704743465633,0.12487899322362052,0.12391093901258471,0.12294288480154889,0.12197483059051308,0.12100677637947725,0.12003872216844143,0.11907066795740562,0.1181026137463698,0.11713455953533398,0.11713455953533398,0.11616650532429816,0.11519845111326234,0.11423039690222653,0.1132623426911907,0.1122942884801549,0.11132623426911907,0.11035818005808325,0.10939012584704744,0.10842207163601161,0.1074540174249758,0.10648596321393998,0.10551790900290416,0.10454985479186835,0.10358180058083252,0.10261374636979671,0.10164569215876089,0.10067763794772508,0.09970958373668926,0.09874152952565343,0.09777347531461762,0.0968054211035818,0.09583736689254599,0.09486931268151017,0.09390125847047434,0.09293320425943853,0.09196515004840271,0.0909970958373669,0.09002904162633107,0.08906098741529525,0.08809293320425944,0.08712487899322362,0.08615682478218781,0.08518877057115198,0.08518877057115198,0.08422071636011616,0.08325266214908035,0.08228460793804453,0.08131655372700872,0.0803484995159729,0.07938044530493708,0.07841239109390126,0.07744433688286544,0.07647628267182963,0.0755082284607938,0.07454017424975799,0.07357212003872217,0.07260406582768635,0.07163601161665054,0.07066795740561471,0.0696999031945789,0.06873184898354308,0.06776379477250725,0.06679574056147145,0.06582768635043562,0.06485963213939981,0.06485963213939981,0.06389157792836399,0.06292352371732816,0.061955469506292354,0.06098741529525654,0.060019361084220714,0.0590513068731849,0.05808325266214908,0.057115198451113264,0.05614714424007745,0.05517909002904162,0.05421103581800581,0.05324298160696999,0.05227492739593417,0.051306873184898356,0.05033881897386254,0.049370764762826716,0.0484027105517909,0.04743465634075508,0.046466602129719266,0.04549854791868345,0.044530493707647625,0.04356243949661181,0.04259438528557599,0.041626331074540175,0.04065827686350436,0.03969022265246854,0.03872216844143272,0.0377541142303969,0.0377541142303969,0.036786060019361085,0.03581800580832527,0.03484995159728945,0.03388189738625363,0.03291384317521781,0.031945788964181994,0.030977734753146177,0.030009680542110357,0.02904162633107454,0.028073572120038724,0.027105517909002903,0.026137463697967087,0.02516940948693127,0.02420135527589545,0.023233301064859633,0.022265246853823813,0.021297192642787996,0.02032913843175218,0.01936108422071636,0.018393030009680542,0.017424975798644726,0.016456921587608905,0.015488867376573089,0.01452081316553727,0.013552758954501452,0.012584704743465635,0.011616650532429816,0.010648596321393998,0.00968054211035818,0.008712487899322363,0.007744433688286544,0.006776379477250726,0.005808325266214908,0.00484027105517909,0.003872216844143272,0.002904162633107454,0.001936108422071636,0.000968054211035818,0.0],\"xaxis\":\"x\",\"y\":[0.7341862117981521,0.7347083926031295,0.7352313167259786,0.7357549857549858,0.7362794012829651,0.7368045649072753,0.7373304782298359,0.7371428571428571,0.7376697641172266,0.7381974248927039,0.7387258410880458,0.7392550143266475,0.7397849462365591,0.7403156384505022,0.7408470926058865,0.7413793103448276,0.7419122933141624,0.7424460431654676,0.7429805615550756,0.7435158501440923,0.7440519105984138,0.7445887445887446,0.7451263537906138,0.744942196531792,0.7454808387563269,0.7460202604920405,0.7458363504706734,0.7463768115942029,0.7469180565627266,0.7474600870827286,0.7480029048656499,0.748546511627907,0.7490909090909091,0.7496360989810772,0.7501820830298617,0.750728862973761,0.75127644055434,0.7518248175182481,0.7523739956172388,0.7529239766081871,0.7534747622531089,0.7540263543191801,0.7545787545787546,0.7543988269794721,0.7549523110785032,0.7555066079295154,0.7553269654665687,0.7558823529411764,0.7564385577630611,0.7562592047128129,0.7568165070007369,0.7573746312684366,0.7579335793357933,0.758493353028065,0.7590539541759054,0.7588757396449705,0.7594374537379719,0.76,0.759822090437361,0.7603857566765578,0.7609502598366741,0.7615156017830609,0.7620817843866171,0.7626488095238095,0.763216679076694,0.763785394932936,0.7643549589858315,0.7649253731343284,0.765496639283047,0.7653213751868461,0.7658937920718025,0.7664670658682635,0.7670411985018727,0.7668665667166417,0.7666916729182296,0.7672672672672672,0.7678437265214124,0.7684210526315789,0.7689992475545523,0.7695783132530121,0.7701582516955539,0.770739064856712,0.7713207547169811,0.7719033232628398,0.7724867724867724,0.7723146747352496,0.7728993186979561,0.7727272727272727,0.7733131159969674,0.7738998482549317,0.7737281700835231,0.7743161094224924,0.7741444866920152,0.7747336377473364,0.7753236862147753,0.7759146341463414,0.7765064836003052,0.7770992366412214,0.7776928953399541,0.7782874617737003,0.7788829380260138,0.7787136294027565,0.7793103448275862,0.7799079754601227,0.7805065234075211,0.7811059907834101,0.781706379707917,0.7823076923076923,0.7829099307159353,0.7835130970724191,0.7833461835003855,0.783179012345679,0.783011583011583,0.7836166924265843,0.7842227378190255,0.7848297213622291,0.7854376452362509,0.7852713178294574,0.7851047323506595,0.7857142857142857,0.7863247863247863,0.7869362363919129,0.7875486381322957,0.7881619937694704,0.7887763055339049,0.7893915756630265,0.790007806401249,0.790625,0.7912431587177482,0.7918622848200313,0.7924823805794832,0.792319749216301,0.7929411764705883,0.792778649921507,0.793401413982718,0.7932389937106918,0.7930763178599528,0.7929133858267716,0.793538219070134,0.7933753943217665,0.7932123125493291,0.7930489731437599,0.7936758893280632,0.7943037974683544,0.7941409342834521,0.794770206022187,0.7954004758128469,0.7952380952380952,0.7950754567116759,0.7957074721780604,0.7963404932378679,0.7969745222929936,0.796812749003984,0.7966507177033493,0.7972865123703112,0.797923322683706,0.7985611510791367,0.7992,0.799839871897518,0.7996794871794872,0.7995188452285485,0.8001605136436597,0.8008032128514057,0.8014469453376206,0.8020917135961384,0.8027375201288245,0.8025785656728445,0.8032258064516129,0.8038740920096852,0.8045234248788369,0.8051738075990299,0.8058252427184466,0.8064777327935223,0.807131280388979,0.8077858880778589,0.8084415584415584,0.8090982940698619,0.8089430894308943,0.8096013018714402,0.8102605863192183,0.8101059494702526,0.8107667210440457,0.8106122448979591,0.8112745098039216,0.8111201962387572,0.8117839607201309,0.8116298116298116,0.8122950819672131,0.8129614438063987,0.812807881773399,0.8126540673788003,0.8133223684210527,0.8139917695473251,0.814662273476112,0.8153338829348722,0.816006600660066,0.8166804293971924,0.8173553719008264,0.8172043010752689,0.8178807947019867,0.8185584092792046,0.8192371475953566,0.8199170124481328,0.8197674418604651,0.8204488778054863,0.8211314475873545,0.8218151540383014,0.8216666666666667,0.8223519599666389,0.8230383973288815,0.8237259816207184,0.8235785953177257,0.8234309623430962,0.8232830820770519,0.8239731768650461,0.8246644295302014,0.8245172124265323,0.8243697478991596,0.8250630782169891,0.8257575757575758,0.8256107834877844,0.8254637436762225,0.8261603375527427,0.8260135135135135,0.8267117497886729,0.8265651438240271,0.8264182895850973,0.826271186440678,0.8261238337574215,0.8268251273344652,0.8275276125743416,0.8282312925170068,0.828936170212766,0.8296422487223168,0.8294970161977835,0.8293515358361775,0.8300597779675492,0.8307692307692308,0.8306244653550042,0.8304794520547946,0.8303341902313625,0.8310463121783876,0.8317596566523605,0.8324742268041238,0.8331900257953568,0.8339070567986231,0.8337639965546942,0.8344827586206897,0.8352027610008628,0.8359240069084629,0.8357821953327571,0.8365051903114187,0.8372294372294372,0.8370883882149047,0.836947094535993,0.8368055555555556,0.8375325803649001,0.8382608695652174,0.8389904264577894,0.8388501742160279,0.8387096774193549,0.8385689354275742,0.8393013100436681,0.840034965034965,0.8398950131233596,0.840630472854641,0.8404907975460123,0.8403508771929824,0.8410886742756805,0.8418277680140598,0.8425681618293756,0.8433098591549296,0.8431718061674008,0.8430335097001763,0.8437775816416593,0.8436395759717314,0.843501326259947,0.8433628318584071,0.8432240921169176,0.8430851063829787,0.8429458740017747,0.8436944937833037,0.8435555555555555,0.844306049822064,0.8450578806767587,0.8458110516934046,0.8465655664585192,0.8473214285714286,0.8471849865951743,0.8470483005366727,0.846911369740376,0.8467741935483871,0.8466367713004485,0.8464991023339318,0.8463611859838275,0.8462230215827338,0.8460846084608461,0.8468468468468469,0.8476104598737602,0.8474729241877257,0.8473351400180669,0.8471971066907775,0.8470588235294118,0.8469202898550725,0.8467815049864007,0.8466424682395645,0.846503178928247,0.8463636363636363,0.8462238398544131,0.8469945355191257,0.8468550592525068,0.8476277372262774,0.8484018264840183,0.8491773308957953,0.8490393412625801,0.8489010989010989,0.84967919340055,0.8495412844036697,0.8503213957759412,0.8501838235294118,0.8500459981600736,0.8499079189686924,0.8506912442396314,0.8514760147601476,0.8522622345337026,0.8530499075785583,0.8529139685476411,0.8537037037037037,0.8544949026876738,0.8552875695732839,0.8560817084493965,0.8559479553903345,0.8558139534883721,0.8566108007448789,0.8564771668219944,0.8563432835820896,0.8571428571428571,0.8570093457943925,0.8568755846585594,0.8576779026217228,0.8575445173383318,0.8574108818011257,0.8572769953051643,0.8571428571428571,0.8570084666039511,0.8568738229755178,0.8576814326107446,0.8584905660377359,0.8583569405099151,0.8582230623818525,0.859035004730369,0.8598484848484849,0.8597156398104265,0.8595825426944972,0.8603988603988604,0.8612167300380228,0.8610846812559467,0.8609523809523809,0.8617731172545281,0.8625954198473282,0.8634192932187201,0.8632887189292543,0.8631578947368421,0.8639846743295019,0.8648130393096836,0.8646833013435701,0.8655139289145053,0.8663461538461539,0.8662175168431184,0.8660886319845857,0.8659594985535197,0.8658301158301158,0.8657004830917875,0.8665377176015474,0.8664085188770572,0.8662790697674418,0.8661493695441319,0.8660194174757282,0.8658892128279884,0.8657587548638133,0.866601752677702,0.8664717348927875,0.8673170731707317,0.8671875,0.8670576735092864,0.8669275929549902,0.8667972575905974,0.8666666666666667,0.8675171736997056,0.8673870333988212,0.8682399213372665,0.8690944881889764,0.8699507389162562,0.8708086785009862,0.8716683119447186,0.8725296442687747,0.8733926805143423,0.8732673267326733,0.8741328047571854,0.8740079365079365,0.8738828202581926,0.8747514910536779,0.8756218905472637,0.8764940239043825,0.8763708873379861,0.8772455089820359,0.8771228771228772,0.878,0.8778778778778779,0.8777555110220441,0.8786359077231695,0.8785140562248996,0.878391959798995,0.8792756539235412,0.8801611278952669,0.8810483870967742,0.8809283551967709,0.8818181818181818,0.8827098078867543,0.8825910931174089,0.8834853090172239,0.8833671399594321,0.884263959390863,0.8851626016260162,0.8860630722278738,0.8869653767820774,0.8868501529051988,0.886734693877551,0.8866189989785496,0.8875255623721882,0.887410440122825,0.8872950819672131,0.8882051282051282,0.8880903490759754,0.8890030832476875,0.8888888888888888,0.8898043254376931,0.8896907216494845,0.8895768833849329,0.8894628099173554,0.890382626680455,0.8902691511387164,0.8901554404145078,0.8900414937759336,0.8899273104880582,0.8898128898128899,0.8896982310093653,0.8895833333333333,0.8905109489051095,0.8914405010438413,0.8913270637408568,0.891213389121339,0.8921465968586387,0.8920335429769392,0.8919202518363064,0.8928571428571429,0.8927444794952681,0.8926315789473684,0.8925184404636459,0.8934599156118144,0.8944033790918691,0.8942917547568711,0.8941798941798942,0.8951271186440678,0.8960763520678685,0.8970276008492569,0.8969181721572795,0.8978723404255319,0.8977635782747604,0.8976545842217484,0.8975453575240128,0.8985042735042735,0.8983957219251337,0.8982869379014989,0.8981779206859593,0.8991416309012875,0.8990332975295381,0.8989247311827957,0.898815931108719,0.8987068965517241,0.8985976267529665,0.8984881209503239,0.8994594594594595,0.8993506493506493,0.8992416034669556,0.8991323210412148,0.8990228013029316,0.9,0.9009793253536452,0.900871459694989,0.9007633587786259,0.9006550218340611,0.9016393442622951,0.9026258205689278,0.9036144578313253,0.9035087719298246,0.9034028540065862,0.9043956043956044,0.9053905390539054,0.9052863436123348,0.9051819184123484,0.9061810154525386,0.9060773480662984,0.9059734513274337,0.9058693244739756,0.9068736141906873,0.9078801331853497,0.9077777777777778,0.9076751946607341,0.9075723830734966,0.907469342251951,0.9073660714285714,0.9072625698324023,0.9071588366890381,0.9081746920492721,0.9080717488789237,0.9090909090909091,0.9089887640449438,0.9088863892013498,0.9099099099099099,0.910935738444194,0.9108352144469526,0.9107344632768362,0.9106334841628959,0.9105322763306908,0.9104308390022676,0.9103291713961408,0.9102272727272728,0.9112627986348123,0.9111617312072893,0.9110604332953249,0.9121004566210046,0.9131428571428571,0.9130434782608695,0.9129438717067583,0.9128440366972477,0.912743972445465,0.9137931034482759,0.9136939010356732,0.913594470046083,0.9134948096885813,0.9133949191685913,0.9132947976878613,0.9131944444444444,0.9142526071842411,0.91415313225058,0.9140534262485482,0.9151162790697674,0.9150174621653085,0.916083916083916,0.9159859976662778,0.9158878504672897,0.9157894736842105,0.9156908665105387,0.9155920281359906,0.9154929577464789,0.9153936545240893,0.9152941176470588,0.9151943462897526,0.9150943396226415,0.9161747343565525,0.9160756501182034,0.9171597633136095,0.9182464454976303,0.9181494661921709,0.9180522565320665,0.9179548156956004,0.9178571428571428,0.9177592371871275,0.9176610978520287,0.9175627240143369,0.9186602870813397,0.918562874251497,0.919664268585132,0.9195678271308524,0.9194711538461539,0.9193742478941035,0.9192771084337349,0.9203860072376358,0.9214975845410628,0.9214026602176542,0.9213075060532687,0.9212121212121213,0.9211165048543689,0.9210206561360875,0.9209245742092458,0.9208282582216809,0.9219512195121952,0.9218559218559218,0.921760391198044,0.9216646266829865,0.9215686274509803,0.9226993865030675,0.9226044226044227,0.922509225092251,0.9224137931034483,0.9223181257706535,0.9222222222222223,0.9221260815822002,0.9232673267326733,0.9244114002478315,0.9255583126550868,0.9254658385093167,0.9266169154228856,0.9265255292652553,0.92643391521197,0.9263420724094882,0.92625,0.9274092615769712,0.9273182957393483,0.9272271016311167,0.9271356783919598,0.9270440251572327,0.9269521410579346,0.9281210592686002,0.928030303030303,0.9279393173198482,0.9278481012658227,0.9277566539923955,0.9276649746192893,0.9275730622617535,0.9274809160305344,0.9273885350318471,0.9272959183673469,0.9284802043422733,0.928388746803069,0.9282970550576184,0.9282051282051282,0.9281129653401797,0.9280205655526992,0.9292149292149292,0.9291237113402062,0.9290322580645162,0.9289405684754521,0.9288486416558862,0.9287564766839378,0.9299610894941635,0.9298701298701298,0.929778933680104,0.9296875,0.9295958279009127,0.9295039164490861,0.9294117647058824,0.9293193717277487,0.9305373525557011,0.931758530183727,0.9316688567674113,0.9315789473684211,0.932806324110672,0.9327176781002638,0.9326287978863936,0.9325396825396826,0.9324503311258279,0.9323607427055703,0.9322709163346613,0.9321808510638298,0.933422103861518,0.9333333333333333,0.9345794392523364,0.9344919786096256,0.9344042838018741,0.935656836461126,0.9355704697986578,0.9354838709677419,0.9353970390309556,0.9353099730458221,0.9352226720647774,0.9351351351351351,0.9350473612990527,0.9363143631436315,0.9362279511533242,0.936141304347826,0.9360544217687075,0.9359673024523161,0.9358799454297408,0.9357923497267759,0.9357045143638851,0.9356164383561644,0.9355281207133059,0.9354395604395604,0.936726272352132,0.9366391184573003,0.9379310344827586,0.9378453038674033,0.9377593360995851,0.9390581717451524,0.9389736477115118,0.9388888888888889,0.9388038942976356,0.9387186629526463,0.9386331938633193,0.9385474860335196,0.9398601398601398,0.9397759103641457,0.9396914446002805,0.9396067415730337,0.939521800281294,0.9394366197183098,0.9393511988716502,0.9392655367231638,0.9391796322489392,0.9390934844192634,0.9390070921985816,0.9389204545454546,0.9402560455192034,0.9401709401709402,0.9400855920114123,0.94,0.9399141630901288,0.9412607449856734,0.9411764705882353,0.9410919540229885,0.9410071942446043,0.9409221902017291,0.9408369408369408,0.9421965317919075,0.9421128798842258,0.9434782608695652,0.9448476052249637,0.9447674418604651,0.9446870451237264,0.9446064139941691,0.9445255474452555,0.9444444444444444,0.9443631039531479,0.9442815249266863,0.9441997063142438,0.9455882352941176,0.9455081001472754,0.9454277286135693,0.946824224519941,0.9482248520710059,0.9481481481481482,0.9480712166172107,0.9479940564635958,0.9479166666666666,0.9478390461997019,0.9477611940298507,0.9476831091180867,0.9476047904191617,0.9475262368815592,0.9474474474474475,0.9473684210526315,0.947289156626506,0.947209653092006,0.947129909365559,0.9470499243570348,0.946969696969697,0.9468892261001517,0.9483282674772037,0.9482496194824962,0.948170731707317,0.9480916030534351,0.9480122324159022,0.9479326186830015,0.9478527607361963,0.9477726574500768,0.9476923076923077,0.9491525423728814,0.9490740740740741,0.9489953632148377,0.9489164086687306,0.9503875968992248,0.9503105590062112,0.9517884914463453,0.9517133956386293,0.9516380655226209,0.9515625,0.9530516431924883,0.9529780564263323,0.9529042386185244,0.9528301886792453,0.952755905511811,0.9526813880126183,0.95260663507109,0.9525316455696202,0.9524564183835182,0.953968253968254,0.9538950715421304,0.9538216560509554,0.9537480063795853,0.9536741214057508,0.9552,0.9551282051282052,0.9550561797752809,0.954983922829582,0.9549114331723028,0.9564516129032258,0.9563812600969306,0.9563106796116505,0.9562398703403565,0.9561688311688312,0.9560975609756097,0.9560260586319218,0.9575856443719413,0.9575163398692811,0.9574468085106383,0.9573770491803278,0.9573070607553367,0.9572368421052632,0.957166392092257,0.9570957095709571,0.9570247933884297,0.956882255389718,0.9584717607973422,0.9584026622296173,0.9583333333333334,0.9582637729549248,0.9581939799331104,0.9581239530988275,0.9580536912751678,0.957983193277311,0.9579124579124579,0.9578414839797639,0.9577702702702703,0.9576988155668359,0.9576271186440678,0.9575551782682513,0.9574829931972789,0.9574105621805792,0.9573378839590444,0.9572649572649573,0.9571917808219178,0.9571183533447685,0.9570446735395189,0.9569707401032702,0.9568965517241379,0.9568221070811744,0.9567474048442907,0.9566724436741768,0.9565972222222222,0.9565217391304348,0.9564459930313589,0.956369982547993,0.9562937062937062,0.9562171628721541,0.956140350877193,0.9560632688927944,0.9559859154929577,0.9559082892416225,0.9558303886925795,0.9557522123893806,0.9556737588652482,0.9573712255772646,0.9572953736654805,0.9572192513368984,0.9571428571428572,0.9570661896243292,0.956989247311828,0.9569120287253142,0.9568345323741008,0.9567567567567568,0.9584837545126353,0.9584086799276673,0.9583333333333334,0.9582577132486388,0.9581818181818181,0.9581056466302368,0.958029197080292,0.9579524680073126,0.9578754578754579,0.9577981651376147,0.9577205882352942,0.9576427255985267,0.9575645756457565,0.9574861367837338,0.9574074074074074,0.9573283858998145,0.9591078066914498,0.9590316573556797,0.9589552238805971,0.9588785046728971,0.9588014981273408,0.9587242026266416,0.9586466165413534,0.9585687382297552,0.960377358490566,0.9603024574669187,0.9602272727272727,0.9601518026565465,0.9600760456273765,0.96,0.9599236641221374,0.9598470363288719,0.9597701149425287,0.9596928982725528,0.9596153846153846,0.9595375722543352,0.9594594594594594,0.9593810444874274,0.9593023255813954,0.9592233009708738,0.9591439688715954,0.9590643274853801,0.958984375,0.958904109589041,0.9588235294117647,0.9587426326129665,0.9586614173228346,0.9585798816568047,0.958498023715415,0.9584158415841584,0.9583333333333334,0.9582504970178927,0.9581673306772909,0.9580838323353293,0.958,0.9579158316633266,0.9578313253012049,0.9577464788732394,0.9596774193548387,0.9595959595959596,0.9595141700404858,0.9594320486815415,0.959349593495935,0.9592668024439919,0.9591836734693877,0.9591002044989775,0.9590163934426229,0.9589322381930184,0.9588477366255144,0.9587628865979382,0.9586776859504132,0.9585921325051759,0.9585062240663901,0.9584199584199584,0.9583333333333334,0.9582463465553236,0.9581589958158996,0.9580712788259959,0.957983193277311,0.9578947368421052,0.9578059071729957,0.9577167019027484,0.9576271186440678,0.9575371549893843,0.9574468085106383,0.9573560767590619,0.9572649572649573,0.9571734475374732,0.9570815450643777,0.956989247311828,0.9568965517241379,0.9568034557235421,0.9567099567099567,0.9566160520607375,0.9565217391304348,0.9564270152505446,0.9563318777292577,0.9562363238512035,0.956140350877193,0.9560439560439561,0.9559471365638766,0.9558498896247241,0.9557522123893806,0.9556541019955654,0.9555555555555556,0.955456570155902,0.9553571428571429,0.9552572706935123,0.9551569506726457,0.9550561797752809,0.954954954954955,0.9548532731376975,0.9547511312217195,0.9546485260770975,0.9545454545454546,0.9567198177676538,0.95662100456621,0.9565217391304348,0.9564220183486238,0.9563218390804598,0.9585253456221198,0.9584295612009238,0.9583333333333334,0.9582366589327146,0.958139534883721,0.958041958041958,0.9579439252336449,0.9578454332552693,0.9577464788732394,0.9576470588235294,0.9575471698113207,0.9574468085106383,0.957345971563981,0.9572446555819477,0.9571428571428572,0.9570405727923628,0.9569377990430622,0.9568345323741008,0.9591346153846154,0.9590361445783132,0.9589371980676329,0.9588377723970944,0.9587378640776699,0.9586374695863747,0.9585365853658536,0.9584352078239609,0.9583333333333334,0.9582309582309583,0.958128078817734,0.9580246913580247,0.9579207920792079,0.9578163771712159,0.9577114427860697,0.9600997506234414,0.96,0.9598997493734336,0.9597989949748744,0.9596977329974811,0.9595959595959596,0.959493670886076,0.9593908629441624,0.9592875318066157,0.9591836734693877,0.959079283887468,0.958974358974359,0.9588688946015425,0.9587628865979382,0.958656330749354,0.9585492227979274,0.9584415584415584,0.9609375,0.9608355091383812,0.9607329842931938,0.9606299212598425,0.9631578947368421,0.9630606860158312,0.9629629629629629,0.9628647214854111,0.9627659574468085,0.9626666666666667,0.9625668449197861,0.9624664879356568,0.9623655913978495,0.9622641509433962,0.9621621621621622,0.962059620596206,0.9619565217391305,0.9618528610354223,0.9644808743169399,0.9643835616438357,0.9642857142857143,0.9641873278236914,0.9640883977900553,0.96398891966759,0.9638888888888889,0.9637883008356546,0.9664804469273743,0.9663865546218487,0.9662921348314607,0.9661971830985916,0.9661016949152542,0.9660056657223796,0.9659090909090909,0.9658119658119658,0.9657142857142857,0.9684813753581661,0.9683908045977011,0.968299711815562,0.9682080924855492,0.9681159420289855,0.9680232558139535,0.967930029154519,0.9678362573099415,0.967741935483871,0.9676470588235294,0.967551622418879,0.9674556213017751,0.9673590504451038,0.9672619047619048,0.9671641791044776,0.9670658682634731,0.9669669669669669,0.9668674698795181,0.9667673716012085,0.9666666666666667,0.9696048632218845,0.9695121951219512,0.9694189602446484,0.9693251533742331,0.9692307692307692,0.9691358024691358,0.9690402476780186,0.968944099378882,0.9688473520249221,0.96875,0.9686520376175548,0.9685534591194969,0.9684542586750788,0.9683544303797469,0.9714285714285714,0.9713375796178344,0.9712460063897763,0.9711538461538461,0.9710610932475884,0.9709677419354839,0.970873786407767,0.9707792207792207,0.9706840390879479,0.9705882352941176,0.9704918032786886,0.9703947368421053,0.9702970297029703,0.9701986754966887,0.9700996677740864,0.97,0.9698996655518395,0.9697986577181208,0.9696969696969697,0.9695945945945946,0.9694915254237289,0.9693877551020408,0.9692832764505119,0.9691780821917808,0.9690721649484536,0.9689655172413794,0.9688581314878892,0.96875,0.9686411149825784,0.9685314685314685,0.9719298245614035,0.971830985915493,0.9717314487632509,0.9716312056737588,0.9715302491103203,0.9714285714285714,0.9713261648745519,0.9712230215827338,0.9711191335740073,0.9710144927536232,0.9709090909090909,0.9708029197080292,0.9706959706959707,0.9705882352941176,0.9704797047970479,0.9703703703703703,0.9702602230483272,0.9701492537313433,0.9700374531835206,0.9699248120300752,0.969811320754717,0.9696969696969697,0.9695817490494296,0.9694656488549618,0.9693486590038314,0.9692307692307692,0.9691119691119691,0.9689922480620154,0.9688715953307393,0.96875,0.9686274509803922,0.968503937007874,0.9683794466403162,0.9682539682539683,0.9721115537848606,0.972,0.9718875502008032,0.9717741935483871,0.97165991902834,0.9715447154471545,0.9714285714285714,0.9713114754098361,0.9711934156378601,0.9710743801652892,0.970954356846473,0.9708333333333333,0.9707112970711297,0.9705882352941176,0.9704641350210971,0.9703389830508474,0.9702127659574468,0.9700854700854701,0.9699570815450643,0.9698275862068966,0.9696969696969697,0.9695652173913043,0.9694323144104804,0.9692982456140351,0.9691629955947136,0.9690265486725663,0.9688888888888889,0.96875,0.968609865470852,0.9684684684684685,0.9683257918552036,0.9681818181818181,0.9680365296803652,0.9678899082568807,0.967741935483871,0.9675925925925926,0.9674418604651163,0.9672897196261683,0.9671361502347418,0.9716981132075472,0.9715639810426541,0.9714285714285714,0.9712918660287081,0.9711538461538461,0.9710144927536232,0.970873786407767,0.9707317073170731,0.9705882352941176,0.9704433497536946,0.9702970297029703,0.9701492537313433,0.97,0.9698492462311558,0.9696969696969697,0.9695431472081218,0.9693877551020408,0.9692307692307692,0.9690721649484536,0.9689119170984456,0.96875,0.9685863874345549,0.968421052631579,0.9682539682539683,0.9680851063829787,0.9679144385026738,0.9731182795698925,0.972972972972973,0.9728260869565217,0.9726775956284153,0.9725274725274725,0.9723756906077348,0.9722222222222222,0.9720670391061452,0.9719101123595506,0.9717514124293786,0.9715909090909091,0.9714285714285714,0.9712643678160919,0.9710982658959537,0.9709302325581395,0.9707602339181286,0.9705882352941176,0.9704142011834319,0.9702380952380952,0.9700598802395209,0.9698795180722891,0.9696969696969697,0.9695121951219512,0.9693251533742331,0.9691358024691358,0.968944099378882,0.96875,0.9685534591194969,0.9683544303797469,0.9681528662420382,0.967948717948718,0.967741935483871,0.9675324675324676,0.9673202614379085,0.9671052631578947,0.9668874172185431,0.9666666666666667,0.9664429530201343,0.9662162162162162,0.9659863945578231,0.9657534246575342,0.9655172413793104,0.9652777777777778,0.965034965034965,0.9647887323943662,0.9645390070921985,0.9642857142857143,0.9640287769784173,0.9637681159420289,0.9635036496350365,0.9632352941176471,0.9703703703703703,0.9701492537313433,0.9699248120300752,0.9696969696969697,0.9694656488549618,0.9692307692307692,0.9689922480620154,0.96875,0.968503937007874,0.9682539682539683,0.968,0.9758064516129032,0.975609756097561,0.9754098360655737,0.9752066115702479,0.975,0.9747899159663865,0.9745762711864406,0.9743589743589743,0.9741379310344828,0.9739130434782609,0.9736842105263158,0.9734513274336283,0.9732142857142857,0.972972972972973,0.9727272727272728,0.9724770642201835,0.9722222222222222,0.9719626168224299,0.9716981132075472,0.9714285714285714,0.9711538461538461,0.970873786407767,0.9705882352941176,0.9702970297029703,0.97,0.9696969696969697,0.9693877551020408,0.9690721649484536,0.96875,0.968421052631579,0.9680851063829787,0.967741935483871,0.967391304347826,0.967032967032967,0.9777777777777777,0.9775280898876404,0.9772727272727273,0.9770114942528736,0.9767441860465116,0.9764705882352941,0.9761904761904762,0.9759036144578314,0.975609756097561,0.9753086419753086,0.975,0.9746835443037974,0.9743589743589743,0.974025974025974,0.9736842105263158,0.9733333333333334,0.972972972972973,0.9726027397260274,0.9722222222222222,0.971830985915493,0.9714285714285714,0.9710144927536232,0.9852941176470589,0.9850746268656716,0.9848484848484849,0.9846153846153847,0.984375,0.9841269841269841,0.9838709677419355,0.9836065573770492,0.9833333333333333,0.9830508474576272,0.9827586206896551,0.9824561403508771,0.9821428571428571,0.9818181818181818,0.9814814814814815,0.9811320754716981,0.9807692307692307,0.9803921568627451,0.98,0.9795918367346939,0.9791666666666666,0.9787234042553191,0.9782608695652174,0.9777777777777777,0.9772727272727273,0.9767441860465116,0.9761904761904762,0.975609756097561,0.975,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0],\"yaxis\":\"y\",\"type\":\"scatter\"}],                        {\"template\":{\"data\":{\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"white\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"white\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"#C8D4E3\",\"linecolor\":\"#C8D4E3\",\"minorgridcolor\":\"#C8D4E3\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"#C8D4E3\",\"linecolor\":\"#C8D4E3\",\"minorgridcolor\":\"#C8D4E3\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"choropleth\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"choropleth\"}],\"contourcarpet\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"contourcarpet\"}],\"contour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"contour\"}],\"heatmapgl\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmapgl\"}],\"heatmap\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmap\"}],\"histogram2dcontour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2dcontour\"}],\"histogram2d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2d\"}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"mesh3d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"mesh3d\"}],\"parcoords\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"parcoords\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}],\"scatter3d\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatter3d\"}],\"scattercarpet\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattercarpet\"}],\"scattergeo\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergeo\"}],\"scattergl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergl\"}],\"scattermapbox\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattermapbox\"}],\"scatterpolargl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolargl\"}],\"scatterpolar\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolar\"}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"scatterternary\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterternary\"}],\"surface\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"surface\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}]},\"layout\":{\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"autotypenumbers\":\"strict\",\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]],\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]},\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"geo\":{\"bgcolor\":\"white\",\"lakecolor\":\"white\",\"landcolor\":\"white\",\"showlakes\":true,\"showland\":true,\"subunitcolor\":\"#C8D4E3\"},\"hoverlabel\":{\"align\":\"left\"},\"hovermode\":\"closest\",\"mapbox\":{\"style\":\"light\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"white\",\"polar\":{\"angularaxis\":{\"gridcolor\":\"#EBF0F8\",\"linecolor\":\"#EBF0F8\",\"ticks\":\"\"},\"bgcolor\":\"white\",\"radialaxis\":{\"gridcolor\":\"#EBF0F8\",\"linecolor\":\"#EBF0F8\",\"ticks\":\"\"}},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"white\",\"gridcolor\":\"#DFE8F3\",\"gridwidth\":2,\"linecolor\":\"#EBF0F8\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"#EBF0F8\"},\"yaxis\":{\"backgroundcolor\":\"white\",\"gridcolor\":\"#DFE8F3\",\"gridwidth\":2,\"linecolor\":\"#EBF0F8\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"#EBF0F8\"},\"zaxis\":{\"backgroundcolor\":\"white\",\"gridcolor\":\"#DFE8F3\",\"gridwidth\":2,\"linecolor\":\"#EBF0F8\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"#EBF0F8\"}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"ternary\":{\"aaxis\":{\"gridcolor\":\"#DFE8F3\",\"linecolor\":\"#A2B1C6\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"#DFE8F3\",\"linecolor\":\"#A2B1C6\",\"ticks\":\"\"},\"bgcolor\":\"white\",\"caxis\":{\"gridcolor\":\"#DFE8F3\",\"linecolor\":\"#A2B1C6\",\"ticks\":\"\"}},\"title\":{\"x\":0.05},\"xaxis\":{\"automargin\":true,\"gridcolor\":\"#EBF0F8\",\"linecolor\":\"#EBF0F8\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"#EBF0F8\",\"zerolinewidth\":2},\"yaxis\":{\"automargin\":true,\"gridcolor\":\"#EBF0F8\",\"linecolor\":\"#EBF0F8\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"#EBF0F8\",\"zerolinewidth\":2}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"Recall\"},\"constrain\":\"domain\"},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"Precision\"},\"scaleanchor\":\"x\",\"scaleratio\":1},\"legend\":{\"tracegroupgap\":0},\"title\":{\"text\":\"Precision-Recall Curve (AUC=0.8558)\"},\"height\":600,\"width\":800,\"shapes\":[{\"line\":{\"dash\":\"dash\"},\"type\":\"line\",\"x0\":0,\"x1\":1,\"y0\":1,\"y1\":0}],\"showlegend\":false},                        {\"responsive\": true}                    ).then(function(){\n","                            \n","var gd = document.getElementById('1e149bf0-bc6a-4179-a047-3b046fd1f79c');\n","var x = new MutationObserver(function (mutations, observer) {{\n","        var display = window.getComputedStyle(gd).display;\n","        if (!display || display === 'none') {{\n","            console.log([gd, 'removed!']);\n","            Plotly.purge(gd);\n","            observer.disconnect();\n","        }}\n","}});\n","\n","// Listen for the removal of the full notebook cells\n","var notebookContainer = gd.closest('#notebook-container');\n","if (notebookContainer) {{\n","    x.observe(notebookContainer, {childList: true});\n","}}\n","\n","// Listen for the clearing of the current output cell\n","var outputEl = gd.closest('.output');\n","if (outputEl) {{\n","    x.observe(outputEl, {childList: true});\n","}}\n","\n","                        })                };                });            </script>        </div>"]},"metadata":{},"output_type":"display_data"}],"source":["target_score = log_clf.predict_proba(features_valid)[:, 1]\n","\n","fpr, tpr, thresholds = roc_curve(target_valid, target_score)\n","\n","fig = px.area(\n","    x=fpr, y=tpr,\n","    title=f'ROC Curve (AUC={auc(fpr, tpr):.4f})',\n","    labels=dict(x='False Positive Rate', y='True Positive Rate'),\n","    width=800, height=600\n",")\n","fig.add_shape(\n","    type='line', line=dict(dash='dash'),\n","    x0=0, x1=1, y0=0, y1=1\n",")\n","\n","fig.update_yaxes(scaleanchor=\"x\", scaleratio=1)\n","fig.update_xaxes(constrain='domain')\n","fig.update_layout(showlegend=False)\n","fig.show()\n","\n","precision, recall, thresholds = precision_recall_curve(target_valid, target_score)\n","\n","fig = px.area(\n","    x=recall, y=precision,\n","    title=f'Precision-Recall Curve (AUC={auc(fpr, tpr):.4f})',\n","    labels=dict(x='Recall', y='Precision'),\n","    width=800, height=600\n",")\n","fig.add_shape(\n","    type='line', line=dict(dash='dash'),\n","    x0=0, x1=1, y0=1, y1=0\n",")\n","fig.update_yaxes(scaleanchor=\"x\", scaleratio=1)\n","fig.update_xaxes(constrain='domain')\n","fig.update_layout(showlegend=False)\n","fig.show()"]},{"cell_type":"markdown","metadata":{},"source":["-------"]},{"cell_type":"markdown","metadata":{},"source":["# Ridge Classification"]},{"cell_type":"code","execution_count":126,"metadata":{"execution":{"iopub.execute_input":"2023-11-30T16:53:45.727614Z","iopub.status.busy":"2023-11-30T16:53:45.727326Z","iopub.status.idle":"2023-11-30T16:53:53.993580Z","shell.execute_reply":"2023-11-30T16:53:53.992668Z","shell.execute_reply.started":"2023-11-30T16:53:45.727589Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Runtime:\n"]}],"source":["ridge_model = RidgeClassifier(random_state=random_state)\n","ridge_parameters = [{\"alpha\": list(range(0,1000)), \n","                     \"fit_intercept\": [True, False], \n","                     \"solver\": ['svd', 'cholesky', 'lsqr', 'sparse_cg', 'sag', 'saga']}]\n","\n","ridge_clf = RandomizedSearchCV(ridge_model, ridge_parameters, scoring='roc_auc', n_jobs=-1, cv=cv)\n","ridge_clf.fit(features_train, target_train)\n","# create a variable for the best model\n","best_ridge = ridge_clf.best_estimator_\n","ridge_pred = best_ridge.predict(features_valid)\n","print('Runtime:')"]},{"cell_type":"code","execution_count":127,"metadata":{"execution":{"iopub.execute_input":"2023-11-30T16:53:53.995997Z","iopub.status.busy":"2023-11-30T16:53:53.995322Z","iopub.status.idle":"2023-11-30T16:53:55.327974Z","shell.execute_reply":"2023-11-30T16:53:55.326745Z","shell.execute_reply.started":"2023-11-30T16:53:53.995962Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Cross Validation Scores: [0.84617808 0.83467742 0.85218894 0.77952189 0.81604263 0.84153226\n"," 0.8        0.80063364 0.85821197 0.83434466 0.8157631  0.80884217\n"," 0.82825461 0.81782834 0.85132488 0.81730991 0.83090438 0.80095046\n"," 0.841684   0.83287101 0.78468941 0.83548387 0.85538594 0.83084677\n"," 0.82272465 0.84720622 0.83977535 0.82825461 0.82033056 0.78730351\n"," 0.82587278 0.8155818  0.81201037 0.83666475 0.84881912 0.85192972\n"," 0.80815092 0.80918779 0.83434466 0.82350902 0.81942893 0.82675691\n"," 0.81466014 0.79795507 0.83372696 0.8249712  0.85567396 0.83231567\n"," 0.83394013 0.831773  ]\n"]}],"source":["ridge_scores = cross_val_score(ridge_model, features_train, target_train, cv=cv, scoring='roc_auc')\n","print('Cross Validation Scores: {}'.format(ridge_scores))"]},{"cell_type":"code","execution_count":128,"metadata":{"execution":{"iopub.execute_input":"2023-11-30T16:53:55.330829Z","iopub.status.busy":"2023-11-30T16:53:55.329727Z","iopub.status.idle":"2023-11-30T16:53:55.339315Z","shell.execute_reply":"2023-11-30T16:53:55.338321Z","shell.execute_reply.started":"2023-11-30T16:53:55.330795Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Best hyperparameters: {'solver': 'sparse_cg', 'fit_intercept': True, 'alpha': 331}\n","\n","Best score: 0.8246271047773726\n","\n","Average Cross Validation Score: 0.8259667628691291\n"]}],"source":["# summary\n","print('Best hyperparameters:',  ridge_clf.best_params_)\n","print()\n","print('Best score:',  ridge_clf.best_score_)\n","print()\n","print('Average Cross Validation Score: {}'.format(ridge_scores.mean()))"]},{"cell_type":"markdown","metadata":{},"source":["-------"]},{"cell_type":"markdown","metadata":{},"source":["# LightGBM"]},{"cell_type":"code","execution_count":129,"metadata":{"execution":{"iopub.execute_input":"2023-11-30T16:53:55.342580Z","iopub.status.busy":"2023-11-30T16:53:55.341944Z","iopub.status.idle":"2023-11-30T16:54:28.984603Z","shell.execute_reply":"2023-11-30T16:54:28.983717Z","shell.execute_reply.started":"2023-11-30T16:53:55.342548Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["[LightGBM] [Warning] feature_fraction is set=0.2, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2\n","[LightGBM] [Warning] bagging_fraction is set=1, subsample=1.0 will be ignored. Current value: bagging_fraction=1\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Warning] feature_fraction is set=0.2, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2\n","[LightGBM] [Warning] bagging_fraction is set=1, subsample=1.0 will be ignored. Current value: bagging_fraction=1\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Warning] feature_fraction is set=0.2, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2\n","[LightGBM] [Warning] bagging_fraction is set=1, subsample=1.0 will be ignored. Current value: bagging_fraction=1\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Warning] feature_fraction is set=0.2, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2\n","[LightGBM] [Warning] bagging_fraction is set=1, subsample=1.0 will be ignored. Current value: bagging_fraction=1\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Warning] feature_fraction is set=0.2, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2\n","[LightGBM] [Warning] bagging_fraction is set=1, subsample=1.0 will be ignored. Current value: bagging_fraction=1\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Warning] feature_fraction is set=0.2, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2\n","[LightGBM] [Warning] bagging_fraction is set=1, subsample=1.0 will be ignored. Current value: bagging_fraction=1\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Warning] feature_fraction is set=0.2, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2\n","[LightGBM] [Warning] bagging_fraction is set=1, subsample=1.0 will be ignored. Current value: bagging_fraction=1\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Warning] feature_fraction is set=0.2, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2\n","[LightGBM] [Warning] bagging_fraction is set=1, subsample=1.0 will be ignored. Current value: bagging_fraction=1\n","[LightGBM] [Warning] feature_fraction is set=0.2, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2\n","[LightGBM] [Warning] bagging_fraction is set=1, subsample=1.0 will be ignored. Current value: bagging_fraction=1\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Warning] feature_fraction is set=0.2, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2\n","[LightGBM] [Warning] bagging_fraction is set=1, subsample=1.0 will be ignored. Current value: bagging_fraction=1\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Warning] feature_fraction is set=0.2, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2\n","[LightGBM] [Warning] bagging_fraction is set=1, subsample=1.0 will be ignored. Current value: bagging_fraction=1\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Info] Number of positive: 2788, number of negative: 1008\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Warning] feature_fraction is set=0.2, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2\n","[LightGBM] [Warning] bagging_fraction is set=1, subsample=1.0 will be ignored. Current value: bagging_fraction=1\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Warning] feature_fraction is set=0.2, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2\n","[LightGBM] [Warning] bagging_fraction is set=1, subsample=1.0 will be ignored. Current value: bagging_fraction=1\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Warning] feature_fraction is set=0.2, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2\n","[LightGBM] [Warning] bagging_fraction is set=1, subsample=1.0 will be ignored. Current value: bagging_fraction=1\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008893 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008870 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 570[LightGBM] [Info] Total Bins 570\n","\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008522 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.009419 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.009433 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 570[LightGBM] [Info] Total Bins 570\n","\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.009378 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006\n","\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734457 -> initscore=1.017356\n","[LightGBM] [Info] Start training from score 1.016006\n","[LightGBM] [Info] Start training from score 1.016006\n","[LightGBM] [Info] Start training from score 1.016006\n","[LightGBM] [Info] Start training from score 1.016006\n","[LightGBM] [Info] Start training from score 1.016006\n","[LightGBM] [Info] Start training from score 1.017356\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005340 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006\n","[LightGBM] [Info] Start training from score 1.016006\n","[LightGBM] [Warning] feature_fraction is set=0.2, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2\n","[LightGBM] [Warning] bagging_fraction is set=1, subsample=1.0 will be ignored. Current value: bagging_fraction=1\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Warning] feature_fraction is set=0.2, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2\n","[LightGBM] [Warning] bagging_fraction is set=1, subsample=1.0 will be ignored. Current value: bagging_fraction=1\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000788 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006\n","[LightGBM] [Info] Start training from score 1.016006\n","[LightGBM] [Warning] feature_fraction is set=0.2, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2\n","[LightGBM] [Warning] bagging_fraction is set=1, subsample=1.0 will be ignored. Current value: bagging_fraction=1\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Warning] feature_fraction is set=0.2, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2\n","[LightGBM] [Warning] bagging_fraction is set=1, subsample=1.0 will be ignored. Current value: bagging_fraction=1\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Warning] feature_fraction is set=0.2, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2\n","[LightGBM] [Warning] bagging_fraction is set=1, subsample=1.0 will be ignored. Current value: bagging_fraction=1\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Info] Number of positive: 2788, number of negative: 1008\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000983 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734457 -> initscore=1.017356\n","[LightGBM] [Info] Start training from score 1.017356\n","[LightGBM] [Warning] feature_fraction is set=0.2, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2\n","[LightGBM] [Warning] bagging_fraction is set=1, subsample=1.0 will be ignored. Current value: bagging_fraction=1\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Warning] feature_fraction is set=0.2, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2\n","[LightGBM] [Warning] bagging_fraction is set=1, subsample=1.0 will be ignored. Current value: bagging_fraction=1\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Warning] feature_fraction is set=0.2, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2\n","[LightGBM] [Warning] bagging_fraction is set=1, subsample=1.0 will be ignored. Current value: bagging_fraction=1\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003119 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006\n","[LightGBM] [Info] Start training from score 1.016006\n","[LightGBM] [Warning] feature_fraction is set=0.2, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2\n","[LightGBM] [Warning] bagging_fraction is set=1, subsample=1.0 will be ignored. Current value: bagging_fraction=1\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Warning] feature_fraction is set=0.2, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2\n","[LightGBM] [Warning] bagging_fraction is set=1, subsample=1.0 will be ignored. Current value: bagging_fraction=1\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Warning] feature_fraction is set=0.2, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2\n","[LightGBM] [Warning] bagging_fraction is set=1, subsample=1.0 will be ignored. Current value: bagging_fraction=1\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Warning] feature_fraction is set=0.2, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2\n","[LightGBM] [Warning] bagging_fraction is set=1, subsample=1.0 will be ignored. Current value: bagging_fraction=1\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Warning] feature_fraction is set=0.2, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2\n","[LightGBM] [Warning] bagging_fraction is set=1, subsample=1.0 will be ignored. Current value: bagging_fraction=1\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001028 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006\n","[LightGBM] [Info] Start training from score 1.016006\n","[LightGBM] [Warning] feature_fraction is set=0.2, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2\n","[LightGBM] [Warning] bagging_fraction is set=1, subsample=1.0 will be ignored. Current value: bagging_fraction=1\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Warning] feature_fraction is set=0.2, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2\n","[LightGBM] [Warning] bagging_fraction is set=1, subsample=1.0 will be ignored. Current value: bagging_fraction=1\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Warning] feature_fraction is set=0.2, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2\n","[LightGBM] [Warning] bagging_fraction is set=1, subsample=1.0 will be ignored. Current value: bagging_fraction=1\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Warning] feature_fraction is set=0.2, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2\n","[LightGBM] [Warning] bagging_fraction is set=1, subsample=1.0 will be ignored. Current value: bagging_fraction=1\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Warning] feature_fraction is set=0.2, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2\n","[LightGBM] [Warning] bagging_fraction is set=1, subsample=1.0 will be ignored. Current value: bagging_fraction=1\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.014551 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006\n","[LightGBM] [Info] Start training from score 1.016006\n","[LightGBM] [Warning] feature_fraction is set=0.2, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2\n","[LightGBM] [Warning] bagging_fraction is set=1, subsample=1.0 will be ignored. Current value: bagging_fraction=1\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Warning] feature_fraction is set=0.2, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2\n","[LightGBM] [Warning] bagging_fraction is set=1, subsample=1.0 will be ignored. Current value: bagging_fraction=1\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Warning] feature_fraction is set=0.2, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2\n","[LightGBM] [Warning] bagging_fraction is set=1, subsample=1.0 will be ignored. Current value: bagging_fraction=1\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001752 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Warning] feature_fraction is set=0.2, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2\n","[LightGBM] [Warning] bagging_fraction is set=1, subsample=1.0 will be ignored. Current value: bagging_fraction=1\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Warning] feature_fraction is set=0.2, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2\n","[LightGBM] [Warning] bagging_fraction is set=1, subsample=1.0 will be ignored. Current value: bagging_fraction=1\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004745 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006\n","[LightGBM] [Info] Start training from score 1.016006\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004008 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006\n","[LightGBM] [Info] Start training from score 1.016006\n","[LightGBM] [Warning] feature_fraction is set=0.2, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2\n","[LightGBM] [Warning] bagging_fraction is set=1, subsample=1.0 will be ignored. Current value: bagging_fraction=1\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006\n","[LightGBM] [Info] Start training from score 1.016006[LightGBM] [Warning] feature_fraction is set=0.2, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2\n","[LightGBM] [Warning] bagging_fraction is set=1, subsample=1.0 will be ignored. Current value: bagging_fraction=1\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","\n","[LightGBM] [Warning] feature_fraction is set=0.2, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2\n","[LightGBM] [Warning] bagging_fraction is set=1, subsample=1.0 will be ignored. Current value: bagging_fraction=1\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003438 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006\n","[LightGBM] [Info] Start training from score 1.016006\n","[LightGBM] [Warning] feature_fraction is set=0.2, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2\n","[LightGBM] [Warning] bagging_fraction is set=1, subsample=1.0 will be ignored. Current value: bagging_fraction=1\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Warning] feature_fraction is set=0.2, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2\n","[LightGBM] [Warning] bagging_fraction is set=1, subsample=1.0 will be ignored. Current value: bagging_fraction=1\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Warning] feature_fraction is set=0.2, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2\n","[LightGBM] [Warning] bagging_fraction is set=1, subsample=1.0 will be ignored. Current value: bagging_fraction=1\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Warning] feature_fraction is set=0.2, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2\n","[LightGBM] [Warning] bagging_fraction is set=1, subsample=1.0 will be ignored. Current value: bagging_fraction=1\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Warning] feature_fraction is set=0.2, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2\n","[LightGBM] [Warning] bagging_fraction is set=1, subsample=1.0 will be ignored. Current value: bagging_fraction=1\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Info] Number of positive: 2788, number of negative: 1009\n","[LightGBM] [Warning] feature_fraction is set=0.2, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2\n","[LightGBM] [Warning] bagging_fraction is set=1, subsample=1.0 will be ignored. Current value: bagging_fraction=1\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Info] Number of positive: 2788, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002663 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3797, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734264 -> initscore=1.016365\n","[LightGBM] [Info] Start training from score 1.016365\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007614 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3797, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734264 -> initscore=1.016365\n","[LightGBM] [Info] Start training from score 1.016365\n","[LightGBM] [Warning] feature_fraction is set=0.2, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2\n","[LightGBM] [Warning] bagging_fraction is set=1, subsample=1.0 will be ignored. Current value: bagging_fraction=1\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Warning] feature_fraction is set=0.2, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2\n","[LightGBM] [Warning] bagging_fraction is set=1, subsample=1.0 will be ignored. Current value: bagging_fraction=1\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Warning] feature_fraction is set=0.2, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2\n","[LightGBM] [Warning] bagging_fraction is set=1, subsample=1.0 will be ignored. Current value: bagging_fraction=1\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Info] Number of positive: 2788, number of negative: 1008\n","[LightGBM] [Warning] feature_fraction is set=0.2, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2\n","[LightGBM] [Warning] bagging_fraction is set=1, subsample=1.0 will be ignored. Current value: bagging_fraction=1\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Warning] feature_fraction is set=0.2, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2\n","[LightGBM] [Warning] bagging_fraction is set=1, subsample=1.0 will be ignored. Current value: bagging_fraction=1\n","[LightGBM] [Warning] feature_fraction is set=0.2, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2\n","[LightGBM] [Warning] bagging_fraction is set=1, subsample=1.0 will be ignored. Current value: bagging_fraction=1\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.038728 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Warning] feature_fraction is set=0.2, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2\n","[LightGBM] [Warning] bagging_fraction is set=1, subsample=1.0 will be ignored. Current value: bagging_fraction=1\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734457 -> initscore=1.017356\n","[LightGBM] [Warning] feature_fraction is set=0.2, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2\n","[LightGBM] [Warning] bagging_fraction is set=1, subsample=1.0 will be ignored. Current value: bagging_fraction=1\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Info] Start training from score 1.017356\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002779 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006\n","[LightGBM] [Info] Start training from score 1.016006\n","[LightGBM] [Warning] feature_fraction is set=0.2, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2\n","[LightGBM] [Warning] bagging_fraction is set=1, subsample=1.0 will be ignored. Current value: bagging_fraction=1\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Warning] feature_fraction is set=0.2, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2\n","[LightGBM] [Warning] bagging_fraction is set=1, subsample=1.0 will be ignored. Current value: bagging_fraction=1\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.014031 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Warning] feature_fraction is set=0.2, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2\n","[LightGBM] [Warning] bagging_fraction is set=1, subsample=1.0 will be ignored. Current value: bagging_fraction=1\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006\n","[LightGBM] [Info] Start training from score 1.016006\n","[LightGBM] [Warning] feature_fraction is set=0.2, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2\n","[LightGBM] [Warning] bagging_fraction is set=1, subsample=1.0 will be ignored. Current value: bagging_fraction=1\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Warning] feature_fraction is set=0.2, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2\n","[LightGBM] [Warning] bagging_fraction is set=1, subsample=1.0 will be ignored. Current value: bagging_fraction=1\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001716 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006\n","[LightGBM] [Info] Start training from score 1.016006\n","[LightGBM] [Warning] feature_fraction is set=0.2, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2\n","[LightGBM] [Warning] bagging_fraction is set=1, subsample=1.0 will be ignored. Current value: bagging_fraction=1\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Warning] feature_fraction is set=0.2, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2\n","[LightGBM] [Warning] bagging_fraction is set=1, subsample=1.0 will be ignored. Current value: bagging_fraction=1\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Warning] feature_fraction is set=0.2, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2\n","[LightGBM] [Warning] bagging_fraction is set=1, subsample=1.0 will be ignored. Current value: bagging_fraction=1\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Warning] feature_fraction is set=0.2, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2\n","[LightGBM] [Warning] bagging_fraction is set=1, subsample=1.0 will be ignored. Current value: bagging_fraction=1\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Warning] feature_fraction is set=0.2, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2\n","[LightGBM] [Warning] bagging_fraction is set=1, subsample=1.0 will be ignored. Current value: bagging_fraction=1\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003170 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006\n","[LightGBM] [Info] Start training from score 1.016006\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010347 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006\n","[LightGBM] [Info] Start training from score 1.016006\n","[LightGBM] [Warning] feature_fraction is set=0.2, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2\n","[LightGBM] [Warning] bagging_fraction is set=1, subsample=1.0 will be ignored. Current value: bagging_fraction=1\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Warning] feature_fraction is set=0.2, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2\n","[LightGBM] [Warning] bagging_fraction is set=1, subsample=1.0 will be ignored. Current value: bagging_fraction=1\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Warning] feature_fraction is set=0.2, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2\n","[LightGBM] [Warning] bagging_fraction is set=1, subsample=1.0 will be ignored. Current value: bagging_fraction=1\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001887 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006\n","[LightGBM] [Info] Start training from score 1.016006\n","[LightGBM] [Warning] feature_fraction is set=0.2, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2\n","[LightGBM] [Warning] bagging_fraction is set=1, subsample=1.0 will be ignored. Current value: bagging_fraction=1\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Warning] feature_fraction is set=0.2, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2\n","[LightGBM] [Warning] bagging_fraction is set=1, subsample=1.0 will be ignored. Current value: bagging_fraction=1\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Warning] feature_fraction is set=0.2, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2\n","[LightGBM] [Warning] bagging_fraction is set=1, subsample=1.0 will be ignored. Current value: bagging_fraction=1\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001684 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006\n","[LightGBM] [Info] Start training from score 1.016006\n","[LightGBM] [Warning] feature_fraction is set=0.2, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2\n","[LightGBM] [Warning] bagging_fraction is set=1, subsample=1.0 will be ignored. Current value: bagging_fraction=1\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Warning] feature_fraction is set=0.2, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2\n","[LightGBM] [Warning] bagging_fraction is set=1, subsample=1.0 will be ignored. Current value: bagging_fraction=1\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Warning] feature_fraction is set=0.2, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2\n","[LightGBM] [Warning] bagging_fraction is set=1, subsample=1.0 will be ignored. Current value: bagging_fraction=1\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Info] Number of positive: 2788, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001905 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3797, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734264 -> initscore=1.016365\n","[LightGBM] [Info] Start training from score 1.016365\n","[LightGBM] [Warning] feature_fraction is set=0.2, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2\n","[LightGBM] [Warning] bagging_fraction is set=1, subsample=1.0 will be ignored. Current value: bagging_fraction=1\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Warning] feature_fraction is set=0.2, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2\n","[LightGBM] [Warning] bagging_fraction is set=1, subsample=1.0 will be ignored. Current value: bagging_fraction=1\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Warning] feature_fraction is set=0.2, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2\n","[LightGBM] [Warning] bagging_fraction is set=1, subsample=1.0 will be ignored. Current value: bagging_fraction=1\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Info] Number of positive: 2788, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001045 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3797, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734264 -> initscore=1.016365\n","[LightGBM] [Info] Start training from score 1.016365\n","[LightGBM] [Warning] feature_fraction is set=0.2, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2\n","[LightGBM] [Warning] bagging_fraction is set=1, subsample=1.0 will be ignored. Current value: bagging_fraction=1\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Warning] feature_fraction is set=0.2, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2\n","[LightGBM] [Warning] bagging_fraction is set=1, subsample=1.0 will be ignored. Current value: bagging_fraction=1\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Warning] feature_fraction is set=0.2, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2\n","[LightGBM] [Warning] bagging_fraction is set=1, subsample=1.0 will be ignored. Current value: bagging_fraction=1\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Info] Number of positive: 2788, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000879 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3797, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734264 -> initscore=1.016365\n","[LightGBM] [Info] Start training from score 1.016365\n","[LightGBM] [Warning] feature_fraction is set=0.2, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2\n","[LightGBM] [Warning] bagging_fraction is set=1, subsample=1.0 will be ignored. Current value: bagging_fraction=1\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Warning] feature_fraction is set=0.2, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2\n","[LightGBM] [Warning] bagging_fraction is set=1, subsample=1.0 will be ignored. Current value: bagging_fraction=1\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Info] Number of positive: 2788, number of negative: 1008\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003540 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734457 -> initscore=1.017356\n","[LightGBM] [Info] Start training from score 1.017356\n","[LightGBM] [Warning] feature_fraction is set=0.2, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2\n","[LightGBM] [Warning] bagging_fraction is set=1, subsample=1.0 will be ignored. Current value: bagging_fraction=1\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Warning] feature_fraction is set=0.2, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2\n","[LightGBM] [Warning] bagging_fraction is set=1, subsample=1.0 will be ignored. Current value: bagging_fraction=1\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Warning] feature_fraction is set=0.2, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2\n","[LightGBM] [Warning] bagging_fraction is set=1, subsample=1.0 will be ignored. Current value: bagging_fraction=1\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001996 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006\n","[LightGBM] [Info] Start training from score 1.016006\n","[LightGBM] [Warning] feature_fraction is set=0.2, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2\n","[LightGBM] [Warning] bagging_fraction is set=1, subsample=1.0 will be ignored. Current value: bagging_fraction=1\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Warning] feature_fraction is set=0.2, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2\n","[LightGBM] [Warning] bagging_fraction is set=1, subsample=1.0 will be ignored. Current value: bagging_fraction=1\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Warning] feature_fraction is set=0.2, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2\n","[LightGBM] [Warning] bagging_fraction is set=1, subsample=1.0 will be ignored. Current value: bagging_fraction=1\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001147 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006\n","[LightGBM] [Info] Start training from score 1.016006\n","[LightGBM] [Warning] feature_fraction is set=0.2, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2\n","[LightGBM] [Warning] bagging_fraction is set=1, subsample=1.0 will be ignored. Current value: bagging_fraction=1\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Warning] feature_fraction is set=0.2, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2\n","[LightGBM] [Warning] bagging_fraction is set=1, subsample=1.0 will be ignored. Current value: bagging_fraction=1\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Warning] feature_fraction is set=0.2, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2\n","[LightGBM] [Warning] bagging_fraction is set=1, subsample=1.0 will be ignored. Current value: bagging_fraction=1\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004504 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006\n","[LightGBM] [Info] Start training from score 1.016006\n","[LightGBM] [Warning] feature_fraction is set=0.2, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2\n","[LightGBM] [Warning] bagging_fraction is set=1, subsample=1.0 will be ignored. Current value: bagging_fraction=1\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Warning] feature_fraction is set=0.2, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2\n","[LightGBM] [Warning] bagging_fraction is set=1, subsample=1.0 will be ignored. Current value: bagging_fraction=1\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Warning] feature_fraction is set=0.2, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2\n","[LightGBM] [Warning] bagging_fraction is set=1, subsample=1.0 will be ignored. Current value: bagging_fraction=1\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Warning] feature_fraction is set=0.2, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2\n","[LightGBM] [Warning] bagging_fraction is set=1, subsample=1.0 will be ignored. Current value: bagging_fraction=1\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Warning] feature_fraction is set=0.2, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2\n","[LightGBM] [Warning] bagging_fraction is set=1, subsample=1.0 will be ignored. Current value: bagging_fraction=1\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Warning] feature_fraction is set=0.2, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2\n","[LightGBM] [Warning] bagging_fraction is set=1, subsample=1.0 will be ignored. Current value: bagging_fraction=1\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Warning] feature_fraction is set=0.2, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2\n","[LightGBM] [Warning] bagging_fraction is set=1, subsample=1.0 will be ignored. Current value: bagging_fraction=1\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Warning] feature_fraction is set=0.2, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2\n","[LightGBM] [Warning] bagging_fraction is set=1, subsample=1.0 will be ignored. Current value: bagging_fraction=1\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002743 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006\n","[LightGBM] [Info] Start training from score 1.016006\n","[LightGBM] [Warning] feature_fraction is set=0.2, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2\n","[LightGBM] [Warning] bagging_fraction is set=1, subsample=1.0 will be ignored. Current value: bagging_fraction=1\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Warning] feature_fraction is set=0.2, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2\n","[LightGBM] [Warning] bagging_fraction is set=1, subsample=1.0 will be ignored. Current value: bagging_fraction=1\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Warning] feature_fraction is set=0.2, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2\n","[LightGBM] [Warning] bagging_fraction is set=1, subsample=1.0 will be ignored. Current value: bagging_fraction=1\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001199 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006\n","[LightGBM] [Info] Start training from score 1.016006\n","[LightGBM] [Warning] feature_fraction is set=0.2, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2\n","[LightGBM] [Warning] bagging_fraction is set=1, subsample=1.0 will be ignored. Current value: bagging_fraction=1\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.094962 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006\n","[LightGBM] [Info] Start training from score 1.016006\n","[LightGBM] [Warning] feature_fraction is set=0.2, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2\n","[LightGBM] [Warning] bagging_fraction is set=1, subsample=1.0 will be ignored. Current value: bagging_fraction=1\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Warning] feature_fraction is set=0.2, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2\n","[LightGBM] [Warning] bagging_fraction is set=1, subsample=1.0 will be ignored. Current value: bagging_fraction=1\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Warning] feature_fraction is set=0.2, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2\n","[LightGBM] [Warning] bagging_fraction is set=1, subsample=1.0 will be ignored. Current value: bagging_fraction=1\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Warning] feature_fraction is set=0.2, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2\n","[LightGBM] [Warning] bagging_fraction is set=1, subsample=1.0 will be ignored. Current value: bagging_fraction=1\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Warning] feature_fraction is set=0.2, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2\n","[LightGBM] [Warning] bagging_fraction is set=1, subsample=1.0 will be ignored. Current value: bagging_fraction=1\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Info] Number of positive: 2788, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001955 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Warning] feature_fraction is set=0.2, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2\n","[LightGBM] [Warning] bagging_fraction is set=1, subsample=1.0 will be ignored. Current value: bagging_fraction=1\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3797, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734264 -> initscore=1.016365\n","[LightGBM] [Info] Start training from score 1.016365\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.011284 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of positive: 2788, number of negative: 1009\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006\n","[LightGBM] [Info] Start training from score 1.016006\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.040774 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3797, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734264 -> initscore=1.016365\n","[LightGBM] [Info] Start training from score 1.016365\n","[LightGBM] [Warning] feature_fraction is set=0.2, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2\n","[LightGBM] [Warning] bagging_fraction is set=1, subsample=1.0 will be ignored. Current value: bagging_fraction=1\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Warning] feature_fraction is set=0.2, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2\n","[LightGBM] [Warning] bagging_fraction is set=1, subsample=1.0 will be ignored. Current value: bagging_fraction=1\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Warning] feature_fraction is set=0.2, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2\n","[LightGBM] [Warning] bagging_fraction is set=1, subsample=1.0 will be ignored. Current value: bagging_fraction=1\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Warning] feature_fraction is set=0.2, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2\n","[LightGBM] [Warning] bagging_fraction is set=1, subsample=1.0 will be ignored. Current value: bagging_fraction=1\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Info] Number of positive: 2788, number of negative: 1008\n","[LightGBM] [Warning] feature_fraction is set=0.2, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2\n","[LightGBM] [Warning] bagging_fraction is set=1, subsample=1.0 will be ignored. Current value: bagging_fraction=1\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Warning] feature_fraction is set=0.2, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2\n","[LightGBM] [Warning] bagging_fraction is set=1, subsample=1.0 will be ignored. Current value: bagging_fraction=1\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003578 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006\n","[LightGBM] [Info] Start training from score 1.016006\n","[LightGBM] [Warning] feature_fraction is set=0.2, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2\n","[LightGBM] [Warning] bagging_fraction is set=1, subsample=1.0 will be ignored. Current value: bagging_fraction=1\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Warning] feature_fraction is set=0.2, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2\n","[LightGBM] [Warning] bagging_fraction is set=1, subsample=1.0 will be ignored. Current value: bagging_fraction=1\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Warning] feature_fraction is set=0.2, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2\n","[LightGBM] [Warning] bagging_fraction is set=1, subsample=1.0 will be ignored. Current value: bagging_fraction=1\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.011722 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734457 -> initscore=1.017356\n","[LightGBM] [Info] Start training from score 1.017356\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.015131 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006\n","[LightGBM] [Info] Start training from score 1.016006\n","[LightGBM] [Warning] feature_fraction is set=0.2, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2\n","[LightGBM] [Warning] bagging_fraction is set=1, subsample=1.0 will be ignored. Current value: bagging_fraction=1\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Warning] feature_fraction is set=0.2, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2\n","[LightGBM] [Warning] bagging_fraction is set=1, subsample=1.0 will be ignored. Current value: bagging_fraction=1\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Warning] feature_fraction is set=0.2, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2\n","[LightGBM] [Warning] bagging_fraction is set=1, subsample=1.0 will be ignored. Current value: bagging_fraction=1\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003325 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006\n","[LightGBM] [Info] Start training from score 1.016006\n","[LightGBM] [Warning] feature_fraction is set=0.2, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2\n","[LightGBM] [Warning] bagging_fraction is set=1, subsample=1.0 will be ignored. Current value: bagging_fraction=1\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Warning] feature_fraction is set=0.2, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2\n","[LightGBM] [Warning] bagging_fraction is set=1, subsample=1.0 will be ignored. Current value: bagging_fraction=1\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Warning] feature_fraction is set=0.2, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2\n","[LightGBM] [Warning] bagging_fraction is set=1, subsample=1.0 will be ignored. Current value: bagging_fraction=1\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001326 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006\n","[LightGBM] [Info] Start training from score 1.016006\n","[LightGBM] [Warning] feature_fraction is set=0.2, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2\n","[LightGBM] [Warning] bagging_fraction is set=1, subsample=1.0 will be ignored. Current value: bagging_fraction=1\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Warning] feature_fraction is set=0.2, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2\n","[LightGBM] [Warning] bagging_fraction is set=1, subsample=1.0 will be ignored. Current value: bagging_fraction=1\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Warning] feature_fraction is set=0.2, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2\n","[LightGBM] [Warning] bagging_fraction is set=1, subsample=1.0 will be ignored. Current value: bagging_fraction=1\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Warning] feature_fraction is set=0.2, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2\n","[LightGBM] [Warning] bagging_fraction is set=1, subsample=1.0 will be ignored. Current value: bagging_fraction=1\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Info] Number of positive: 2788, number of negative: 1009\n","[LightGBM] [Warning] feature_fraction is set=0.2, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2\n","[LightGBM] [Warning] bagging_fraction is set=1, subsample=1.0 will be ignored. Current value: bagging_fraction=1\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000592 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3797, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734264 -> initscore=1.016365\n","[LightGBM] [Info] Start training from score 1.016365\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005054 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006\n","[LightGBM] [Info] Start training from score 1.016006\n","[LightGBM] [Warning] feature_fraction is set=0.2, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2\n","[LightGBM] [Warning] bagging_fraction is set=1, subsample=1.0 will be ignored. Current value: bagging_fraction=1\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Warning] feature_fraction is set=0.2, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2\n","[LightGBM] [Warning] bagging_fraction is set=1, subsample=1.0 will be ignored. Current value: bagging_fraction=1\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Warning] feature_fraction is set=0.2, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2\n","[LightGBM] [Warning] bagging_fraction is set=1, subsample=1.0 will be ignored. Current value: bagging_fraction=1\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002881 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Warning] feature_fraction is set=0.2, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2\n","[LightGBM] [Warning] bagging_fraction is set=1, subsample=1.0 will be ignored. Current value: bagging_fraction=1\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006\n","[LightGBM] [Info] Start training from score 1.016006\n","[LightGBM] [Warning] feature_fraction is set=0.2, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2\n","[LightGBM] [Warning] bagging_fraction is set=1, subsample=1.0 will be ignored. Current value: bagging_fraction=1\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Warning] feature_fraction is set=0.2, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2\n","[LightGBM] [Warning] bagging_fraction is set=1, subsample=1.0 will be ignored. Current value: bagging_fraction=1\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003075 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006\n","[LightGBM] [Info] Start training from score 1.016006\n","[LightGBM] [Warning] feature_fraction is set=0.2, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2\n","[LightGBM] [Warning] bagging_fraction is set=1, subsample=1.0 will be ignored. Current value: bagging_fraction=1\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Warning] feature_fraction is set=0.2, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2\n","[LightGBM] [Warning] bagging_fraction is set=1, subsample=1.0 will be ignored. Current value: bagging_fraction=1\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Warning] feature_fraction is set=0.2, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2\n","[LightGBM] [Warning] bagging_fraction is set=1, subsample=1.0 will be ignored. Current value: bagging_fraction=1\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Info] Number of positive: 2788, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003089 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3797, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734264 -> initscore=1.016365\n","[LightGBM] [Info] Start training from score 1.016365\n","[LightGBM] [Warning] feature_fraction is set=0.2, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2\n","[LightGBM] [Warning] bagging_fraction is set=1, subsample=1.0 will be ignored. Current value: bagging_fraction=1\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Warning] feature_fraction is set=0.2, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2\n","[LightGBM] [Warning] bagging_fraction is set=1, subsample=1.0 will be ignored. Current value: bagging_fraction=1\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Warning] feature_fraction is set=0.2, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2\n","[LightGBM] [Warning] bagging_fraction is set=1, subsample=1.0 will be ignored. Current value: bagging_fraction=1\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Info] Number of positive: 2788, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003329 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3797, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734264 -> initscore=1.016365\n","[LightGBM] [Info] Start training from score 1.016365\n","[LightGBM] [Warning] feature_fraction is set=0.2, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2\n","[LightGBM] [Warning] bagging_fraction is set=1, subsample=1.0 will be ignored. Current value: bagging_fraction=1\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Info] Number of positive: 2788, number of negative: 1008\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002133 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734457 -> initscore=1.017356\n","[LightGBM] [Info] Start training from score 1.017356\n","[LightGBM] [Warning] feature_fraction is set=0.2, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2\n","[LightGBM] [Warning] bagging_fraction is set=1, subsample=1.0 will be ignored. Current value: bagging_fraction=1\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Warning] feature_fraction is set=0.2, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2\n","[LightGBM] [Warning] bagging_fraction is set=1, subsample=1.0 will be ignored. Current value: bagging_fraction=1\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001699 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006\n","[LightGBM] [Info] Start training from score 1.016006\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002447 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006\n","[LightGBM] [Info] Start training from score 1.016006\n","[LightGBM] [Warning] feature_fraction is set=0.2, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2\n","[LightGBM] [Warning] bagging_fraction is set=1, subsample=1.0 will be ignored. Current value: bagging_fraction=1\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003290 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006\n","[LightGBM] [Info] Start training from score 1.016006\n","[LightGBM] [Warning] feature_fraction is set=0.2, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2\n","[LightGBM] [Warning] bagging_fraction is set=1, subsample=1.0 will be ignored. Current value: bagging_fraction=1\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002991 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006\n","[LightGBM] [Info] Start training from score 1.016006\n","[LightGBM] [Warning] feature_fraction is set=0.2, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2\n","[LightGBM] [Warning] bagging_fraction is set=1, subsample=1.0 will be ignored. Current value: bagging_fraction=1\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Warning] feature_fraction is set=0.2, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2\n","[LightGBM] [Warning] bagging_fraction is set=1, subsample=1.0 will be ignored. Current value: bagging_fraction=1\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002912 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006\n","[LightGBM] [Info] Start training from score 1.016006\n","[LightGBM] [Warning] feature_fraction is set=0.2, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2\n","[LightGBM] [Warning] bagging_fraction is set=1, subsample=1.0 will be ignored. Current value: bagging_fraction=1\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Warning] feature_fraction is set=0.2, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2\n","[LightGBM] [Warning] bagging_fraction is set=1, subsample=1.0 will be ignored. Current value: bagging_fraction=1\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001560 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006\n","[LightGBM] [Info] Start training from score 1.016006\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Warning] feature_fraction is set=0.2, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2\n","[LightGBM] [Warning] bagging_fraction is set=1, subsample=1.0 will be ignored. Current value: bagging_fraction=1\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Info] Number of positive: 2788, number of negative: 1009\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010551 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006\n","[LightGBM] [Info] Start training from score 1.016006\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.088933 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3797, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734264 -> initscore=1.016365\n","[LightGBM] [Info] Start training from score 1.016365\n","[LightGBM] [Info] Number of positive: 2788, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013241 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3797, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734264 -> initscore=1.016365\n","[LightGBM] [Info] Start training from score 1.016365\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Info] Number of positive: 2788, number of negative: 1008\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003267 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734457 -> initscore=1.017356\n","[LightGBM] [Info] Start training from score 1.017356\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003189 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006\n","[LightGBM] [Info] Start training from score 1.016006\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001958 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006\n","[LightGBM] [Info] Start training from score 1.016006\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001632 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006\n","[LightGBM] [Info] Start training from score 1.016006\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002046 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006\n","[LightGBM] [Info] Start training from score 1.016006\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001739 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006\n","[LightGBM] [Info] Start training from score 1.016006\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001851 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006\n","[LightGBM] [Info] Start training from score 1.016006\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Info] Number of positive: 2788, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000977 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3797, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734264 -> initscore=1.016365\n","[LightGBM] [Info] Start training from score 1.016365\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012644 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006\n","[LightGBM] [Info] Start training from score 1.016006\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Info] Number of positive: 2788, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004590 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3797, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734264 -> initscore=1.016365\n","[LightGBM] [Info] Start training from score 1.016365\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Info] Number of positive: 2788, number of negative: 1008\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004323 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734457 -> initscore=1.017356\n","[LightGBM] [Info] Start training from score 1.017356\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006877 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006\n","[LightGBM] [Info] Start training from score 1.016006\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007828 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006\n","[LightGBM] [Info] Start training from score 1.016006\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002231 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006\n","[LightGBM] [Info] Start training from score 1.016006\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004185 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006\n","[LightGBM] [Info] Start training from score 1.016006\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001540 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006\n","[LightGBM] [Info] Start training from score 1.016006\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007680 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004489 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006\n","[LightGBM] [Info] Start training from score 1.016006\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006\n","[LightGBM] [Info] Start training from score 1.016006\n","[LightGBM] [Info] Number of positive: 2788, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003645 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3797, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734264 -> initscore=1.016365\n","[LightGBM] [Info] Start training from score 1.016365\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Info] Number of positive: 2788, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002464 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3797, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734264 -> initscore=1.016365\n","[LightGBM] [Info] Start training from score 1.016365\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001476 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006\n","[LightGBM] [Info] Start training from score 1.016006\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Info] Number of positive: 2788, number of negative: 1008\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001193 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734457 -> initscore=1.017356\n","[LightGBM] [Info] Start training from score 1.017356\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010302 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006\n","[LightGBM] [Info] Start training from score 1.016006\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.127382 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006\n","[LightGBM] [Info] Start training from score 1.016006\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001900 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006\n","[LightGBM] [Info] Start training from score 1.016006\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002280 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006\n","[LightGBM] [Info] Start training from score 1.016006\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001042 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006\n","[LightGBM] [Info] Start training from score 1.016006\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002277 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006\n","[LightGBM] [Info] Start training from score 1.016006\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Info] Number of positive: 2788, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002923 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3797, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734264 -> initscore=1.016365\n","[LightGBM] [Info] Start training from score 1.016365\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Info] Number of positive: 2788, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001935 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3797, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734264 -> initscore=1.016365\n","[LightGBM] [Info] Start training from score 1.016365\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Info] Number of positive: 2788, number of negative: 1008\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002526 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734457 -> initscore=1.017356\n","[LightGBM] [Info] Start training from score 1.017356\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005758 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006\n","[LightGBM] [Info] Start training from score 1.016006\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001459 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006\n","[LightGBM] [Info] Start training from score 1.016006\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001841 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006\n","[LightGBM] [Info] Start training from score 1.016006\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.116637 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006\n","[LightGBM] [Info] Start training from score 1.016006\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002456 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006\n","[LightGBM] [Info] Start training from score 1.016006\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001730 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006\n","[LightGBM] [Info] Start training from score 1.016006\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001036 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006\n","[LightGBM] [Info] Start training from score 1.016006\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Info] Number of positive: 2788, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001787 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3797, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734264 -> initscore=1.016365\n","[LightGBM] [Info] Start training from score 1.016365\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Info] Number of positive: 2788, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002029 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3797, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734264 -> initscore=1.016365\n","[LightGBM] [Info] Start training from score 1.016365\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.1, subsample=1.0 will be ignored. Current value: bagging_fraction=0.1\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.1, subsample=1.0 will be ignored. Current value: bagging_fraction=0.1\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n","[LightGBM] [Info] Number of positive: 2788, number of negative: 1008\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000805 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734457 -> initscore=1.017356\n","[LightGBM] [Info] Start training from score 1.017356\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.1, subsample=1.0 will be ignored. Current value: bagging_fraction=0.1\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.1, subsample=1.0 will be ignored. Current value: bagging_fraction=0.1\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002000 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006\n","[LightGBM] [Info] Start training from score 1.016006\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.1, subsample=1.0 will be ignored. Current value: bagging_fraction=0.1\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.1, subsample=1.0 will be ignored. Current value: bagging_fraction=0.1\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.016641 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006\n","[LightGBM] [Info] Start training from score 1.016006\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.1, subsample=1.0 will be ignored. Current value: bagging_fraction=0.1\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.1, subsample=1.0 will be ignored. Current value: bagging_fraction=0.1\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001729 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006\n","[LightGBM] [Info] Start training from score 1.016006\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.1, subsample=1.0 will be ignored. Current value: bagging_fraction=0.1\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.1, subsample=1.0 will be ignored. Current value: bagging_fraction=0.1\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013406 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006\n","[LightGBM] [Info] Start training from score 1.016006\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.1, subsample=1.0 will be ignored. Current value: bagging_fraction=0.1\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.1, subsample=1.0 will be ignored. Current value: bagging_fraction=0.1\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000954 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006\n","[LightGBM] [Info] Start training from score 1.016006\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.1, subsample=1.0 will be ignored. Current value: bagging_fraction=0.1\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.1, subsample=1.0 will be ignored. Current value: bagging_fraction=0.1\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002115 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006\n","[LightGBM] [Info] Start training from score 1.016006\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.1, subsample=1.0 will be ignored. Current value: bagging_fraction=0.1\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.1, subsample=1.0 will be ignored. Current value: bagging_fraction=0.1\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001815 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006\n","[LightGBM] [Info] Start training from score 1.016006\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.1, subsample=1.0 will be ignored. Current value: bagging_fraction=0.1\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.1, subsample=1.0 will be ignored. Current value: bagging_fraction=0.1\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n","[LightGBM] [Info] Number of positive: 2788, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001855 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3797, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734264 -> initscore=1.016365\n","[LightGBM] [Info] Start training from score 1.016365\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.1, subsample=1.0 will be ignored. Current value: bagging_fraction=0.1\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.1, subsample=1.0 will be ignored. Current value: bagging_fraction=0.1\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n","[LightGBM] [Info] Number of positive: 2788, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001902 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3797, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734264 -> initscore=1.016365\n","[LightGBM] [Info] Start training from score 1.016365\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.1, subsample=1.0 will be ignored. Current value: bagging_fraction=0.1\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.1, subsample=1.0 will be ignored. Current value: bagging_fraction=0.1\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.1, subsample=1.0 will be ignored. Current value: bagging_fraction=0.1\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n","[LightGBM] [Info] Number of positive: 2788, number of negative: 1008\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001880 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734457 -> initscore=1.017356\n","[LightGBM] [Info] Start training from score 1.017356\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.1, subsample=1.0 will be ignored. Current value: bagging_fraction=0.1\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.1, subsample=1.0 will be ignored. Current value: bagging_fraction=0.1\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.1, subsample=1.0 will be ignored. Current value: bagging_fraction=0.1\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.009351 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006\n","[LightGBM] [Info] Start training from score 1.016006\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.1, subsample=1.0 will be ignored. Current value: bagging_fraction=0.1\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.1, subsample=1.0 will be ignored. Current value: bagging_fraction=0.1\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.1, subsample=1.0 will be ignored. Current value: bagging_fraction=0.1\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.1, subsample=1.0 will be ignored. Current value: bagging_fraction=0.1\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002423 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006\n","[LightGBM] [Info] Start training from score 1.016006\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.1, subsample=1.0 will be ignored. Current value: bagging_fraction=0.1\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.1, subsample=1.0 will be ignored. Current value: bagging_fraction=0.1\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.1, subsample=1.0 will be ignored. Current value: bagging_fraction=0.1\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.1, subsample=1.0 will be ignored. Current value: bagging_fraction=0.1\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001775 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.1, subsample=1.0 will be ignored. Current value: bagging_fraction=0.1\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Info] Start training from score 1.016006\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001876 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006\n","[LightGBM] [Info] Start training from score 1.016006\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.1, subsample=1.0 will be ignored. Current value: bagging_fraction=0.1\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.1, subsample=1.0 will be ignored. Current value: bagging_fraction=0.1\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.1, subsample=1.0 will be ignored. Current value: bagging_fraction=0.1\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.1, subsample=1.0 will be ignored. Current value: bagging_fraction=0.1\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n","\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003630 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006\n","[LightGBM] [Info] Start training from score 1.016006\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.1, subsample=1.0 will be ignored. Current value: bagging_fraction=0.1\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.1, subsample=1.0 will be ignored. Current value: bagging_fraction=0.1\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.033211 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006\n","[LightGBM] [Info] Start training from score 1.016006\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.1, subsample=1.0 will be ignored. Current value: bagging_fraction=0.1\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.1, subsample=1.0 will be ignored. Current value: bagging_fraction=0.1\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.1, subsample=1.0 will be ignored. Current value: bagging_fraction=0.1\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001811 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006\n","[LightGBM] [Info] Start training from score 1.016006\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.1, subsample=1.0 will be ignored. Current value: bagging_fraction=0.1\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.1, subsample=1.0 will be ignored. Current value: bagging_fraction=0.1\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.1, subsample=1.0 will be ignored. Current value: bagging_fraction=0.1\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n","[LightGBM] [Info] Number of positive: 2788, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004742 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3797, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734264 -> initscore=1.016365\n","[LightGBM] [Info] Start training from score 1.016365\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.1, subsample=1.0 will be ignored. Current value: bagging_fraction=0.1\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.1, subsample=1.0 will be ignored. Current value: bagging_fraction=0.1\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.1, subsample=1.0 will be ignored. Current value: bagging_fraction=0.1\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n","[LightGBM] [Info] Number of positive: 2788, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001826 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3797, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734264 -> initscore=1.016365\n","[LightGBM] [Info] Start training from score 1.016365\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.1, subsample=1.0 will be ignored. Current value: bagging_fraction=0.1\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.1, subsample=1.0 will be ignored. Current value: bagging_fraction=0.1\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.1, subsample=1.0 will be ignored. Current value: bagging_fraction=0.1\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n","[LightGBM] [Info] Number of positive: 2788, number of negative: 1008\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001697 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734457 -> initscore=1.017356\n","[LightGBM] [Info] Start training from score 1.017356\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.1, subsample=1.0 will be ignored. Current value: bagging_fraction=0.1\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.1, subsample=1.0 will be ignored. Current value: bagging_fraction=0.1\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.1, subsample=1.0 will be ignored. Current value: bagging_fraction=0.1\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001864 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006\n","[LightGBM] [Info] Start training from score 1.016006\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.1, subsample=1.0 will be ignored. Current value: bagging_fraction=0.1\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.1, subsample=1.0 will be ignored. Current value: bagging_fraction=0.1\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.1, subsample=1.0 will be ignored. Current value: bagging_fraction=0.1\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.1, subsample=1.0 will be ignored. Current value: bagging_fraction=0.1\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.027897 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006\n","[LightGBM] [Info] Start training from score 1.016006\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.1, subsample=1.0 will be ignored. Current value: bagging_fraction=0.1\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.1, subsample=1.0 will be ignored. Current value: bagging_fraction=0.1\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.015063 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006\n","[LightGBM] [Info] Start training from score 1.016006\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.1, subsample=1.0 will be ignored. Current value: bagging_fraction=0.1\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.1, subsample=1.0 will be ignored. Current value: bagging_fraction=0.1\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.1, subsample=1.0 will be ignored. Current value: bagging_fraction=0.1\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001827 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006\n","[LightGBM] [Info] Start training from score 1.016006\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.1, subsample=1.0 will be ignored. Current value: bagging_fraction=0.1\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.1, subsample=1.0 will be ignored. Current value: bagging_fraction=0.1\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.1, subsample=1.0 will be ignored. Current value: bagging_fraction=0.1\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002241 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006\n","[LightGBM] [Info] Start training from score 1.016006\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.1, subsample=1.0 will be ignored. Current value: bagging_fraction=0.1\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.1, subsample=1.0 will be ignored. Current value: bagging_fraction=0.1\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.1, subsample=1.0 will be ignored. Current value: bagging_fraction=0.1\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001583 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006\n","[LightGBM] [Info] Start training from score 1.016006\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.1, subsample=1.0 will be ignored. Current value: bagging_fraction=0.1\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.1, subsample=1.0 will be ignored. Current value: bagging_fraction=0.1\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.1, subsample=1.0 will be ignored. Current value: bagging_fraction=0.1\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.011114 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006\n","[LightGBM] [Info] Start training from score 1.016006\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.1, subsample=1.0 will be ignored. Current value: bagging_fraction=0.1\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.1, subsample=1.0 will be ignored. Current value: bagging_fraction=0.1\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.1, subsample=1.0 will be ignored. Current value: bagging_fraction=0.1\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n","[LightGBM] [Info] Number of positive: 2788, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002309 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3797, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734264 -> initscore=1.016365\n","[LightGBM] [Info] Start training from score 1.016365\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.1, subsample=1.0 will be ignored. Current value: bagging_fraction=0.1\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.1, subsample=1.0 will be ignored. Current value: bagging_fraction=0.1\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.1, subsample=1.0 will be ignored. Current value: bagging_fraction=0.1\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n","[LightGBM] [Info] Number of positive: 2788, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002708 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3797, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734264 -> initscore=1.016365\n","[LightGBM] [Info] Start training from score 1.016365\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.1, subsample=1.0 will be ignored. Current value: bagging_fraction=0.1\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.1, subsample=1.0 will be ignored. Current value: bagging_fraction=0.1\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.1, subsample=1.0 will be ignored. Current value: bagging_fraction=0.1\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.1, subsample=1.0 will be ignored. Current value: bagging_fraction=0.1\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.1, subsample=1.0 will be ignored. Current value: bagging_fraction=0.1\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.1, subsample=1.0 will be ignored. Current value: bagging_fraction=0.1\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n","[LightGBM] [Info] Number of positive: 2788, number of negative: 1008\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001558 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001823 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734457 -> initscore=1.017356\n","[LightGBM] [Info] Start training from score 1.017356\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006\n","[LightGBM] [Info] Start training from score 1.016006\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.1, subsample=1.0 will be ignored. Current value: bagging_fraction=0.1\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.1, subsample=1.0 will be ignored. Current value: bagging_fraction=0.1\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.1, subsample=1.0 will be ignored. Current value: bagging_fraction=0.1\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002861 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006\n","[LightGBM] [Info] Start training from score 1.016006\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.1, subsample=1.0 will be ignored. Current value: bagging_fraction=0.1\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.1, subsample=1.0 will be ignored. Current value: bagging_fraction=0.1\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.1, subsample=1.0 will be ignored. Current value: bagging_fraction=0.1\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.1, subsample=1.0 will be ignored. Current value: bagging_fraction=0.1\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.1, subsample=1.0 will be ignored. Current value: bagging_fraction=0.1\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.1, subsample=1.0 will be ignored. Current value: bagging_fraction=0.1\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002307 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006\n","[LightGBM] [Info] Start training from score 1.016006\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.038944 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006\n","[LightGBM] [Info] Start training from score 1.016006\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.1, subsample=1.0 will be ignored. Current value: bagging_fraction=0.1\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.1, subsample=1.0 will be ignored. Current value: bagging_fraction=0.1\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.1, subsample=1.0 will be ignored. Current value: bagging_fraction=0.1\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001452 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006\n","[LightGBM] [Info] Start training from score 1.016006\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.1, subsample=1.0 will be ignored. Current value: bagging_fraction=0.1\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.1, subsample=1.0 will be ignored. Current value: bagging_fraction=0.1\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.1, subsample=1.0 will be ignored. Current value: bagging_fraction=0.1\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001356 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006\n","[LightGBM] [Info] Start training from score 1.016006\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.1, subsample=1.0 will be ignored. Current value: bagging_fraction=0.1\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.1, subsample=1.0 will be ignored. Current value: bagging_fraction=0.1\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.1, subsample=1.0 will be ignored. Current value: bagging_fraction=0.1\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001941 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006\n","[LightGBM] [Info] Start training from score 1.016006\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.1, subsample=1.0 will be ignored. Current value: bagging_fraction=0.1\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.1, subsample=1.0 will be ignored. Current value: bagging_fraction=0.1\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.1, subsample=1.0 will be ignored. Current value: bagging_fraction=0.1\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n","[LightGBM] [Info] Number of positive: 2788, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.024473 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3797, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734264 -> initscore=1.016365\n","[LightGBM] [Info] Start training from score 1.016365\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.1, subsample=1.0 will be ignored. Current value: bagging_fraction=0.1\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.1, subsample=1.0 will be ignored. Current value: bagging_fraction=0.1\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.1, subsample=1.0 will be ignored. Current value: bagging_fraction=0.1\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n","[LightGBM] [Info] Number of positive: 2788, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002674 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3797, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734264 -> initscore=1.016365\n","[LightGBM] [Info] Start training from score 1.016365\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.1, subsample=1.0 will be ignored. Current value: bagging_fraction=0.1\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.1, subsample=1.0 will be ignored. Current value: bagging_fraction=0.1\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.1, subsample=1.0 will be ignored. Current value: bagging_fraction=0.1\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n","[LightGBM] [Info] Number of positive: 2788, number of negative: 1008\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001281 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734457 -> initscore=1.017356\n","[LightGBM] [Info] Start training from score 1.017356\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.1, subsample=1.0 will be ignored. Current value: bagging_fraction=0.1\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.1, subsample=1.0 will be ignored. Current value: bagging_fraction=0.1\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.1, subsample=1.0 will be ignored. Current value: bagging_fraction=0.1\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004944 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006\n","[LightGBM] [Info] Start training from score 1.016006\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.1, subsample=1.0 will be ignored. Current value: bagging_fraction=0.1\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.1, subsample=1.0 will be ignored. Current value: bagging_fraction=0.1\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.1, subsample=1.0 will be ignored. Current value: bagging_fraction=0.1\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002127 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006\n","[LightGBM] [Info] Start training from score 1.016006\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.1, subsample=1.0 will be ignored. Current value: bagging_fraction=0.1\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.1, subsample=1.0 will be ignored. Current value: bagging_fraction=0.1\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.1, subsample=1.0 will be ignored. Current value: bagging_fraction=0.1\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001960 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006\n","[LightGBM] [Info] Start training from score 1.016006\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.1, subsample=1.0 will be ignored. Current value: bagging_fraction=0.1\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.1, subsample=1.0 will be ignored. Current value: bagging_fraction=0.1\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.1, subsample=1.0 will be ignored. Current value: bagging_fraction=0.1\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003443 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006\n","[LightGBM] [Info] Start training from score 1.016006\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.1, subsample=1.0 will be ignored. Current value: bagging_fraction=0.1\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.1, subsample=1.0 will be ignored. Current value: bagging_fraction=0.1\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.1, subsample=1.0 will be ignored. Current value: bagging_fraction=0.1\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.1, subsample=1.0 will be ignored. Current value: bagging_fraction=0.1\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.1, subsample=1.0 will be ignored. Current value: bagging_fraction=0.1\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.1, subsample=1.0 will be ignored. Current value: bagging_fraction=0.1\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002728 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006\n","[LightGBM] [Info] Start training from score 1.016006\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.1, subsample=1.0 will be ignored. Current value: bagging_fraction=0.1\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.127143 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.1, subsample=1.0 will be ignored. Current value: bagging_fraction=0.1\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006\n","[LightGBM] [Info] Start training from score 1.016006\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.1, subsample=1.0 will be ignored. Current value: bagging_fraction=0.1\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003568 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006\n","[LightGBM] [Info] Start training from score 1.016006\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.1, subsample=1.0 will be ignored. Current value: bagging_fraction=0.1\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.1, subsample=1.0 will be ignored. Current value: bagging_fraction=0.1\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.1, subsample=1.0 will be ignored. Current value: bagging_fraction=0.1\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.1, subsample=1.0 will be ignored. Current value: bagging_fraction=0.1\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n","[LightGBM] [Info] Number of positive: 2788, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002387 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3797, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734264 -> initscore=1.016365\n","[LightGBM] [Info] Start training from score 1.016365\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.1, subsample=1.0 will be ignored. Current value: bagging_fraction=0.1\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.1, subsample=1.0 will be ignored. Current value: bagging_fraction=0.1\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n","[LightGBM] [Info] Number of positive: 2788, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002265 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3797, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734264 -> initscore=1.016365\n","[LightGBM] [Info] Start training from score 1.016365\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.1, subsample=1.0 will be ignored. Current value: bagging_fraction=0.1\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n","[LightGBM] [Warning] feature_fraction is set=1, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1\n","[LightGBM] [Warning] bagging_fraction is set=1, subsample=1.0 will be ignored. Current value: bagging_fraction=1\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] feature_fraction is set=1, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1\n","[LightGBM] [Warning] bagging_fraction is set=1, subsample=1.0 will be ignored. Current value: bagging_fraction=1\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Info] Number of positive: 2788, number of negative: 1008\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001623 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734457 -> initscore=1.017356\n","[LightGBM] [Info] Start training from score 1.017356\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.1, subsample=1.0 will be ignored. Current value: bagging_fraction=0.1\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n","[LightGBM] [Warning] feature_fraction is set=1, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1\n","[LightGBM] [Warning] bagging_fraction is set=1, subsample=1.0 will be ignored. Current value: bagging_fraction=1\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] feature_fraction is set=1, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1\n","[LightGBM] [Warning] bagging_fraction is set=1, subsample=1.0 will be ignored. Current value: bagging_fraction=1\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002553 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006\n","[LightGBM] [Info] Start training from score 1.016006\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.1, subsample=1.0 will be ignored. Current value: bagging_fraction=0.1\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n","[LightGBM] [Warning] feature_fraction is set=1, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1\n","[LightGBM] [Warning] bagging_fraction is set=1, subsample=1.0 will be ignored. Current value: bagging_fraction=1\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] feature_fraction is set=1, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1\n","[LightGBM] [Warning] bagging_fraction is set=1, subsample=1.0 will be ignored. Current value: bagging_fraction=1\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002047 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006\n","[LightGBM] [Info] Start training from score 1.016006\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.1, subsample=1.0 will be ignored. Current value: bagging_fraction=0.1\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n","[LightGBM] [Warning] feature_fraction is set=1, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1\n","[LightGBM] [Warning] bagging_fraction is set=1, subsample=1.0 will be ignored. Current value: bagging_fraction=1\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] feature_fraction is set=1, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1\n","[LightGBM] [Warning] bagging_fraction is set=1, subsample=1.0 will be ignored. Current value: bagging_fraction=1\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003874 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006\n","[LightGBM] [Info] Start training from score 1.016006\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.1, subsample=1.0 will be ignored. Current value: bagging_fraction=0.1\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n","[LightGBM] [Warning] feature_fraction is set=1, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1\n","[LightGBM] [Warning] bagging_fraction is set=1, subsample=1.0 will be ignored. Current value: bagging_fraction=1\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] feature_fraction is set=1, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1\n","[LightGBM] [Warning] bagging_fraction is set=1, subsample=1.0 will be ignored. Current value: bagging_fraction=1\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003198 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006\n","[LightGBM] [Info] Start training from score 1.016006\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.1, subsample=1.0 will be ignored. Current value: bagging_fraction=0.1\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.1, subsample=1.0 will be ignored. Current value: bagging_fraction=0.1\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n","[LightGBM] [Warning] feature_fraction is set=1, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1\n","[LightGBM] [Warning] bagging_fraction is set=1, subsample=1.0 will be ignored. Current value: bagging_fraction=1\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] feature_fraction is set=1, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1\n","[LightGBM] [Warning] bagging_fraction is set=1, subsample=1.0 will be ignored. Current value: bagging_fraction=1\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.1, subsample=1.0 will be ignored. Current value: bagging_fraction=0.1\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n","[LightGBM] [Warning] feature_fraction is set=1, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1\n","[LightGBM] [Warning] bagging_fraction is set=1, subsample=1.0 will be ignored. Current value: bagging_fraction=1\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] feature_fraction is set=1, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1\n","[LightGBM] [Warning] bagging_fraction is set=1, subsample=1.0 will be ignored. Current value: bagging_fraction=1\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] feature_fraction is set=1, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1\n","[LightGBM] [Warning] bagging_fraction is set=1, subsample=1.0 will be ignored. Current value: bagging_fraction=1\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002509 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Info] Start training from score 1.016006\n","[LightGBM] [Warning] feature_fraction is set=1, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1\n","[LightGBM] [Warning] bagging_fraction is set=1, subsample=1.0 will be ignored. Current value: bagging_fraction=1\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006650 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006\n","[LightGBM] [Info] Start training from score 1.016006\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.014422 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.1, subsample=1.0 will be ignored. Current value: bagging_fraction=0.1\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006\n","[LightGBM] [Info] Start training from score 1.016006\n","[LightGBM] [Warning] feature_fraction is set=1, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1\n","[LightGBM] [Warning] bagging_fraction is set=1, subsample=1.0 will be ignored. Current value: bagging_fraction=1\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] feature_fraction is set=1, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1\n","[LightGBM] [Warning] bagging_fraction is set=1, subsample=1.0 will be ignored. Current value: bagging_fraction=1\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Info] Number of positive: 2788, number of negative: 1009\n","[LightGBM] [Warning] feature_fraction is set=1, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1\n","[LightGBM] [Warning] bagging_fraction is set=1, subsample=1.0 will be ignored. Current value: bagging_fraction=1\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.111448 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3797, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734264 -> initscore=1.016365\n","[LightGBM] [Info] Start training from score 1.016365\n","[LightGBM] [Warning] feature_fraction is set=1, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1\n","[LightGBM] [Warning] bagging_fraction is set=1, subsample=1.0 will be ignored. Current value: bagging_fraction=1\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] feature_fraction is set=1, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1\n","[LightGBM] [Warning] bagging_fraction is set=1, subsample=1.0 will be ignored. Current value: bagging_fraction=1\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Info] Number of positive: 2788, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003202 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3797, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734264 -> initscore=1.016365\n","[LightGBM] [Info] Start training from score 1.016365\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.1, subsample=1.0 will be ignored. Current value: bagging_fraction=0.1\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n","[LightGBM] [Warning] feature_fraction is set=1, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1\n","[LightGBM] [Warning] bagging_fraction is set=1, subsample=1.0 will be ignored. Current value: bagging_fraction=1\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] feature_fraction is set=1, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1\n","[LightGBM] [Warning] bagging_fraction is set=1, subsample=1.0 will be ignored. Current value: bagging_fraction=1\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Info] Number of positive: 2788, number of negative: 1008\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.033079 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734457 -> initscore=1.017356\n","[LightGBM] [Info] Start training from score 1.017356\n","[LightGBM] [Warning] feature_fraction is set=1, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1\n","[LightGBM] [Warning] bagging_fraction is set=1, subsample=1.0 will be ignored. Current value: bagging_fraction=1\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] feature_fraction is set=1, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1\n","[LightGBM] [Warning] bagging_fraction is set=1, subsample=1.0 will be ignored. Current value: bagging_fraction=1\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] feature_fraction is set=1, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1\n","[LightGBM] [Warning] bagging_fraction is set=1, subsample=1.0 will be ignored. Current value: bagging_fraction=1\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003429 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006\n","[LightGBM] [Info] Start training from score 1.016006\n","[LightGBM] [Warning] feature_fraction is set=1, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1\n","[LightGBM] [Warning] bagging_fraction is set=1, subsample=1.0 will be ignored. Current value: bagging_fraction=1\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] feature_fraction is set=1, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1\n","[LightGBM] [Warning] bagging_fraction is set=1, subsample=1.0 will be ignored. Current value: bagging_fraction=1\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] feature_fraction is set=1, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1\n","[LightGBM] [Warning] bagging_fraction is set=1, subsample=1.0 will be ignored. Current value: bagging_fraction=1\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003622 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006\n","[LightGBM] [Info] Start training from score 1.016006\n","[LightGBM] [Warning] feature_fraction is set=1, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1\n","[LightGBM] [Warning] bagging_fraction is set=1, subsample=1.0 will be ignored. Current value: bagging_fraction=1\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] feature_fraction is set=1, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1\n","[LightGBM] [Warning] bagging_fraction is set=1, subsample=1.0 will be ignored. Current value: bagging_fraction=1\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] feature_fraction is set=1, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1\n","[LightGBM] [Warning] bagging_fraction is set=1, subsample=1.0 will be ignored. Current value: bagging_fraction=1\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003491 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006\n","[LightGBM] [Info] Start training from score 1.016006\n","[LightGBM] [Warning] feature_fraction is set=1, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1\n","[LightGBM] [Warning] bagging_fraction is set=1, subsample=1.0 will be ignored. Current value: bagging_fraction=1\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] feature_fraction is set=1, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1\n","[LightGBM] [Warning] bagging_fraction is set=1, subsample=1.0 will be ignored. Current value: bagging_fraction=1\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] feature_fraction is set=1, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1\n","[LightGBM] [Warning] bagging_fraction is set=1, subsample=1.0 will be ignored. Current value: bagging_fraction=1\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004701 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006\n","[LightGBM] [Info] Start training from score 1.016006\n","[LightGBM] [Warning] feature_fraction is set=1, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1\n","[LightGBM] [Warning] bagging_fraction is set=1, subsample=1.0 will be ignored. Current value: bagging_fraction=1\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] feature_fraction is set=1, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1\n","[LightGBM] [Warning] bagging_fraction is set=1, subsample=1.0 will be ignored. Current value: bagging_fraction=1\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] feature_fraction is set=1, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1\n","[LightGBM] [Warning] bagging_fraction is set=1, subsample=1.0 will be ignored. Current value: bagging_fraction=1\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002106 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006\n","[LightGBM] [Info] Start training from score 1.016006\n","[LightGBM] [Warning] feature_fraction is set=1, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1\n","[LightGBM] [Warning] bagging_fraction is set=1, subsample=1.0 will be ignored. Current value: bagging_fraction=1\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] feature_fraction is set=1, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1\n","[LightGBM] [Warning] bagging_fraction is set=1, subsample=1.0 will be ignored. Current value: bagging_fraction=1\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] feature_fraction is set=1, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1\n","[LightGBM] [Warning] bagging_fraction is set=1, subsample=1.0 will be ignored. Current value: bagging_fraction=1\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005302 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006\n","[LightGBM] [Info] Start training from score 1.016006\n","[LightGBM] [Warning] feature_fraction is set=1, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1\n","[LightGBM] [Warning] bagging_fraction is set=1, subsample=1.0 will be ignored. Current value: bagging_fraction=1\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] feature_fraction is set=1, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1\n","[LightGBM] [Warning] bagging_fraction is set=1, subsample=1.0 will be ignored. Current value: bagging_fraction=1\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] feature_fraction is set=1, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1\n","[LightGBM] [Warning] bagging_fraction is set=1, subsample=1.0 will be ignored. Current value: bagging_fraction=1\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.051950 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006\n","[LightGBM] [Info] Start training from score 1.016006\n","[LightGBM] [Warning] feature_fraction is set=1, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1\n","[LightGBM] [Warning] bagging_fraction is set=1, subsample=1.0 will be ignored. Current value: bagging_fraction=1\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] feature_fraction is set=1, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1\n","[LightGBM] [Warning] bagging_fraction is set=1, subsample=1.0 will be ignored. Current value: bagging_fraction=1\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] feature_fraction is set=1, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1\n","[LightGBM] [Warning] bagging_fraction is set=1, subsample=1.0 will be ignored. Current value: bagging_fraction=1\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Info] Number of positive: 2788, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004530 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3797, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734264 -> initscore=1.016365\n","[LightGBM] [Info] Start training from score 1.016365\n","[LightGBM] [Warning] feature_fraction is set=1, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1\n","[LightGBM] [Warning] bagging_fraction is set=1, subsample=1.0 will be ignored. Current value: bagging_fraction=1\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] feature_fraction is set=1, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1\n","[LightGBM] [Warning] bagging_fraction is set=1, subsample=1.0 will be ignored. Current value: bagging_fraction=1\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] feature_fraction is set=1, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1\n","[LightGBM] [Warning] bagging_fraction is set=1, subsample=1.0 will be ignored. Current value: bagging_fraction=1\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Info] Number of positive: 2788, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002809 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3797, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734264 -> initscore=1.016365\n","[LightGBM] [Info] Start training from score 1.016365\n","[LightGBM] [Warning] feature_fraction is set=1, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1\n","[LightGBM] [Warning] bagging_fraction is set=1, subsample=1.0 will be ignored. Current value: bagging_fraction=1\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] feature_fraction is set=1, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1\n","[LightGBM] [Warning] bagging_fraction is set=1, subsample=1.0 will be ignored. Current value: bagging_fraction=1\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] feature_fraction is set=1, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1\n","[LightGBM] [Warning] bagging_fraction is set=1, subsample=1.0 will be ignored. Current value: bagging_fraction=1\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Info] Number of positive: 2788, number of negative: 1008\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002744 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734457 -> initscore=1.017356\n","[LightGBM] [Info] Start training from score 1.017356\n","[LightGBM] [Warning] feature_fraction is set=1, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1\n","[LightGBM] [Warning] bagging_fraction is set=1, subsample=1.0 will be ignored. Current value: bagging_fraction=1\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] feature_fraction is set=1, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1\n","[LightGBM] [Warning] bagging_fraction is set=1, subsample=1.0 will be ignored. Current value: bagging_fraction=1\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] feature_fraction is set=1, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1\n","[LightGBM] [Warning] bagging_fraction is set=1, subsample=1.0 will be ignored. Current value: bagging_fraction=1\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000737 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006\n","[LightGBM] [Info] Start training from score 1.016006\n","[LightGBM] [Warning] feature_fraction is set=1, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1\n","[LightGBM] [Warning] bagging_fraction is set=1, subsample=1.0 will be ignored. Current value: bagging_fraction=1\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] feature_fraction is set=1, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1\n","[LightGBM] [Warning] bagging_fraction is set=1, subsample=1.0 will be ignored. Current value: bagging_fraction=1\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] feature_fraction is set=1, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1\n","[LightGBM] [Warning] bagging_fraction is set=1, subsample=1.0 will be ignored. Current value: bagging_fraction=1\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005074 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006\n","[LightGBM] [Info] Start training from score 1.016006\n","[LightGBM] [Warning] feature_fraction is set=1, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1\n","[LightGBM] [Warning] bagging_fraction is set=1, subsample=1.0 will be ignored. Current value: bagging_fraction=1\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] feature_fraction is set=1, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1\n","[LightGBM] [Warning] bagging_fraction is set=1, subsample=1.0 will be ignored. Current value: bagging_fraction=1\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] feature_fraction is set=1, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1\n","[LightGBM] [Warning] bagging_fraction is set=1, subsample=1.0 will be ignored. Current value: bagging_fraction=1\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003493 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006\n","[LightGBM] [Info] Start training from score 1.016006\n","[LightGBM] [Warning] feature_fraction is set=1, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1\n","[LightGBM] [Warning] bagging_fraction is set=1, subsample=1.0 will be ignored. Current value: bagging_fraction=1\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] feature_fraction is set=1, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1\n","[LightGBM] [Warning] bagging_fraction is set=1, subsample=1.0 will be ignored. Current value: bagging_fraction=1\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] feature_fraction is set=1, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1\n","[LightGBM] [Warning] bagging_fraction is set=1, subsample=1.0 will be ignored. Current value: bagging_fraction=1\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002858 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006\n","[LightGBM] [Info] Start training from score 1.016006\n","[LightGBM] [Warning] feature_fraction is set=1, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1\n","[LightGBM] [Warning] bagging_fraction is set=1, subsample=1.0 will be ignored. Current value: bagging_fraction=1\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] feature_fraction is set=1, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1\n","[LightGBM] [Warning] bagging_fraction is set=1, subsample=1.0 will be ignored. Current value: bagging_fraction=1\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] feature_fraction is set=1, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1\n","[LightGBM] [Warning] bagging_fraction is set=1, subsample=1.0 will be ignored. Current value: bagging_fraction=1\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003759 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006\n","[LightGBM] [Info] Start training from score 1.016006\n","[LightGBM] [Warning] feature_fraction is set=1, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1\n","[LightGBM] [Warning] bagging_fraction is set=1, subsample=1.0 will be ignored. Current value: bagging_fraction=1\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] feature_fraction is set=1, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1\n","[LightGBM] [Warning] bagging_fraction is set=1, subsample=1.0 will be ignored. Current value: bagging_fraction=1\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] feature_fraction is set=1, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1\n","[LightGBM] [Warning] bagging_fraction is set=1, subsample=1.0 will be ignored. Current value: bagging_fraction=1\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004985 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006\n","[LightGBM] [Info] Start training from score 1.016006\n","[LightGBM] [Warning] feature_fraction is set=1, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1\n","[LightGBM] [Warning] bagging_fraction is set=1, subsample=1.0 will be ignored. Current value: bagging_fraction=1\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] feature_fraction is set=1, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1\n","[LightGBM] [Warning] bagging_fraction is set=1, subsample=1.0 will be ignored. Current value: bagging_fraction=1\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] feature_fraction is set=1, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1\n","[LightGBM] [Warning] bagging_fraction is set=1, subsample=1.0 will be ignored. Current value: bagging_fraction=1\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008556 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006\n","[LightGBM] [Info] Start training from score 1.016006\n","[LightGBM] [Warning] feature_fraction is set=1, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1\n","[LightGBM] [Warning] bagging_fraction is set=1, subsample=1.0 will be ignored. Current value: bagging_fraction=1\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] feature_fraction is set=1, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1\n","[LightGBM] [Warning] bagging_fraction is set=1, subsample=1.0 will be ignored. Current value: bagging_fraction=1\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] feature_fraction is set=1, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1\n","[LightGBM] [Warning] bagging_fraction is set=1, subsample=1.0 will be ignored. Current value: bagging_fraction=1\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Info] Number of positive: 2788, number of negative: 1009\n","[LightGBM] [Warning] feature_fraction is set=1, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1\n","[LightGBM] [Warning] bagging_fraction is set=1, subsample=1.0 will be ignored. Current value: bagging_fraction=1\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007536 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3797, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734264 -> initscore=1.016365\n","[LightGBM] [Info] Start training from score 1.016365\n","[LightGBM] [Warning] feature_fraction is set=1, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1\n","[LightGBM] [Warning] bagging_fraction is set=1, subsample=1.0 will be ignored. Current value: bagging_fraction=1\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] feature_fraction is set=1, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1\n","[LightGBM] [Warning] bagging_fraction is set=1, subsample=1.0 will be ignored. Current value: bagging_fraction=1\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Info] Number of positive: 2788, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005119 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3797, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734264 -> initscore=1.016365\n","[LightGBM] [Info] Start training from score 1.016365\n","[LightGBM] [Warning] feature_fraction is set=1, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1\n","[LightGBM] [Warning] bagging_fraction is set=1, subsample=1.0 will be ignored. Current value: bagging_fraction=1\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] feature_fraction is set=1, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1\n","[LightGBM] [Warning] bagging_fraction is set=1, subsample=1.0 will be ignored. Current value: bagging_fraction=1\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] feature_fraction is set=1, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1\n","[LightGBM] [Warning] bagging_fraction is set=1, subsample=1.0 will be ignored. Current value: bagging_fraction=1\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Info] Number of positive: 2788, number of negative: 1008\n","[LightGBM] [Warning] feature_fraction is set=1, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1\n","[LightGBM] [Warning] bagging_fraction is set=1, subsample=1.0 will be ignored. Current value: bagging_fraction=1\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] feature_fraction is set=1, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1\n","[LightGBM] [Warning] bagging_fraction is set=1, subsample=1.0 will be ignored. Current value: bagging_fraction=1\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] feature_fraction is set=1, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1\n","[LightGBM] [Warning] bagging_fraction is set=1, subsample=1.0 will be ignored. Current value: bagging_fraction=1\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006713 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006\n","[LightGBM] [Info] Start training from score 1.016006\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004694 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734457 -> initscore=1.017356\n","[LightGBM] [Info] Start training from score 1.017356\n","[LightGBM] [Warning] feature_fraction is set=1, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1\n","[LightGBM] [Warning] bagging_fraction is set=1, subsample=1.0 will be ignored. Current value: bagging_fraction=1\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] feature_fraction is set=1, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1\n","[LightGBM] [Warning] bagging_fraction is set=1, subsample=1.0 will be ignored. Current value: bagging_fraction=1\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] feature_fraction is set=1, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1\n","[LightGBM] [Warning] bagging_fraction is set=1, subsample=1.0 will be ignored. Current value: bagging_fraction=1\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007971 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006\n","[LightGBM] [Info] Start training from score 1.016006\n","[LightGBM] [Warning] feature_fraction is set=1, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1\n","[LightGBM] [Warning] bagging_fraction is set=1, subsample=1.0 will be ignored. Current value: bagging_fraction=1\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] feature_fraction is set=1, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1\n","[LightGBM] [Warning] bagging_fraction is set=1, subsample=1.0 will be ignored. Current value: bagging_fraction=1\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] feature_fraction is set=1, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1\n","[LightGBM] [Warning] bagging_fraction is set=1, subsample=1.0 will be ignored. Current value: bagging_fraction=1\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004864 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006\n","[LightGBM] [Info] Start training from score 1.016006\n","[LightGBM] [Warning] feature_fraction is set=1, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1\n","[LightGBM] [Warning] bagging_fraction is set=1, subsample=1.0 will be ignored. Current value: bagging_fraction=1\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] feature_fraction is set=1, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1\n","[LightGBM] [Warning] bagging_fraction is set=1, subsample=1.0 will be ignored. Current value: bagging_fraction=1\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] feature_fraction is set=1, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1\n","[LightGBM] [Warning] bagging_fraction is set=1, subsample=1.0 will be ignored. Current value: bagging_fraction=1\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002386 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006\n","[LightGBM] [Info] Start training from score 1.016006\n","[LightGBM] [Warning] feature_fraction is set=1, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1\n","[LightGBM] [Warning] bagging_fraction is set=1, subsample=1.0 will be ignored. Current value: bagging_fraction=1\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] feature_fraction is set=1, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1\n","[LightGBM] [Warning] bagging_fraction is set=1, subsample=1.0 will be ignored. Current value: bagging_fraction=1\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] feature_fraction is set=1, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1\n","[LightGBM] [Warning] bagging_fraction is set=1, subsample=1.0 will be ignored. Current value: bagging_fraction=1\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004792 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006\n","[LightGBM] [Info] Start training from score 1.016006\n","[LightGBM] [Warning] feature_fraction is set=1, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1\n","[LightGBM] [Warning] bagging_fraction is set=1, subsample=1.0 will be ignored. Current value: bagging_fraction=1\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] feature_fraction is set=1, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1\n","[LightGBM] [Warning] bagging_fraction is set=1, subsample=1.0 will be ignored. Current value: bagging_fraction=1\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] feature_fraction is set=1, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1\n","[LightGBM] [Warning] bagging_fraction is set=1, subsample=1.0 will be ignored. Current value: bagging_fraction=1\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000683 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006\n","[LightGBM] [Info] Start training from score 1.016006\n","[LightGBM] [Warning] feature_fraction is set=1, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1\n","[LightGBM] [Warning] bagging_fraction is set=1, subsample=1.0 will be ignored. Current value: bagging_fraction=1\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] feature_fraction is set=1, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1\n","[LightGBM] [Warning] bagging_fraction is set=1, subsample=1.0 will be ignored. Current value: bagging_fraction=1\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] feature_fraction is set=1, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1\n","[LightGBM] [Warning] bagging_fraction is set=1, subsample=1.0 will be ignored. Current value: bagging_fraction=1\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004493 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006\n","[LightGBM] [Info] Start training from score 1.016006\n","[LightGBM] [Warning] feature_fraction is set=1, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1\n","[LightGBM] [Warning] bagging_fraction is set=1, subsample=1.0 will be ignored. Current value: bagging_fraction=1\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] feature_fraction is set=1, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1\n","[LightGBM] [Warning] bagging_fraction is set=1, subsample=1.0 will be ignored. Current value: bagging_fraction=1\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] feature_fraction is set=1, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1\n","[LightGBM] [Warning] bagging_fraction is set=1, subsample=1.0 will be ignored. Current value: bagging_fraction=1\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Info] Number of positive: 2788, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005195 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3797, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734264 -> initscore=1.016365\n","[LightGBM] [Info] Start training from score 1.016365\n","[LightGBM] [Warning] feature_fraction is set=1, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1\n","[LightGBM] [Warning] bagging_fraction is set=1, subsample=1.0 will be ignored. Current value: bagging_fraction=1\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] feature_fraction is set=1, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1\n","[LightGBM] [Warning] bagging_fraction is set=1, subsample=1.0 will be ignored. Current value: bagging_fraction=1\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] feature_fraction is set=1, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1\n","[LightGBM] [Warning] bagging_fraction is set=1, subsample=1.0 will be ignored. Current value: bagging_fraction=1\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Info] Number of positive: 2788, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002154 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3797, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734264 -> initscore=1.016365\n","[LightGBM] [Info] Start training from score 1.016365\n","[LightGBM] [Warning] feature_fraction is set=1, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1\n","[LightGBM] [Warning] bagging_fraction is set=1, subsample=1.0 will be ignored. Current value: bagging_fraction=1\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] feature_fraction is set=1, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1\n","[LightGBM] [Warning] bagging_fraction is set=1, subsample=1.0 will be ignored. Current value: bagging_fraction=1\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] feature_fraction is set=1, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1\n","[LightGBM] [Warning] bagging_fraction is set=1, subsample=1.0 will be ignored. Current value: bagging_fraction=1\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Info] Number of positive: 2788, number of negative: 1008\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001869 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734457 -> initscore=1.017356\n","[LightGBM] [Info] Start training from score 1.017356\n","[LightGBM] [Warning] feature_fraction is set=1, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1\n","[LightGBM] [Warning] bagging_fraction is set=1, subsample=1.0 will be ignored. Current value: bagging_fraction=1\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] feature_fraction is set=1, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1\n","[LightGBM] [Warning] bagging_fraction is set=1, subsample=1.0 will be ignored. Current value: bagging_fraction=1\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] feature_fraction is set=1, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1\n","[LightGBM] [Warning] bagging_fraction is set=1, subsample=1.0 will be ignored. Current value: bagging_fraction=1\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001168 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006\n","[LightGBM] [Info] Start training from score 1.016006\n","[LightGBM] [Warning] feature_fraction is set=1, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1\n","[LightGBM] [Warning] bagging_fraction is set=1, subsample=1.0 will be ignored. Current value: bagging_fraction=1\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] feature_fraction is set=1, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1\n","[LightGBM] [Warning] bagging_fraction is set=1, subsample=1.0 will be ignored. Current value: bagging_fraction=1\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] feature_fraction is set=1, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1\n","[LightGBM] [Warning] bagging_fraction is set=1, subsample=1.0 will be ignored. Current value: bagging_fraction=1\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003915 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006\n","[LightGBM] [Info] Start training from score 1.016006\n","[LightGBM] [Warning] feature_fraction is set=1, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1\n","[LightGBM] [Warning] bagging_fraction is set=1, subsample=1.0 will be ignored. Current value: bagging_fraction=1\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] feature_fraction is set=1, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1\n","[LightGBM] [Warning] bagging_fraction is set=1, subsample=1.0 will be ignored. Current value: bagging_fraction=1\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] feature_fraction is set=1, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1\n","[LightGBM] [Warning] bagging_fraction is set=1, subsample=1.0 will be ignored. Current value: bagging_fraction=1\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002012 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Warning] feature_fraction is set=1, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1\n","[LightGBM] [Warning] bagging_fraction is set=1, subsample=1.0 will be ignored. Current value: bagging_fraction=1\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] feature_fraction is set=1, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1\n","[LightGBM] [Warning] bagging_fraction is set=1, subsample=1.0 will be ignored. Current value: bagging_fraction=1\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] feature_fraction is set=1, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1\n","[LightGBM] [Warning] bagging_fraction is set=1, subsample=1.0 will be ignored. Current value: bagging_fraction=1\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006\n","[LightGBM] [Info] Start training from score 1.016006\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.034394 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006\n","[LightGBM] [Info] Start training from score 1.016006\n","[LightGBM] [Warning] feature_fraction is set=1, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1\n","[LightGBM] [Warning] bagging_fraction is set=1, subsample=1.0 will be ignored. Current value: bagging_fraction=1\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] feature_fraction is set=1, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1\n","[LightGBM] [Warning] bagging_fraction is set=1, subsample=1.0 will be ignored. Current value: bagging_fraction=1\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] feature_fraction is set=1, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1\n","[LightGBM] [Warning] bagging_fraction is set=1, subsample=1.0 will be ignored. Current value: bagging_fraction=1\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002778 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006\n","[LightGBM] [Info] Start training from score 1.016006\n","[LightGBM] [Warning] feature_fraction is set=1, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1\n","[LightGBM] [Warning] bagging_fraction is set=1, subsample=1.0 will be ignored. Current value: bagging_fraction=1\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] feature_fraction is set=1, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1\n","[LightGBM] [Warning] bagging_fraction is set=1, subsample=1.0 will be ignored. Current value: bagging_fraction=1\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] feature_fraction is set=1, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1\n","[LightGBM] [Warning] bagging_fraction is set=1, subsample=1.0 will be ignored. Current value: bagging_fraction=1\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002616 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006\n","[LightGBM] [Info] Start training from score 1.016006\n","[LightGBM] [Warning] feature_fraction is set=1, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1\n","[LightGBM] [Warning] bagging_fraction is set=1, subsample=1.0 will be ignored. Current value: bagging_fraction=1\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] feature_fraction is set=1, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1\n","[LightGBM] [Warning] bagging_fraction is set=1, subsample=1.0 will be ignored. Current value: bagging_fraction=1\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] feature_fraction is set=1, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1\n","[LightGBM] [Warning] bagging_fraction is set=1, subsample=1.0 will be ignored. Current value: bagging_fraction=1\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002070 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006\n","[LightGBM] [Info] Start training from score 1.016006\n","[LightGBM] [Warning] feature_fraction is set=1, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1\n","[LightGBM] [Warning] bagging_fraction is set=1, subsample=1.0 will be ignored. Current value: bagging_fraction=1\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] feature_fraction is set=1, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1\n","[LightGBM] [Warning] bagging_fraction is set=1, subsample=1.0 will be ignored. Current value: bagging_fraction=1\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] feature_fraction is set=1, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1\n","[LightGBM] [Warning] bagging_fraction is set=1, subsample=1.0 will be ignored. Current value: bagging_fraction=1\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Info] Number of positive: 2788, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003363 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3797, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734264 -> initscore=1.016365\n","[LightGBM] [Info] Start training from score 1.016365\n","[LightGBM] [Warning] feature_fraction is set=1, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1\n","[LightGBM] [Warning] bagging_fraction is set=1, subsample=1.0 will be ignored. Current value: bagging_fraction=1\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] feature_fraction is set=1, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1\n","[LightGBM] [Warning] bagging_fraction is set=1, subsample=1.0 will be ignored. Current value: bagging_fraction=1\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] feature_fraction is set=1, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1\n","[LightGBM] [Warning] bagging_fraction is set=1, subsample=1.0 will be ignored. Current value: bagging_fraction=1\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] feature_fraction is set=1, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1\n","[LightGBM] [Warning] bagging_fraction is set=1, subsample=1.0 will be ignored. Current value: bagging_fraction=1\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Info] Number of positive: 2788, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002767 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3797, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734264 -> initscore=1.016365\n","[LightGBM] [Info] Start training from score 1.016365\n","[LightGBM] [Warning] feature_fraction is set=1, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Warning] feature_fraction is set=1, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Info] Number of positive: 2788, number of negative: 1008\n","[LightGBM] [Warning] feature_fraction is set=1, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Warning] feature_fraction is set=1, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Warning] feature_fraction is set=1, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Warning] feature_fraction is set=1, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Warning] feature_fraction is set=1, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Warning] feature_fraction is set=1, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Warning] feature_fraction is set=1, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Warning] feature_fraction is set=1, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Warning] feature_fraction is set=1, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Warning] feature_fraction is set=1, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Warning] feature_fraction is set=1, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Warning] feature_fraction is set=1, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Warning] feature_fraction is set=1, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Warning] feature_fraction is set=1, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Warning] feature_fraction is set=1, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Warning] feature_fraction is set=1, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Info] Number of positive: 2788, number of negative: 1009\n","[LightGBM] [Warning] feature_fraction is set=1, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Warning] feature_fraction is set=1, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Info] Number of positive: 2788, number of negative: 1009\n","[LightGBM] [Warning] feature_fraction is set=1, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Warning] feature_fraction is set=1, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Info] Number of positive: 2788, number of negative: 1008\n"]},{"name":"stderr","output_type":"stream","text":["[LightGBM] [Fatal] Check failed: (config->bagging_freq > 0 && config->bagging_fraction < 1.0f && config->bagging_fraction > 0.0f) || (config->feature_fraction < 1.0f && config->feature_fraction > 0.0f) at /private/var/folders/wc/6x35sr293njgq8g1xgc9fzlm0000gn/T/pip-install-6ex9ihsl/lightgbm_69c33b30bfb64530b2236e55e790ae94/src/boosting/rf.hpp, line 37 .\n","\n","[LightGBM] [Fatal] Check failed: (config->bagging_freq > 0 && config->bagging_fraction < 1.0f && config->bagging_fraction > 0.0f) || (config->feature_fraction < 1.0f && config->feature_fraction > 0.0f) at /private/var/folders/wc/6x35sr293njgq8g1xgc9fzlm0000gn/T/pip-install-6ex9ihsl/lightgbm_69c33b30bfb64530b2236e55e790ae94/src/boosting/rf.hpp, line 37 .\n","\n","[LightGBM] [Fatal] Check failed: (config->bagging_freq > 0 && config->bagging_fraction < 1.0f && config->bagging_fraction > 0.0f) || (config->feature_fraction < 1.0f && config->feature_fraction > 0.0f) at /private/var/folders/wc/6x35sr293njgq8g1xgc9fzlm0000gn/T/pip-install-6ex9ihsl/lightgbm_69c33b30bfb64530b2236e55e790ae94/src/boosting/rf.hpp, line 37 .\n","\n","[LightGBM] [Fatal] Check failed: (config->bagging_freq > 0 && config->bagging_fraction < 1.0f && config->bagging_fraction > 0.0f) || (config->feature_fraction < 1.0f && config->feature_fraction > 0.0f) at /private/var/folders/wc/6x35sr293njgq8g1xgc9fzlm0000gn/T/pip-install-6ex9ihsl/lightgbm_69c33b30bfb64530b2236e55e790ae94/src/boosting/rf.hpp, line 37 .\n","\n","[LightGBM] [Fatal] Check failed: (config->bagging_freq > 0 && config->bagging_fraction < 1.0f && config->bagging_fraction > 0.0f) || (config->feature_fraction < 1.0f && config->feature_fraction > 0.0f) at /private/var/folders/wc/6x35sr293njgq8g1xgc9fzlm0000gn/T/pip-install-6ex9ihsl/lightgbm_69c33b30bfb64530b2236e55e790ae94/src/boosting/rf.hpp, line 37 .\n","\n","[LightGBM] [Fatal] Check failed: (config->bagging_freq > 0 && config->bagging_fraction < 1.0f && config->bagging_fraction > 0.0f) || (config->feature_fraction < 1.0f && config->feature_fraction > 0.0f) at /private/var/folders/wc/6x35sr293njgq8g1xgc9fzlm0000gn/T/pip-install-6ex9ihsl/lightgbm_69c33b30bfb64530b2236e55e790ae94/src/boosting/rf.hpp, line 37 .\n","\n","[LightGBM] [Fatal] Check failed: (config->bagging_freq > 0 && config->bagging_fraction < 1.0f && config->bagging_fraction > 0.0f) || (config->feature_fraction < 1.0f && config->feature_fraction > 0.0f) at /private/var/folders/wc/6x35sr293njgq8g1xgc9fzlm0000gn/T/pip-install-6ex9ihsl/lightgbm_69c33b30bfb64530b2236e55e790ae94/src/boosting/rf.hpp, line 37 .\n","\n","[LightGBM] [Fatal] Check failed: (config->bagging_freq > 0 && config->bagging_fraction < 1.0f && config->bagging_fraction > 0.0f) || (config->feature_fraction < 1.0f && config->feature_fraction > 0.0f) at /private/var/folders/wc/6x35sr293njgq8g1xgc9fzlm0000gn/T/pip-install-6ex9ihsl/lightgbm_69c33b30bfb64530b2236e55e790ae94/src/boosting/rf.hpp, line 37 .\n","\n","[LightGBM] [Fatal] Check failed: (config->bagging_freq > 0 && config->bagging_fraction < 1.0f && config->bagging_fraction > 0.0f) || (config->feature_fraction < 1.0f && config->feature_fraction > 0.0f) at /private/var/folders/wc/6x35sr293njgq8g1xgc9fzlm0000gn/T/pip-install-6ex9ihsl/lightgbm_69c33b30bfb64530b2236e55e790ae94/src/boosting/rf.hpp, line 37 .\n","\n","[LightGBM] [Fatal] Check failed: (config->bagging_freq > 0 && config->bagging_fraction < 1.0f && config->bagging_fraction > 0.0f) || (config->feature_fraction < 1.0f && config->feature_fraction > 0.0f) at /private/var/folders/wc/6x35sr293njgq8g1xgc9fzlm0000gn/T/pip-install-6ex9ihsl/lightgbm_69c33b30bfb64530b2236e55e790ae94/src/boosting/rf.hpp, line 37 .\n","\n","[LightGBM] [Fatal] Check failed: (config->bagging_freq > 0 && config->bagging_fraction < 1.0f && config->bagging_fraction > 0.0f) || (config->feature_fraction < 1.0f && config->feature_fraction > 0.0f) at /private/var/folders/wc/6x35sr293njgq8g1xgc9fzlm0000gn/T/pip-install-6ex9ihsl/lightgbm_69c33b30bfb64530b2236e55e790ae94/src/boosting/rf.hpp, line 37 .\n","\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Warning] feature_fraction is set=1, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1\n","[LightGBM] [Warning] bagging_fraction is set=1, subsample=1.0 will be ignored. Current value: bagging_fraction=1\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] feature_fraction is set=1, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Warning] feature_fraction is set=1, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Warning] feature_fraction is set=1, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Warning] feature_fraction is set=1, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Warning] feature_fraction is set=1, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Warning] feature_fraction is set=1, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Warning] feature_fraction is set=1, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Warning] feature_fraction is set=1, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Warning] feature_fraction is set=1, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Warning] feature_fraction is set=1, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Warning] feature_fraction is set=1, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Warning] feature_fraction is set=1, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Warning] feature_fraction is set=1, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1\n","[LightGBM] [Warning] bagging_fraction is set=1, subsample=1.0 will be ignored. Current value: bagging_fraction=1\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] feature_fraction is set=1, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Warning] feature_fraction is set=1, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Info] Number of positive: 2788, number of negative: 1009\n","[LightGBM] [Warning] feature_fraction is set=1, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Warning] feature_fraction is set=1, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Warning] feature_fraction is set=1, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Info] Number of positive: 2788, number of negative: 1008\n","[LightGBM] [Warning] feature_fraction is set=1, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Warning] feature_fraction is set=1, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Warning] feature_fraction is set=1, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] feature_fraction is set=1, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Info] Number of positive: 2788, number of negative: 1009\n","[LightGBM] [Warning] feature_fraction is set=1, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Warning] feature_fraction is set=1, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Warning] feature_fraction is set=1, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Warning] feature_fraction is set=1, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Warning] feature_fraction is set=1, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Warning] feature_fraction is set=1, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Warning] feature_fraction is set=1, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Warning] feature_fraction is set=1, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Warning] feature_fraction is set=1, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Warning] feature_fraction is set=1, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Warning] feature_fraction is set=1, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Warning] feature_fraction is set=1, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Warning] feature_fraction is set=1, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Info] Number of positive: 2788, number of negative: 1009\n"]},{"name":"stderr","output_type":"stream","text":["[LightGBM] [Fatal] Check failed: (config->bagging_freq > 0 && config->bagging_fraction < 1.0f && config->bagging_fraction > 0.0f) || (config->feature_fraction < 1.0f && config->feature_fraction > 0.0f) at /private/var/folders/wc/6x35sr293njgq8g1xgc9fzlm0000gn/T/pip-install-6ex9ihsl/lightgbm_69c33b30bfb64530b2236e55e790ae94/src/boosting/rf.hpp, line 37 .\n","\n","[LightGBM] [Fatal] Check failed: (config->bagging_freq > 0 && config->bagging_fraction < 1.0f && config->bagging_fraction > 0.0f) || (config->feature_fraction < 1.0f && config->feature_fraction > 0.0f) at /private/var/folders/wc/6x35sr293njgq8g1xgc9fzlm0000gn/T/pip-install-6ex9ihsl/lightgbm_69c33b30bfb64530b2236e55e790ae94/src/boosting/rf.hpp, line 37 .\n","\n","[LightGBM] [Fatal] Check failed: (config->bagging_freq > 0 && config->bagging_fraction < 1.0f && config->bagging_fraction > 0.0f) || (config->feature_fraction < 1.0f && config->feature_fraction > 0.0f) at /private/var/folders/wc/6x35sr293njgq8g1xgc9fzlm0000gn/T/pip-install-6ex9ihsl/lightgbm_69c33b30bfb64530b2236e55e790ae94/src/boosting/rf.hpp, line 37 .\n","\n","[LightGBM] [Fatal] Check failed: (config->bagging_freq > 0 && config->bagging_fraction < 1.0f && config->bagging_fraction > 0.0f) || (config->feature_fraction < 1.0f && config->feature_fraction > 0.0f) at /private/var/folders/wc/6x35sr293njgq8g1xgc9fzlm0000gn/T/pip-install-6ex9ihsl/lightgbm_69c33b30bfb64530b2236e55e790ae94/src/boosting/rf.hpp, line 37 .\n","\n","[LightGBM] [Fatal] Check failed: (config->bagging_freq > 0 && config->bagging_fraction < 1.0f && config->bagging_fraction > 0.0f) || (config->feature_fraction < 1.0f && config->feature_fraction > 0.0f) at /private/var/folders/wc/6x35sr293njgq8g1xgc9fzlm0000gn/T/pip-install-6ex9ihsl/lightgbm_69c33b30bfb64530b2236e55e790ae94/src/boosting/rf.hpp, line 37 .\n","\n","[LightGBM] [Fatal] Check failed: (config->bagging_freq > 0 && config->bagging_fraction < 1.0f && config->bagging_fraction > 0.0f) || (config->feature_fraction < 1.0f && config->feature_fraction > 0.0f) at /private/var/folders/wc/6x35sr293njgq8g1xgc9fzlm0000gn/T/pip-install-6ex9ihsl/lightgbm_69c33b30bfb64530b2236e55e790ae94/src/boosting/rf.hpp, line 37 .\n","\n","[LightGBM] [Fatal] Check failed: (config->bagging_freq > 0 && config->bagging_fraction < 1.0f && config->bagging_fraction > 0.0f) || (config->feature_fraction < 1.0f && config->feature_fraction > 0.0f) at /private/var/folders/wc/6x35sr293njgq8g1xgc9fzlm0000gn/T/pip-install-6ex9ihsl/lightgbm_69c33b30bfb64530b2236e55e790ae94/src/boosting/rf.hpp, line 37 .\n","\n","[LightGBM] [Fatal] Check failed: (config->bagging_freq > 0 && config->bagging_fraction < 1.0f && config->bagging_fraction > 0.0f) || (config->feature_fraction < 1.0f && config->feature_fraction > 0.0f) at /private/var/folders/wc/6x35sr293njgq8g1xgc9fzlm0000gn/T/pip-install-6ex9ihsl/lightgbm_69c33b30bfb64530b2236e55e790ae94/src/boosting/rf.hpp, line 37 .\n","\n","[LightGBM] [Fatal] Check failed: (config->bagging_freq > 0 && config->bagging_fraction < 1.0f && config->bagging_fraction > 0.0f) || (config->feature_fraction < 1.0f && config->feature_fraction > 0.0f) at /private/var/folders/wc/6x35sr293njgq8g1xgc9fzlm0000gn/T/pip-install-6ex9ihsl/lightgbm_69c33b30bfb64530b2236e55e790ae94/src/boosting/rf.hpp, line 37 .\n","\n","[LightGBM] [Fatal] Check failed: (config->bagging_freq > 0 && config->bagging_fraction < 1.0f && config->bagging_fraction > 0.0f) || (config->feature_fraction < 1.0f && config->feature_fraction > 0.0f) at /private/var/folders/wc/6x35sr293njgq8g1xgc9fzlm0000gn/T/pip-install-6ex9ihsl/lightgbm_69c33b30bfb64530b2236e55e790ae94/src/boosting/rf.hpp, line 37 .\n","\n","[LightGBM] [Fatal] Check failed: (config->bagging_freq > 0 && config->bagging_fraction < 1.0f && config->bagging_fraction > 0.0f) || (config->feature_fraction < 1.0f && config->feature_fraction > 0.0f) at /private/var/folders/wc/6x35sr293njgq8g1xgc9fzlm0000gn/T/pip-install-6ex9ihsl/lightgbm_69c33b30bfb64530b2236e55e790ae94/src/boosting/rf.hpp, line 37 .\n","\n","[LightGBM] [Fatal] Check failed: (config->bagging_freq > 0 && config->bagging_fraction < 1.0f && config->bagging_fraction > 0.0f) || (config->feature_fraction < 1.0f && config->feature_fraction > 0.0f) at /private/var/folders/wc/6x35sr293njgq8g1xgc9fzlm0000gn/T/pip-install-6ex9ihsl/lightgbm_69c33b30bfb64530b2236e55e790ae94/src/boosting/rf.hpp, line 37 .\n","\n","[LightGBM] [Fatal] Check failed: (config->bagging_freq > 0 && config->bagging_fraction < 1.0f && config->bagging_fraction > 0.0f) || (config->feature_fraction < 1.0f && config->feature_fraction > 0.0f) at /private/var/folders/wc/6x35sr293njgq8g1xgc9fzlm0000gn/T/pip-install-6ex9ihsl/lightgbm_69c33b30bfb64530b2236e55e790ae94/src/boosting/rf.hpp, line 37 .\n","\n","[LightGBM] [Fatal] Check failed: (config->bagging_freq > 0 && config->bagging_fraction < 1.0f && config->bagging_fraction > 0.0f) || (config->feature_fraction < 1.0f && config->feature_fraction > 0.0f) at /private/var/folders/wc/6x35sr293njgq8g1xgc9fzlm0000gn/T/pip-install-6ex9ihsl/lightgbm_69c33b30bfb64530b2236e55e790ae94/src/boosting/rf.hpp, line 37 .\n","\n","[LightGBM] [Fatal] Check failed: (config->bagging_freq > 0 && config->bagging_fraction < 1.0f && config->bagging_fraction > 0.0f) || (config->feature_fraction < 1.0f && config->feature_fraction > 0.0f) at /private/var/folders/wc/6x35sr293njgq8g1xgc9fzlm0000gn/T/pip-install-6ex9ihsl/lightgbm_69c33b30bfb64530b2236e55e790ae94/src/boosting/rf.hpp, line 37 .\n","\n","[LightGBM] [Fatal] Check failed: (config->bagging_freq > 0 && config->bagging_fraction < 1.0f && config->bagging_fraction > 0.0f) || (config->feature_fraction < 1.0f && config->feature_fraction > 0.0f) at /private/var/folders/wc/6x35sr293njgq8g1xgc9fzlm0000gn/T/pip-install-6ex9ihsl/lightgbm_69c33b30bfb64530b2236e55e790ae94/src/boosting/rf.hpp, line 37 .\n","\n","[LightGBM] [Fatal] Check failed: (config->bagging_freq > 0 && config->bagging_fraction < 1.0f && config->bagging_fraction > 0.0f) || (config->feature_fraction < 1.0f && config->feature_fraction > 0.0f) at /private/var/folders/wc/6x35sr293njgq8g1xgc9fzlm0000gn/T/pip-install-6ex9ihsl/lightgbm_69c33b30bfb64530b2236e55e790ae94/src/boosting/rf.hpp, line 37 .\n","\n","[LightGBM] [Fatal] Check failed: (config->bagging_freq > 0 && config->bagging_fraction < 1.0f && config->bagging_fraction > 0.0f) || (config->feature_fraction < 1.0f && config->feature_fraction > 0.0f) at /private/var/folders/wc/6x35sr293njgq8g1xgc9fzlm0000gn/T/pip-install-6ex9ihsl/lightgbm_69c33b30bfb64530b2236e55e790ae94/src/boosting/rf.hpp, line 37 .\n","\n","[LightGBM] [Fatal] Check failed: (config->bagging_freq > 0 && config->bagging_fraction < 1.0f && config->bagging_fraction > 0.0f) || (config->feature_fraction < 1.0f && config->feature_fraction > 0.0f) at /private/var/folders/wc/6x35sr293njgq8g1xgc9fzlm0000gn/T/pip-install-6ex9ihsl/lightgbm_69c33b30bfb64530b2236e55e790ae94/src/boosting/rf.hpp, line 37 .\n","\n","[LightGBM] [Fatal] Check failed: (config->bagging_freq > 0 && config->bagging_fraction < 1.0f && config->bagging_fraction > 0.0f) || (config->feature_fraction < 1.0f && config->feature_fraction > 0.0f) at /private/var/folders/wc/6x35sr293njgq8g1xgc9fzlm0000gn/T/pip-install-6ex9ihsl/lightgbm_69c33b30bfb64530b2236e55e790ae94/src/boosting/rf.hpp, line 37 .\n","\n","[LightGBM] [Fatal] Check failed: (config->bagging_freq > 0 && config->bagging_fraction < 1.0f && config->bagging_fraction > 0.0f) || (config->feature_fraction < 1.0f && config->feature_fraction > 0.0f) at /private/var/folders/wc/6x35sr293njgq8g1xgc9fzlm0000gn/T/pip-install-6ex9ihsl/lightgbm_69c33b30bfb64530b2236e55e790ae94/src/boosting/rf.hpp, line 37 .\n","\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Warning] feature_fraction is set=1, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Warning] feature_fraction is set=1, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Info] Number of positive: 2788, number of negative: 1009\n","[LightGBM] [Warning] feature_fraction is set=1, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Warning] feature_fraction is set=1, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Warning] feature_fraction is set=1, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Warning] feature_fraction is set=1, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Warning] feature_fraction is set=1, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Info] Number of positive: 2788, number of negative: 1008\n","[LightGBM] [Warning] feature_fraction is set=1, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Warning] feature_fraction is set=1, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Warning] feature_fraction is set=1, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Warning] feature_fraction is set=1, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Warning] feature_fraction is set=1, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Warning] feature_fraction is set=1, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Warning] feature_fraction is set=1, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Warning] feature_fraction is set=1, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Warning] feature_fraction is set=1, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Warning] feature_fraction is set=1, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Warning] feature_fraction is set=1, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Info] Number of positive: 2788, number of negative: 1009\n","[LightGBM] [Warning] feature_fraction is set=1, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Warning] feature_fraction is set=1, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Warning] feature_fraction is set=1, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Info] Number of positive: 2788, number of negative: 1009\n","[LightGBM] [Warning] feature_fraction is set=1, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Warning] feature_fraction is set=1, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Info] Number of positive: 2788, number of negative: 1008\n","[LightGBM] [Warning] feature_fraction is set=1, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Warning] feature_fraction is set=1, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Warning] feature_fraction is set=1, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Warning] feature_fraction is set=1, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Warning] feature_fraction is set=1, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Warning] feature_fraction is set=1, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Warning] feature_fraction is set=1, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Warning] feature_fraction is set=1, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Warning] feature_fraction is set=1, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Warning] feature_fraction is set=1, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Warning] feature_fraction is set=1, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Warning] feature_fraction is set=1, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Warning] feature_fraction is set=1, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Warning] feature_fraction is set=1, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Warning] feature_fraction is set=1, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Warning] feature_fraction is set=1, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Warning] feature_fraction is set=1, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Info] Number of positive: 2788, number of negative: 1009\n","[LightGBM] [Warning] feature_fraction is set=1, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Warning] feature_fraction is set=0.2, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Warning] feature_fraction is set=0.2, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Info] Number of positive: 2788, number of negative: 1008\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004219 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734457 -> initscore=1.017356\n","[LightGBM] [Info] Start training from score 1.017356\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"]},{"name":"stderr","output_type":"stream","text":["[LightGBM] [Fatal] Check failed: (config->bagging_freq > 0 && config->bagging_fraction < 1.0f && config->bagging_fraction > 0.0f) || (config->feature_fraction < 1.0f && config->feature_fraction > 0.0f) at /private/var/folders/wc/6x35sr293njgq8g1xgc9fzlm0000gn/T/pip-install-6ex9ihsl/lightgbm_69c33b30bfb64530b2236e55e790ae94/src/boosting/rf.hpp, line 37 .\n","\n","[LightGBM] [Fatal] Check failed: (config->bagging_freq > 0 && config->bagging_fraction < 1.0f && config->bagging_fraction > 0.0f) || (config->feature_fraction < 1.0f && config->feature_fraction > 0.0f) at /private/var/folders/wc/6x35sr293njgq8g1xgc9fzlm0000gn/T/pip-install-6ex9ihsl/lightgbm_69c33b30bfb64530b2236e55e790ae94/src/boosting/rf.hpp, line 37 .\n","\n","[LightGBM] [Fatal] Check failed: (config->bagging_freq > 0 && config->bagging_fraction < 1.0f && config->bagging_fraction > 0.0f) || (config->feature_fraction < 1.0f && config->feature_fraction > 0.0f) at /private/var/folders/wc/6x35sr293njgq8g1xgc9fzlm0000gn/T/pip-install-6ex9ihsl/lightgbm_69c33b30bfb64530b2236e55e790ae94/src/boosting/rf.hpp, line 37 .\n","\n","[LightGBM] [Fatal] Check failed: (config->bagging_freq > 0 && config->bagging_fraction < 1.0f && config->bagging_fraction > 0.0f) || (config->feature_fraction < 1.0f && config->feature_fraction > 0.0f) at /private/var/folders/wc/6x35sr293njgq8g1xgc9fzlm0000gn/T/pip-install-6ex9ihsl/lightgbm_69c33b30bfb64530b2236e55e790ae94/src/boosting/rf.hpp, line 37 .\n","\n","[LightGBM] [Fatal] Check failed: (config->bagging_freq > 0 && config->bagging_fraction < 1.0f && config->bagging_fraction > 0.0f) || (config->feature_fraction < 1.0f && config->feature_fraction > 0.0f) at /private/var/folders/wc/6x35sr293njgq8g1xgc9fzlm0000gn/T/pip-install-6ex9ihsl/lightgbm_69c33b30bfb64530b2236e55e790ae94/src/boosting/rf.hpp, line 37 .\n","\n","[LightGBM] [Fatal] Check failed: (config->bagging_freq > 0 && config->bagging_fraction < 1.0f && config->bagging_fraction > 0.0f) || (config->feature_fraction < 1.0f && config->feature_fraction > 0.0f) at /private/var/folders/wc/6x35sr293njgq8g1xgc9fzlm0000gn/T/pip-install-6ex9ihsl/lightgbm_69c33b30bfb64530b2236e55e790ae94/src/boosting/rf.hpp, line 37 .\n","\n","[LightGBM] [Fatal] Check failed: (config->bagging_freq > 0 && config->bagging_fraction < 1.0f && config->bagging_fraction > 0.0f) || (config->feature_fraction < 1.0f && config->feature_fraction > 0.0f) at /private/var/folders/wc/6x35sr293njgq8g1xgc9fzlm0000gn/T/pip-install-6ex9ihsl/lightgbm_69c33b30bfb64530b2236e55e790ae94/src/boosting/rf.hpp, line 37 .\n","\n","[LightGBM] [Fatal] Check failed: (config->bagging_freq > 0 && config->bagging_fraction < 1.0f && config->bagging_fraction > 0.0f) || (config->feature_fraction < 1.0f && config->feature_fraction > 0.0f) at /private/var/folders/wc/6x35sr293njgq8g1xgc9fzlm0000gn/T/pip-install-6ex9ihsl/lightgbm_69c33b30bfb64530b2236e55e790ae94/src/boosting/rf.hpp, line 37 .\n","\n","[LightGBM] [Fatal] Check failed: (config->bagging_freq > 0 && config->bagging_fraction < 1.0f && config->bagging_fraction > 0.0f) || (config->feature_fraction < 1.0f && config->feature_fraction > 0.0f) at /private/var/folders/wc/6x35sr293njgq8g1xgc9fzlm0000gn/T/pip-install-6ex9ihsl/lightgbm_69c33b30bfb64530b2236e55e790ae94/src/boosting/rf.hpp, line 37 .\n","\n","[LightGBM] [Fatal] Check failed: (config->bagging_freq > 0 && config->bagging_fraction < 1.0f && config->bagging_fraction > 0.0f) || (config->feature_fraction < 1.0f && config->feature_fraction > 0.0f) at /private/var/folders/wc/6x35sr293njgq8g1xgc9fzlm0000gn/T/pip-install-6ex9ihsl/lightgbm_69c33b30bfb64530b2236e55e790ae94/src/boosting/rf.hpp, line 37 .\n","\n","[LightGBM] [Fatal] Check failed: (config->bagging_freq > 0 && config->bagging_fraction < 1.0f && config->bagging_fraction > 0.0f) || (config->feature_fraction < 1.0f && config->feature_fraction > 0.0f) at /private/var/folders/wc/6x35sr293njgq8g1xgc9fzlm0000gn/T/pip-install-6ex9ihsl/lightgbm_69c33b30bfb64530b2236e55e790ae94/src/boosting/rf.hpp, line 37 .\n","\n","[LightGBM] [Fatal] Check failed: (config->bagging_freq > 0 && config->bagging_fraction < 1.0f && config->bagging_fraction > 0.0f) || (config->feature_fraction < 1.0f && config->feature_fraction > 0.0f) at /private/var/folders/wc/6x35sr293njgq8g1xgc9fzlm0000gn/T/pip-install-6ex9ihsl/lightgbm_69c33b30bfb64530b2236e55e790ae94/src/boosting/rf.hpp, line 37 .\n","\n","[LightGBM] [Fatal] Check failed: (config->bagging_freq > 0 && config->bagging_fraction < 1.0f && config->bagging_fraction > 0.0f) || (config->feature_fraction < 1.0f && config->feature_fraction > 0.0f) at /private/var/folders/wc/6x35sr293njgq8g1xgc9fzlm0000gn/T/pip-install-6ex9ihsl/lightgbm_69c33b30bfb64530b2236e55e790ae94/src/boosting/rf.hpp, line 37 .\n","\n","[LightGBM] [Fatal] Check failed: (config->bagging_freq > 0 && config->bagging_fraction < 1.0f && config->bagging_fraction > 0.0f) || (config->feature_fraction < 1.0f && config->feature_fraction > 0.0f) at /private/var/folders/wc/6x35sr293njgq8g1xgc9fzlm0000gn/T/pip-install-6ex9ihsl/lightgbm_69c33b30bfb64530b2236e55e790ae94/src/boosting/rf.hpp, line 37 .\n","\n","[LightGBM] [Fatal] Check failed: (config->bagging_freq > 0 && config->bagging_fraction < 1.0f && config->bagging_fraction > 0.0f) || (config->feature_fraction < 1.0f && config->feature_fraction > 0.0f) at /private/var/folders/wc/6x35sr293njgq8g1xgc9fzlm0000gn/T/pip-install-6ex9ihsl/lightgbm_69c33b30bfb64530b2236e55e790ae94/src/boosting/rf.hpp, line 37 .\n","\n","[LightGBM] [Fatal] Check failed: (config->bagging_freq > 0 && config->bagging_fraction < 1.0f && config->bagging_fraction > 0.0f) || (config->feature_fraction < 1.0f && config->feature_fraction > 0.0f) at /private/var/folders/wc/6x35sr293njgq8g1xgc9fzlm0000gn/T/pip-install-6ex9ihsl/lightgbm_69c33b30bfb64530b2236e55e790ae94/src/boosting/rf.hpp, line 37 .\n","\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Warning] feature_fraction is set=1, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Warning] feature_fraction is set=1, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Info] Number of positive: 2788, number of negative: 1009\n","[LightGBM] [Warning] feature_fraction is set=1, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Warning] feature_fraction is set=0.2, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Warning] feature_fraction is set=0.2, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001745 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006\n","[LightGBM] [Info] Start training from score 1.016006\n","[LightGBM] [Warning] feature_fraction is set=0.2, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Warning] feature_fraction is set=0.2, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003094 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006\n","[LightGBM] [Info] Start training from score 1.016006\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] feature_fraction is set=1, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1\n","[LightGBM] [Warning] bagging_fraction is set=1, subsample=1.0 will be ignored. Current value: bagging_fraction=1\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] feature_fraction is set=0.2, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Warning] feature_fraction is set=0.2, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n"]},{"name":"stderr","output_type":"stream","text":["[LightGBM] [Fatal] Check failed: (config->bagging_freq > 0 && config->bagging_fraction < 1.0f && config->bagging_fraction > 0.0f) || (config->feature_fraction < 1.0f && config->feature_fraction > 0.0f) at /private/var/folders/wc/6x35sr293njgq8g1xgc9fzlm0000gn/T/pip-install-6ex9ihsl/lightgbm_69c33b30bfb64530b2236e55e790ae94/src/boosting/rf.hpp, line 37 .\n","\n","[LightGBM] [Fatal] Check failed: (config->bagging_freq > 0 && config->bagging_fraction < 1.0f && config->bagging_fraction > 0.0f) || (config->feature_fraction < 1.0f && config->feature_fraction > 0.0f) at /private/var/folders/wc/6x35sr293njgq8g1xgc9fzlm0000gn/T/pip-install-6ex9ihsl/lightgbm_69c33b30bfb64530b2236e55e790ae94/src/boosting/rf.hpp, line 37 .\n","\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002808 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006\n","[LightGBM] [Info] Start training from score 1.016006\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] feature_fraction is set=1, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1\n","[LightGBM] [Warning] bagging_fraction is set=1, subsample=1.0 will be ignored. Current value: bagging_fraction=1\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] feature_fraction is set=1, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1\n","[LightGBM] [Warning] bagging_fraction is set=1, subsample=1.0 will be ignored. Current value: bagging_fraction=1\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] feature_fraction is set=0.2, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Warning] feature_fraction is set=0.2, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] feature_fraction is set=0.2, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Info] Number of positive: 2788, number of negative: 1008\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] feature_fraction is set=0.2, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Info] Number of positive: 2788, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.009855 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734457 -> initscore=1.017356\n","[LightGBM] [Info] Start training from score 1.017356\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.018004 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3797, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734264 -> initscore=1.016365\n","[LightGBM] [Info] Start training from score 1.016365\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] feature_fraction is set=1, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1\n","[LightGBM] [Warning] bagging_fraction is set=1, subsample=1.0 will be ignored. Current value: bagging_fraction=1\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] feature_fraction is set=0.2, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] feature_fraction is set=0.2, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Warning] feature_fraction is set=1, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1\n","[LightGBM] [Warning] bagging_fraction is set=1, subsample=1.0 will be ignored. Current value: bagging_fraction=1\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] feature_fraction is set=0.2, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Warning] feature_fraction is set=0.2, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001312 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006\n","[LightGBM] [Info] Start training from score 1.016006\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000728 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006\n","[LightGBM] [Info] Start training from score 1.016006\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] feature_fraction is set=1, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1\n","[LightGBM] [Warning] bagging_fraction is set=1, subsample=1.0 will be ignored. Current value: bagging_fraction=1\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] feature_fraction is set=0.2, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Warning] feature_fraction is set=0.2, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.052734 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006\n","[LightGBM] [Info] Start training from score 1.016006\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] feature_fraction is set=1, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1\n","[LightGBM] [Warning] bagging_fraction is set=1, subsample=1.0 will be ignored. Current value: bagging_fraction=1\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] feature_fraction is set=0.2, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] feature_fraction is set=0.2, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Warning] feature_fraction is set=0.2, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] feature_fraction is set=0.2, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001575 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006\n","[LightGBM] [Info] Start training from score 1.016006\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] feature_fraction is set=0.2, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Info] Number of positive: 2788, number of negative: 1009\n","[LightGBM] [Warning] feature_fraction is set=0.2, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002168 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3797, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734264 -> initscore=1.016365\n","[LightGBM] [Info] Start training from score 1.016365\n","[LightGBM] [Warning] feature_fraction is set=0.2, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] feature_fraction is set=0.2, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.032178 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006\n","[LightGBM] [Info] Start training from score 1.016006\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] feature_fraction is set=0.2, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] feature_fraction is set=0.2, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] feature_fraction is set=0.2, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002899 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006\n","[LightGBM] [Info] Start training from score 1.016006\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] feature_fraction is set=0.2, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Warning] feature_fraction is set=0.2, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] feature_fraction is set=0.2, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004000 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006\n","[LightGBM] [Info] Start training from score 1.016006\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] feature_fraction is set=0.2, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] feature_fraction is set=0.2, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Warning] feature_fraction is set=0.2, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006715 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006\n","[LightGBM] [Info] Start training from score 1.016006\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] feature_fraction is set=0.2, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] feature_fraction is set=0.2, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Warning] feature_fraction is set=0.2, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Warning] feature_fraction is set=0.2, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Warning] feature_fraction is set=0.2, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Info] Number of positive: 2788, number of negative: 1009\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] feature_fraction is set=0.2, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002783 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3797, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734264 -> initscore=1.016365\n","[LightGBM] [Info] Start training from score 1.016365\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.077641 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006\n","[LightGBM] [Warning] feature_fraction is set=0.2, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Info] Start training from score 1.016006\n","[LightGBM] [Warning] feature_fraction is set=0.2, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Warning] feature_fraction is set=0.2, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002223 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006\n","[LightGBM] [Info] Start training from score 1.016006\n","[LightGBM] [Warning] feature_fraction is set=0.2, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Warning] feature_fraction is set=0.2, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] feature_fraction is set=0.2, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002939 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006\n","[LightGBM] [Info] Start training from score 1.016006\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] feature_fraction is set=0.2, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Warning] feature_fraction is set=0.2, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Warning] feature_fraction is set=0.2, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Info] Number of positive: 2788, number of negative: 1009\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003235 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3797, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734264 -> initscore=1.016365\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Info] Start training from score 1.016365\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] feature_fraction is set=0.2, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Warning] feature_fraction is set=0.2, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Warning] feature_fraction is set=0.2, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Info] Number of positive: 2788, number of negative: 1008\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002267 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734457 -> initscore=1.017356\n","[LightGBM] [Info] Start training from score 1.017356\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] feature_fraction is set=0.2, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] feature_fraction is set=0.2, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Warning] feature_fraction is set=0.2, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003589 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006\n","[LightGBM] [Info] Start training from score 1.016006\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] feature_fraction is set=0.2, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] feature_fraction is set=0.2, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Warning] feature_fraction is set=0.2, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] feature_fraction is set=0.2, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Warning] feature_fraction is set=0.2, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003871 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006\n","[LightGBM] [Info] Start training from score 1.016006\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] feature_fraction is set=0.2, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001788 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006\n","[LightGBM] [Info] Start training from score 1.016006\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] feature_fraction is set=0.2, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] feature_fraction is set=0.2, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Warning] feature_fraction is set=0.2, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Info] Number of positive: 2788, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001768 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3797, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734264 -> initscore=1.016365\n","[LightGBM] [Info] Start training from score 1.016365\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] feature_fraction is set=0.2, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] feature_fraction is set=0.2, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Warning] feature_fraction is set=0.2, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Info] Number of positive: 2788, number of negative: 1008\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001165 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734457 -> initscore=1.017356\n","[LightGBM] [Info] Start training from score 1.017356\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] feature_fraction is set=0.2, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] feature_fraction is set=0.2, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Warning] feature_fraction is set=0.2, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001940 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006\n","[LightGBM] [Info] Start training from score 1.016006\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] feature_fraction is set=0.2, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Warning] feature_fraction is set=0.2, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] feature_fraction is set=0.2, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001669 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006\n","[LightGBM] [Info] Start training from score 1.016006\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] feature_fraction is set=0.2, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] feature_fraction is set=0.2, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Warning] feature_fraction is set=0.2, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001439 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006\n","[LightGBM] [Info] Start training from score 1.016006\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] feature_fraction is set=0.2, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Warning] feature_fraction is set=0.2, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Warning] feature_fraction is set=0.2, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Info] Number of positive: 2788, number of negative: 1009\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004124 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3797, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734264 -> initscore=1.016365\n","[LightGBM] [Info] Start training from score 1.016365\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] feature_fraction is set=0.2, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] feature_fraction is set=0.2, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Warning] feature_fraction is set=0.2, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001338 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006\n","[LightGBM] [Info] Start training from score 1.016006\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] feature_fraction is set=0.2, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] feature_fraction is set=0.2, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] feature_fraction is set=0.2, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] feature_fraction is set=0.2, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Warning] feature_fraction is set=0.2, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002120 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006\n","[LightGBM] [Info] Start training from score 1.016006\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] feature_fraction is set=0.2, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003387 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006\n","[LightGBM] [Info] Start training from score 1.016006\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] feature_fraction is set=0.2, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] feature_fraction is set=0.2, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Warning] feature_fraction is set=0.2, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Info] Number of positive: 2788, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002877 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3797, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734264 -> initscore=1.016365\n","[LightGBM] [Info] Start training from score 1.016365\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] feature_fraction is set=0.2, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] feature_fraction is set=0.2, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Warning] feature_fraction is set=0.2, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001164 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006\n","[LightGBM] [Info] Start training from score 1.016006\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] feature_fraction is set=0.2, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Warning] feature_fraction is set=0.2, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Warning] feature_fraction is set=0.2, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001872 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006\n","[LightGBM] [Info] Start training from score 1.016006\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] feature_fraction is set=0.2, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] feature_fraction is set=0.2, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Warning] feature_fraction is set=0.2, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] feature_fraction is set=0.2, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] feature_fraction is set=0.2, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.017344 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Warning] feature_fraction is set=0.2, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001983 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006\n","[LightGBM] [Info] Start training from score 1.016006\n","[LightGBM] [Info] Start training from score 1.016006\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] feature_fraction is set=0.2, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] feature_fraction is set=0.2, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Warning] feature_fraction is set=0.2, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001795 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006\n","[LightGBM] [Info] Start training from score 1.016006\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] feature_fraction is set=0.2, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] feature_fraction is set=0.2, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Warning] feature_fraction is set=0.2, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Info] Number of positive: 2788, number of negative: 1009\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001578 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3797, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734264 -> initscore=1.016365\n","[LightGBM] [Info] Start training from score 1.016365\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] feature_fraction is set=0.2, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Warning] feature_fraction is set=0.2, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] feature_fraction is set=0.2, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Info] Number of positive: 2788, number of negative: 1008\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001836 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734457 -> initscore=1.017356\n","[LightGBM] [Info] Start training from score 1.017356\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] feature_fraction is set=0.2, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] feature_fraction is set=0.2, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Warning] feature_fraction is set=0.2, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001853 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006\n","[LightGBM] [Info] Start training from score 1.016006\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] feature_fraction is set=0.2, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Warning] feature_fraction is set=0.2, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] feature_fraction is set=0.2, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001906 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006\n","[LightGBM] [Info] Start training from score 1.016006\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] feature_fraction is set=0.2, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] feature_fraction is set=0.2, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Warning] feature_fraction is set=0.2, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] feature_fraction is set=0.2, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] feature_fraction is set=0.2, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Warning] feature_fraction is set=0.2, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001977 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006\n","[LightGBM] [Info] Start training from score 1.016006\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.040482 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006\n","[LightGBM] [Info] Start training from score 1.016006\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] feature_fraction is set=0.2, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] feature_fraction is set=0.2, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Warning] feature_fraction is set=0.2, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001923 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006\n","[LightGBM] [Info] Start training from score 1.016006\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] feature_fraction is set=0.2, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] feature_fraction is set=0.2, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] feature_fraction is set=0.2, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001158 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006\n","[LightGBM] [Info] Start training from score 1.016006\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] feature_fraction is set=0.2, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Warning] feature_fraction is set=0.2, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Warning] feature_fraction is set=0.2, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003101 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006\n","[LightGBM] [Info] Start training from score 1.016006\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] feature_fraction is set=0.2, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] feature_fraction is set=0.2, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] feature_fraction is set=0.2, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Info] Number of positive: 2788, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003873 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3797, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734264 -> initscore=1.016365\n","[LightGBM] [Info] Start training from score 1.016365\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] feature_fraction is set=0.2, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Warning] feature_fraction is set=0.2, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Warning] feature_fraction is set=0.2, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Info] Number of positive: 2788, number of negative: 1009\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004011 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3797, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734264 -> initscore=1.016365\n","[LightGBM] [Info] Start training from score 1.016365\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] feature_fraction is set=0.2, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.3, subsample=1.0 will be ignored. Current value: bagging_fraction=0.3\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.3, subsample=1.0 will be ignored. Current value: bagging_fraction=0.3\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Info] Number of positive: 2788, number of negative: 1008\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.023112 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734457 -> initscore=1.017356\n","[LightGBM] [Info] Start training from score 1.017356\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] feature_fraction is set=0.2, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.3, subsample=1.0 will be ignored. Current value: bagging_fraction=0.3\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.3, subsample=1.0 will be ignored. Current value: bagging_fraction=0.3\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002574 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006\n","[LightGBM] [Info] Start training from score 1.016006\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] feature_fraction is set=0.2, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.3, subsample=1.0 will be ignored. Current value: bagging_fraction=0.3\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.3, subsample=1.0 will be ignored. Current value: bagging_fraction=0.3\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000475 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006\n","[LightGBM] [Info] Start training from score 1.016006\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] feature_fraction is set=0.2, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.3, subsample=1.0 will be ignored. Current value: bagging_fraction=0.3\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.3, subsample=1.0 will be ignored. Current value: bagging_fraction=0.3\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003828 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006\n","[LightGBM] [Info] Start training from score 1.016006\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] feature_fraction is set=0.2, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.3, subsample=1.0 will be ignored. Current value: bagging_fraction=0.3\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.3, subsample=1.0 will be ignored. Current value: bagging_fraction=0.3\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002085 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006\n","[LightGBM] [Info] Start training from score 1.016006\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] feature_fraction is set=0.2, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.3, subsample=1.0 will be ignored. Current value: bagging_fraction=0.3\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] feature_fraction is set=0.2, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.3, subsample=1.0 will be ignored. Current value: bagging_fraction=0.3\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003124 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006\n","[LightGBM] [Info] Start training from score 1.016006\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.3, subsample=1.0 will be ignored. Current value: bagging_fraction=0.3\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Warning] feature_fraction is set=0.2, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.3, subsample=1.0 will be ignored. Current value: bagging_fraction=0.3\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.3, subsample=1.0 will be ignored. Current value: bagging_fraction=0.3\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.3, subsample=1.0 will be ignored. Current value: bagging_fraction=0.3\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Info] Number of positive: 2788, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001419 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3797, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734264 -> initscore=1.016365\n","[LightGBM] [Info] Start training from score 1.016365\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.3, subsample=1.0 will be ignored. Current value: bagging_fraction=0.3\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009679 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006\n","[LightGBM] [Info] Start training from score 1.016006\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.3, subsample=1.0 will be ignored. Current value: bagging_fraction=0.3\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.3, subsample=1.0 will be ignored. Current value: bagging_fraction=0.3\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003359 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006\n","[LightGBM] [Info] Start training from score 1.016006\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.3, subsample=1.0 will be ignored. Current value: bagging_fraction=0.3\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.3, subsample=1.0 will be ignored. Current value: bagging_fraction=0.3\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.3, subsample=1.0 will be ignored. Current value: bagging_fraction=0.3\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Info] Number of positive: 2788, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001080 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3797, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734264 -> initscore=1.016365\n","[LightGBM] [Info] Start training from score 1.016365\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.3, subsample=1.0 will be ignored. Current value: bagging_fraction=0.3\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Warning] feature_fraction is set=0.2, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.3, subsample=1.0 will be ignored. Current value: bagging_fraction=0.3\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.3, subsample=1.0 will be ignored. Current value: bagging_fraction=0.3\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Info] Number of positive: 2788, number of negative: 1008\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002435 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734457 -> initscore=1.017356\n","[LightGBM] [Info] Start training from score 1.017356\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.3, subsample=1.0 will be ignored. Current value: bagging_fraction=0.3\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.3, subsample=1.0 will be ignored. Current value: bagging_fraction=0.3\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.3, subsample=1.0 will be ignored. Current value: bagging_fraction=0.3\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.3, subsample=1.0 will be ignored. Current value: bagging_fraction=0.3\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001607 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006\n","[LightGBM] [Info] Start training from score 1.016006\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.3, subsample=1.0 will be ignored. Current value: bagging_fraction=0.3\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003299 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006\n","[LightGBM] [Info] Start training from score 1.016006\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.3, subsample=1.0 will be ignored. Current value: bagging_fraction=0.3\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.3, subsample=1.0 will be ignored. Current value: bagging_fraction=0.3\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.3, subsample=1.0 will be ignored. Current value: bagging_fraction=0.3\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004951 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006\n","[LightGBM] [Info] Start training from score 1.016006\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.3, subsample=1.0 will be ignored. Current value: bagging_fraction=0.3\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.3, subsample=1.0 will be ignored. Current value: bagging_fraction=0.3\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.3, subsample=1.0 will be ignored. Current value: bagging_fraction=0.3\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003958 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006\n","[LightGBM] [Info] Start training from score 1.016006\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.3, subsample=1.0 will be ignored. Current value: bagging_fraction=0.3\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.3, subsample=1.0 will be ignored. Current value: bagging_fraction=0.3\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.3, subsample=1.0 will be ignored. Current value: bagging_fraction=0.3\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002115 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006\n","[LightGBM] [Info] Start training from score 1.016006\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] feature_fraction is set=0.2, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.3, subsample=1.0 will be ignored. Current value: bagging_fraction=0.3\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.3, subsample=1.0 will be ignored. Current value: bagging_fraction=0.3\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.3, subsample=1.0 will be ignored. Current value: bagging_fraction=0.3\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001752 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006\n","[LightGBM] [Info] Start training from score 1.016006\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.3, subsample=1.0 will be ignored. Current value: bagging_fraction=0.3\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.3, subsample=1.0 will be ignored. Current value: bagging_fraction=0.3\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.3, subsample=1.0 will be ignored. Current value: bagging_fraction=0.3\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Info] Number of positive: 2788, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001484 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3797, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734264 -> initscore=1.016365\n","[LightGBM] [Info] Start training from score 1.016365\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.3, subsample=1.0 will be ignored. Current value: bagging_fraction=0.3\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.3, subsample=1.0 will be ignored. Current value: bagging_fraction=0.3\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.3, subsample=1.0 will be ignored. Current value: bagging_fraction=0.3\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.3, subsample=1.0 will be ignored. Current value: bagging_fraction=0.3\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.3, subsample=1.0 will be ignored. Current value: bagging_fraction=0.3\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.3, subsample=1.0 will be ignored. Current value: bagging_fraction=0.3\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Info] Number of positive: 2788, number of negative: 1008\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.022591 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006\n","[LightGBM] [Info] Start training from score 1.016006\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.016050 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734457 -> initscore=1.017356\n","[LightGBM] [Info] Start training from score 1.017356\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.3, subsample=1.0 will be ignored. Current value: bagging_fraction=0.3\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.3, subsample=1.0 will be ignored. Current value: bagging_fraction=0.3\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.3, subsample=1.0 will be ignored. Current value: bagging_fraction=0.3\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Info] Number of positive: 2788, number of negative: 1009\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.3, subsample=1.0 will be ignored. Current value: bagging_fraction=0.3\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.3, subsample=1.0 will be ignored. Current value: bagging_fraction=0.3\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005907 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3797, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734264 -> initscore=1.016365\n","[LightGBM] [Info] Start training from score 1.016365\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005195 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006\n","[LightGBM] [Info] Start training from score 1.016006\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.3, subsample=1.0 will be ignored. Current value: bagging_fraction=0.3\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.3, subsample=1.0 will be ignored. Current value: bagging_fraction=0.3\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.3, subsample=1.0 will be ignored. Current value: bagging_fraction=0.3\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002865 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006\n","[LightGBM] [Info] Start training from score 1.016006\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.3, subsample=1.0 will be ignored. Current value: bagging_fraction=0.3\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.3, subsample=1.0 will be ignored. Current value: bagging_fraction=0.3\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.3, subsample=1.0 will be ignored. Current value: bagging_fraction=0.3\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.3, subsample=1.0 will be ignored. Current value: bagging_fraction=0.3\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.3, subsample=1.0 will be ignored. Current value: bagging_fraction=0.3\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005306 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006\n","[LightGBM] [Info] Start training from score 1.016006\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.3, subsample=1.0 will be ignored. Current value: bagging_fraction=0.3\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.040635 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006\n","[LightGBM] [Info] Start training from score 1.016006\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.3, subsample=1.0 will be ignored. Current value: bagging_fraction=0.3\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.3, subsample=1.0 will be ignored. Current value: bagging_fraction=0.3\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.3, subsample=1.0 will be ignored. Current value: bagging_fraction=0.3\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001669 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006\n","[LightGBM] [Info] Start training from score 1.016006\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.3, subsample=1.0 will be ignored. Current value: bagging_fraction=0.3\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.3, subsample=1.0 will be ignored. Current value: bagging_fraction=0.3\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.3, subsample=1.0 will be ignored. Current value: bagging_fraction=0.3\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001699 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006\n","[LightGBM] [Info] Start training from score 1.016006\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.3, subsample=1.0 will be ignored. Current value: bagging_fraction=0.3\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.3, subsample=1.0 will be ignored. Current value: bagging_fraction=0.3\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.3, subsample=1.0 will be ignored. Current value: bagging_fraction=0.3\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002085 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006\n","[LightGBM] [Info] Start training from score 1.016006\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.3, subsample=1.0 will be ignored. Current value: bagging_fraction=0.3\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.3, subsample=1.0 will be ignored. Current value: bagging_fraction=0.3\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.3, subsample=1.0 will be ignored. Current value: bagging_fraction=0.3\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Info] Number of positive: 2788, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018483 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3797, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734264 -> initscore=1.016365\n","[LightGBM] [Info] Start training from score 1.016365\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.3, subsample=1.0 will be ignored. Current value: bagging_fraction=0.3\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.3, subsample=1.0 will be ignored. Current value: bagging_fraction=0.3\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.3, subsample=1.0 will be ignored. Current value: bagging_fraction=0.3\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.3, subsample=1.0 will be ignored. Current value: bagging_fraction=0.3\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.3, subsample=1.0 will be ignored. Current value: bagging_fraction=0.3\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.3, subsample=1.0 will be ignored. Current value: bagging_fraction=0.3\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Info] Number of positive: 2788, number of negative: 1008\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003162 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] Number of positive: 2788, number of negative: 1009\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734457 -> initscore=1.017356\n","[LightGBM] [Info] Start training from score 1.017356\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.3, subsample=1.0 will be ignored. Current value: bagging_fraction=0.3\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.3, subsample=1.0 will be ignored. Current value: bagging_fraction=0.3\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.3, subsample=1.0 will be ignored. Current value: bagging_fraction=0.3\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004412 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3797, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734264 -> initscore=1.016365\n","[LightGBM] [Info] Start training from score 1.016365\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006320 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006\n","[LightGBM] [Info] Start training from score 1.016006\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.3, subsample=1.0 will be ignored. Current value: bagging_fraction=0.3\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.3, subsample=1.0 will be ignored. Current value: bagging_fraction=0.3\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.3, subsample=1.0 will be ignored. Current value: bagging_fraction=0.3\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.025911 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006\n","[LightGBM] [Info] Start training from score 1.016006\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.3, subsample=1.0 will be ignored. Current value: bagging_fraction=0.3\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.3, subsample=1.0 will be ignored. Current value: bagging_fraction=0.3\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.3, subsample=1.0 will be ignored. Current value: bagging_fraction=0.3\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001612 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006\n","[LightGBM] [Info] Start training from score 1.016006\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.3, subsample=1.0 will be ignored. Current value: bagging_fraction=0.3\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.3, subsample=1.0 will be ignored. Current value: bagging_fraction=0.3\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.3, subsample=1.0 will be ignored. Current value: bagging_fraction=0.3\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001493 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006\n","[LightGBM] [Info] Start training from score 1.016006\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.3, subsample=1.0 will be ignored. Current value: bagging_fraction=0.3\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.3, subsample=1.0 will be ignored. Current value: bagging_fraction=0.3\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.3, subsample=1.0 will be ignored. Current value: bagging_fraction=0.3\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003347 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006\n","[LightGBM] [Info] Start training from score 1.016006\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.3, subsample=1.0 will be ignored. Current value: bagging_fraction=0.3\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.3, subsample=1.0 will be ignored. Current value: bagging_fraction=0.3\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.3, subsample=1.0 will be ignored. Current value: bagging_fraction=0.3\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002048 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006\n","[LightGBM] [Info] Start training from score 1.016006\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.3, subsample=1.0 will be ignored. Current value: bagging_fraction=0.3\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.3, subsample=1.0 will be ignored. Current value: bagging_fraction=0.3\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.3, subsample=1.0 will be ignored. Current value: bagging_fraction=0.3\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005043 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006\n","[LightGBM] [Info] Start training from score 1.016006\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.3, subsample=1.0 will be ignored. Current value: bagging_fraction=0.3\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.3, subsample=1.0 will be ignored. Current value: bagging_fraction=0.3\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.3, subsample=1.0 will be ignored. Current value: bagging_fraction=0.3\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Info] Number of positive: 2788, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002922 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3797, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734264 -> initscore=1.016365\n","[LightGBM] [Info] Start training from score 1.016365\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.3, subsample=1.0 will be ignored. Current value: bagging_fraction=0.3\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.3, subsample=1.0 will be ignored. Current value: bagging_fraction=0.3\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.3, subsample=1.0 will be ignored. Current value: bagging_fraction=0.3\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Info] Number of positive: 2788, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002677 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3797, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734264 -> initscore=1.016365\n","[LightGBM] [Info] Start training from score 1.016365\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.3, subsample=1.0 will be ignored. Current value: bagging_fraction=0.3\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.3, subsample=1.0 will be ignored. Current value: bagging_fraction=0.3\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.3, subsample=1.0 will be ignored. Current value: bagging_fraction=0.3\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.3, subsample=1.0 will be ignored. Current value: bagging_fraction=0.3\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Info] Number of positive: 2788, number of negative: 1008\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002522 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734457 -> initscore=1.017356\n","[LightGBM] [Info] Start training from score 1.017356\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.3, subsample=1.0 will be ignored. Current value: bagging_fraction=0.3\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.3, subsample=1.0 will be ignored. Current value: bagging_fraction=0.3\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.3, subsample=1.0 will be ignored. Current value: bagging_fraction=0.3\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.3, subsample=1.0 will be ignored. Current value: bagging_fraction=0.3\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002457 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006\n","[LightGBM] [Info] Start training from score 1.016006\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.3, subsample=1.0 will be ignored. Current value: bagging_fraction=0.3\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.011399 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006\n","[LightGBM] [Info] Start training from score 1.016006\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.3, subsample=1.0 will be ignored. Current value: bagging_fraction=0.3\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.3, subsample=1.0 will be ignored. Current value: bagging_fraction=0.3\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.3, subsample=1.0 will be ignored. Current value: bagging_fraction=0.3\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.3, subsample=1.0 will be ignored. Current value: bagging_fraction=0.3\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.3, subsample=1.0 will be ignored. Current value: bagging_fraction=0.3\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.170925 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006\n","[LightGBM] [Info] Start training from score 1.016006\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.3, subsample=1.0 will be ignored. Current value: bagging_fraction=0.3\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006559 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006\n","[LightGBM] [Info] Start training from score 1.016006\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.3, subsample=1.0 will be ignored. Current value: bagging_fraction=0.3\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.3, subsample=1.0 will be ignored. Current value: bagging_fraction=0.3\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.3, subsample=1.0 will be ignored. Current value: bagging_fraction=0.3\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.3, subsample=1.0 will be ignored. Current value: bagging_fraction=0.3\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.3, subsample=1.0 will be ignored. Current value: bagging_fraction=0.3\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.3, subsample=1.0 will be ignored. Current value: bagging_fraction=0.3\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001127 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006\n","[LightGBM] [Info] Start training from score 1.016006\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.3, subsample=1.0 will be ignored. Current value: bagging_fraction=0.3\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.088838 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.3, subsample=1.0 will be ignored. Current value: bagging_fraction=0.3\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.3, subsample=1.0 will be ignored. Current value: bagging_fraction=0.3\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006\n","[LightGBM] [Info] Start training from score 1.016006\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001661 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006\n","[LightGBM] [Info] Start training from score 1.016006\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.3, subsample=1.0 will be ignored. Current value: bagging_fraction=0.3\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.3, subsample=1.0 will be ignored. Current value: bagging_fraction=0.3\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.3, subsample=1.0 will be ignored. Current value: bagging_fraction=0.3\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.3, subsample=1.0 will be ignored. Current value: bagging_fraction=0.3\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Info] Number of positive: 2788, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001516 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3797, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734264 -> initscore=1.016365\n","[LightGBM] [Info] Start training from score 1.016365\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.3, subsample=1.0 will be ignored. Current value: bagging_fraction=0.3\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.3, subsample=1.0 will be ignored. Current value: bagging_fraction=0.3\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Info] Number of positive: 2788, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002116 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3797, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734264 -> initscore=1.016365\n","[LightGBM] [Info] Start training from score 1.016365\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.3, subsample=1.0 will be ignored. Current value: bagging_fraction=0.3\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.3, subsample=1.0 will be ignored. Current value: bagging_fraction=0.3\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001151 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006\n","[LightGBM] [Info] Start training from score 1.016006\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.3, subsample=1.0 will be ignored. Current value: bagging_fraction=0.3\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.011653 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006\n","[LightGBM] [Info] Start training from score 1.016006\n","[LightGBM] [Info] Number of positive: 2788, number of negative: 1008\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008582 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734457 -> initscore=1.017356\n","[LightGBM] [Info] Start training from score 1.017356\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.3, subsample=1.0 will be ignored. Current value: bagging_fraction=0.3\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.126927 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006\n","[LightGBM] [Info] Start training from score 1.016006\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.3, subsample=1.0 will be ignored. Current value: bagging_fraction=0.3\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002271 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006\n","[LightGBM] [Info] Start training from score 1.016006\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.3, subsample=1.0 will be ignored. Current value: bagging_fraction=0.3\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.3, subsample=1.0 will be ignored. Current value: bagging_fraction=0.3\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002556 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006\n","[LightGBM] [Info] Start training from score 1.016006\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.3, subsample=1.0 will be ignored. Current value: bagging_fraction=0.3\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003469 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006\n","[LightGBM] [Info] Start training from score 1.016006\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006972 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.3, subsample=1.0 will be ignored. Current value: bagging_fraction=0.3\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006\n","[LightGBM] [Info] Start training from score 1.016006\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Info] Number of positive: 2788, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004350 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3797, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734264 -> initscore=1.016365\n","[LightGBM] [Info] Start training from score 1.016365\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.3, subsample=1.0 will be ignored. Current value: bagging_fraction=0.3\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Info] Number of positive: 2788, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001461 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3797, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734264 -> initscore=1.016365\n","[LightGBM] [Info] Start training from score 1.016365\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Info] Number of positive: 2788, number of negative: 1008\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001199 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734457 -> initscore=1.017356\n","[LightGBM] [Info] Start training from score 1.017356\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006797 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006\n","[LightGBM] [Info] Start training from score 1.016006\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001719 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006\n","[LightGBM] [Info] Start training from score 1.016006\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004654 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006\n","[LightGBM] [Info] Start training from score 1.016006\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001963 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006\n","[LightGBM] [Info] Start training from score 1.016006\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002303 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006\n","[LightGBM] [Info] Start training from score 1.016006\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001821 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006\n","[LightGBM] [Info] Start training from score 1.016006\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.031160 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006\n","[LightGBM] [Info] Start training from score 1.016006\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Info] Number of positive: 2788, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000997 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3797, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734264 -> initscore=1.016365\n","[LightGBM] [Info] Start training from score 1.016365\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Info] Number of positive: 2788, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001714 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3797, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734264 -> initscore=1.016365\n","[LightGBM] [Info] Start training from score 1.016365\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Info] Number of positive: 2788, number of negative: 1008\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002839 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734457 -> initscore=1.017356\n","[LightGBM] [Info] Start training from score 1.017356\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001428 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006\n","[LightGBM] [Info] Start training from score 1.016006\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001741 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006\n","[LightGBM] [Info] Start training from score 1.016006\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002112 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006\n","[LightGBM] [Info] Start training from score 1.016006\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002550 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006\n","[LightGBM] [Info] Start training from score 1.016006\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002268 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006\n","[LightGBM] [Info] Start training from score 1.016006\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001586 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006\n","[LightGBM] [Info] Start training from score 1.016006\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003626 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006\n","[LightGBM] [Info] Start training from score 1.016006\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Info] Number of positive: 2788, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002864 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3797, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734264 -> initscore=1.016365\n","[LightGBM] [Info] Start training from score 1.016365\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Info] Number of positive: 2788, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002079 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3797, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734264 -> initscore=1.016365\n","[LightGBM] [Info] Start training from score 1.016365\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Info] Number of positive: 2788, number of negative: 1008\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002268 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734457 -> initscore=1.017356\n","[LightGBM] [Info] Start training from score 1.017356\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002952 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006\n","[LightGBM] [Info] Start training from score 1.016006\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.126168 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006\n","[LightGBM] [Info] Start training from score 1.016006\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003229 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006\n","[LightGBM] [Info] Start training from score 1.016006\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001674 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006\n","[LightGBM] [Info] Start training from score 1.016006\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.039753 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006\n","[LightGBM] [Info] Start training from score 1.016006\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001843 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006\n","[LightGBM] [Info] Start training from score 1.016006\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Info] Number of positive: 2788, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001712 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3797, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734264 -> initscore=1.016365\n","[LightGBM] [Info] Start training from score 1.016365\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.011374 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006\n","[LightGBM] [Info] Start training from score 1.016006\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Info] Number of positive: 2788, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001998 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3797, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734264 -> initscore=1.016365\n","[LightGBM] [Info] Start training from score 1.016365\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Info] Number of positive: 2788, number of negative: 1008\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001459 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734457 -> initscore=1.017356\n","[LightGBM] [Info] Start training from score 1.017356\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007749 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006\n","[LightGBM] [Info] Start training from score 1.016006\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001977 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006\n","[LightGBM] [Info] Start training from score 1.016006\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002216 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006\n","[LightGBM] [Info] Start training from score 1.016006\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001648 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006\n","[LightGBM] [Info] Start training from score 1.016006\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002897 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006\n","[LightGBM] [Info] Start training from score 1.016006\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001622 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006\n","[LightGBM] [Info] Start training from score 1.016006\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001302 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006\n","[LightGBM] [Info] Start training from score 1.016006\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Info] Number of positive: 2788, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001833 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3797, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734264 -> initscore=1.016365\n","[LightGBM] [Info] Start training from score 1.016365\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Info] Number of positive: 2788, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001696 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3797, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734264 -> initscore=1.016365\n","[LightGBM] [Info] Start training from score 1.016365\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.1, subsample=1.0 will be ignored. Current value: bagging_fraction=0.1\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.1, subsample=1.0 will be ignored. Current value: bagging_fraction=0.1\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Info] Number of positive: 2788, number of negative: 1008\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001062 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734457 -> initscore=1.017356\n","[LightGBM] [Info] Start training from score 1.017356\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.1, subsample=1.0 will be ignored. Current value: bagging_fraction=0.1\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.1, subsample=1.0 will be ignored. Current value: bagging_fraction=0.1\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002465 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006\n","[LightGBM] [Info] Start training from score 1.016006\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.1, subsample=1.0 will be ignored. Current value: bagging_fraction=0.1\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.1, subsample=1.0 will be ignored. Current value: bagging_fraction=0.1\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001541 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006\n","[LightGBM] [Info] Start training from score 1.016006\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.1, subsample=1.0 will be ignored. Current value: bagging_fraction=0.1\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.1, subsample=1.0 will be ignored. Current value: bagging_fraction=0.1\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001450 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006\n","[LightGBM] [Info] Start training from score 1.016006\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.1, subsample=1.0 will be ignored. Current value: bagging_fraction=0.1\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.1, subsample=1.0 will be ignored. Current value: bagging_fraction=0.1\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001119 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006\n","[LightGBM] [Info] Start training from score 1.016006\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.1, subsample=1.0 will be ignored. Current value: bagging_fraction=0.1\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.1, subsample=1.0 will be ignored. Current value: bagging_fraction=0.1\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.057016 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006\n","[LightGBM] [Info] Start training from score 1.016006\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.1, subsample=1.0 will be ignored. Current value: bagging_fraction=0.1\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.1, subsample=1.0 will be ignored. Current value: bagging_fraction=0.1\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005134 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006\n","[LightGBM] [Info] Start training from score 1.016006\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.1, subsample=1.0 will be ignored. Current value: bagging_fraction=0.1\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.1, subsample=1.0 will be ignored. Current value: bagging_fraction=0.1\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001150 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006\n","[LightGBM] [Info] Start training from score 1.016006\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.1, subsample=1.0 will be ignored. Current value: bagging_fraction=0.1\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.1, subsample=1.0 will be ignored. Current value: bagging_fraction=0.1\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Info] Number of positive: 2788, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001663 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3797, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734264 -> initscore=1.016365\n","[LightGBM] [Info] Start training from score 1.016365\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.1, subsample=1.0 will be ignored. Current value: bagging_fraction=0.1\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.1, subsample=1.0 will be ignored. Current value: bagging_fraction=0.1\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Info] Number of positive: 2788, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001815 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3797, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734264 -> initscore=1.016365\n","[LightGBM] [Info] Start training from score 1.016365\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.1, subsample=1.0 will be ignored. Current value: bagging_fraction=0.1\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.1, subsample=1.0 will be ignored. Current value: bagging_fraction=0.1\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.1, subsample=1.0 will be ignored. Current value: bagging_fraction=0.1\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.1, subsample=1.0 will be ignored. Current value: bagging_fraction=0.1\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.1, subsample=1.0 will be ignored. Current value: bagging_fraction=0.1\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Info] Number of positive: 2788, number of negative: 1008\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006277 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734457 -> initscore=1.017356\n","[LightGBM] [Info] Start training from score 1.017356\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.1, subsample=1.0 will be ignored. Current value: bagging_fraction=0.1\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.055180 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006\n","[LightGBM] [Info] Start training from score 1.016006\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.1, subsample=1.0 will be ignored. Current value: bagging_fraction=0.1\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.1, subsample=1.0 will be ignored. Current value: bagging_fraction=0.1\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.1, subsample=1.0 will be ignored. Current value: bagging_fraction=0.1\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002775 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006\n","[LightGBM] [Info] Start training from score 1.016006\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.1, subsample=1.0 will be ignored. Current value: bagging_fraction=0.1\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.1, subsample=1.0 will be ignored. Current value: bagging_fraction=0.1\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.1, subsample=1.0 will be ignored. Current value: bagging_fraction=0.1\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.1, subsample=1.0 will be ignored. Current value: bagging_fraction=0.1\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002592 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006\n","[LightGBM] [Info] Start training from score 1.016006\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.1, subsample=1.0 will be ignored. Current value: bagging_fraction=0.1\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.1, subsample=1.0 will be ignored. Current value: bagging_fraction=0.1\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002542 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006\n","[LightGBM] [Info] Start training from score 1.016006\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.1, subsample=1.0 will be ignored. Current value: bagging_fraction=0.1\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.1, subsample=1.0 will be ignored. Current value: bagging_fraction=0.1\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.1, subsample=1.0 will be ignored. Current value: bagging_fraction=0.1\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002310 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006\n","[LightGBM] [Info] Start training from score 1.016006\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.1, subsample=1.0 will be ignored. Current value: bagging_fraction=0.1\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.1, subsample=1.0 will be ignored. Current value: bagging_fraction=0.1\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.1, subsample=1.0 will be ignored. Current value: bagging_fraction=0.1\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002786 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006\n","[LightGBM] [Info] Start training from score 1.016006\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.1, subsample=1.0 will be ignored. Current value: bagging_fraction=0.1\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.1, subsample=1.0 will be ignored. Current value: bagging_fraction=0.1\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.1, subsample=1.0 will be ignored. Current value: bagging_fraction=0.1\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004110 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006\n","[LightGBM] [Info] Start training from score 1.016006\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.1, subsample=1.0 will be ignored. Current value: bagging_fraction=0.1\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.1, subsample=1.0 will be ignored. Current value: bagging_fraction=0.1\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.1, subsample=1.0 will be ignored. Current value: bagging_fraction=0.1\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Info] Number of positive: 2788, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003980 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3797, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734264 -> initscore=1.016365\n","[LightGBM] [Info] Start training from score 1.016365\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.1, subsample=1.0 will be ignored. Current value: bagging_fraction=0.1\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.1, subsample=1.0 will be ignored. Current value: bagging_fraction=0.1\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.1, subsample=1.0 will be ignored. Current value: bagging_fraction=0.1\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Info] Number of positive: 2788, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002731 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3797, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734264 -> initscore=1.016365\n","[LightGBM] [Info] Start training from score 1.016365\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.1, subsample=1.0 will be ignored. Current value: bagging_fraction=0.1\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.1, subsample=1.0 will be ignored. Current value: bagging_fraction=0.1\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.1, subsample=1.0 will be ignored. Current value: bagging_fraction=0.1\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Info] Number of positive: 2788, number of negative: 1008\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003246 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734457 -> initscore=1.017356\n","[LightGBM] [Info] Start training from score 1.017356\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.1, subsample=1.0 will be ignored. Current value: bagging_fraction=0.1\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.1, subsample=1.0 will be ignored. Current value: bagging_fraction=0.1\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.1, subsample=1.0 will be ignored. Current value: bagging_fraction=0.1\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002280 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006\n","[LightGBM] [Info] Start training from score 1.016006\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.1, subsample=1.0 will be ignored. Current value: bagging_fraction=0.1\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.1, subsample=1.0 will be ignored. Current value: bagging_fraction=0.1\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.1, subsample=1.0 will be ignored. Current value: bagging_fraction=0.1\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.1, subsample=1.0 will be ignored. Current value: bagging_fraction=0.1\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003166 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006\n","[LightGBM] [Info] Start training from score 1.016006\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.1, subsample=1.0 will be ignored. Current value: bagging_fraction=0.1\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.1, subsample=1.0 will be ignored. Current value: bagging_fraction=0.1\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002776 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006\n","[LightGBM] [Info] Start training from score 1.016006\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.1, subsample=1.0 will be ignored. Current value: bagging_fraction=0.1\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.1, subsample=1.0 will be ignored. Current value: bagging_fraction=0.1\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.1, subsample=1.0 will be ignored. Current value: bagging_fraction=0.1\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.1, subsample=1.0 will be ignored. Current value: bagging_fraction=0.1\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.1, subsample=1.0 will be ignored. Current value: bagging_fraction=0.1\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.1, subsample=1.0 will be ignored. Current value: bagging_fraction=0.1\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003477 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006\n","[LightGBM] [Info] Start training from score 1.016006\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007765 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006\n","[LightGBM] [Info] Start training from score 1.016006\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.1, subsample=1.0 will be ignored. Current value: bagging_fraction=0.1\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.1, subsample=1.0 will be ignored. Current value: bagging_fraction=0.1\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.1, subsample=1.0 will be ignored. Current value: bagging_fraction=0.1\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003851 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006\n","[LightGBM] [Info] Start training from score 1.016006\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.1, subsample=1.0 will be ignored. Current value: bagging_fraction=0.1\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.1, subsample=1.0 will be ignored. Current value: bagging_fraction=0.1\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.1, subsample=1.0 will be ignored. Current value: bagging_fraction=0.1\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002160 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006\n","[LightGBM] [Info] Start training from score 1.016006\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.1, subsample=1.0 will be ignored. Current value: bagging_fraction=0.1\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.1, subsample=1.0 will be ignored. Current value: bagging_fraction=0.1\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.1, subsample=1.0 will be ignored. Current value: bagging_fraction=0.1\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Info] Number of positive: 2788, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001339 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3797, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734264 -> initscore=1.016365\n","[LightGBM] [Info] Start training from score 1.016365\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.1, subsample=1.0 will be ignored. Current value: bagging_fraction=0.1\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.1, subsample=1.0 will be ignored. Current value: bagging_fraction=0.1\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.1, subsample=1.0 will be ignored. Current value: bagging_fraction=0.1\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Info] Number of positive: 2788, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002342 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3797, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734264 -> initscore=1.016365\n","[LightGBM] [Info] Start training from score 1.016365\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.1, subsample=1.0 will be ignored. Current value: bagging_fraction=0.1\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.1, subsample=1.0 will be ignored. Current value: bagging_fraction=0.1\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.1, subsample=1.0 will be ignored. Current value: bagging_fraction=0.1\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Info] Number of positive: 2788, number of negative: 1008\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001621 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734457 -> initscore=1.017356\n","[LightGBM] [Info] Start training from score 1.017356\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.1, subsample=1.0 will be ignored. Current value: bagging_fraction=0.1\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.1, subsample=1.0 will be ignored. Current value: bagging_fraction=0.1\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.1, subsample=1.0 will be ignored. Current value: bagging_fraction=0.1\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.1, subsample=1.0 will be ignored. Current value: bagging_fraction=0.1\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.1, subsample=1.0 will be ignored. Current value: bagging_fraction=0.1\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001654 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006\n","[LightGBM] [Info] Start training from score 1.016006\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.1, subsample=1.0 will be ignored. Current value: bagging_fraction=0.1\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.1, subsample=1.0 will be ignored. Current value: bagging_fraction=0.1\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.1, subsample=1.0 will be ignored. Current value: bagging_fraction=0.1\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.1, subsample=1.0 will be ignored. Current value: bagging_fraction=0.1\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.1, subsample=1.0 will be ignored. Current value: bagging_fraction=0.1\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.1, subsample=1.0 will be ignored. Current value: bagging_fraction=0.1\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001806 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006\n","[LightGBM] [Info] Start training from score 1.016006\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008875 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006\n","[LightGBM] [Info] Start training from score 1.016006\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.1, subsample=1.0 will be ignored. Current value: bagging_fraction=0.1\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004128 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006\n","[LightGBM] [Info] Start training from score 1.016006\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.1, subsample=1.0 will be ignored. Current value: bagging_fraction=0.1\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.1, subsample=1.0 will be ignored. Current value: bagging_fraction=0.1\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.1, subsample=1.0 will be ignored. Current value: bagging_fraction=0.1\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002047 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006\n","[LightGBM] [Info] Start training from score 1.016006\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.1, subsample=1.0 will be ignored. Current value: bagging_fraction=0.1\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.1, subsample=1.0 will be ignored. Current value: bagging_fraction=0.1\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.1, subsample=1.0 will be ignored. Current value: bagging_fraction=0.1\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001719 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006\n","[LightGBM] [Info] Start training from score 1.016006\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.1, subsample=1.0 will be ignored. Current value: bagging_fraction=0.1\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.1, subsample=1.0 will be ignored. Current value: bagging_fraction=0.1\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.1, subsample=1.0 will be ignored. Current value: bagging_fraction=0.1\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002916 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006\n","[LightGBM] [Info] Start training from score 1.016006\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.1, subsample=1.0 will be ignored. Current value: bagging_fraction=0.1\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.1, subsample=1.0 will be ignored. Current value: bagging_fraction=0.1\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.1, subsample=1.0 will be ignored. Current value: bagging_fraction=0.1\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Info] Number of positive: 2788, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001827 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3797, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734264 -> initscore=1.016365\n","[LightGBM] [Info] Start training from score 1.016365\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.1, subsample=1.0 will be ignored. Current value: bagging_fraction=0.1\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.1, subsample=1.0 will be ignored. Current value: bagging_fraction=0.1\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.1, subsample=1.0 will be ignored. Current value: bagging_fraction=0.1\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Info] Number of positive: 2788, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001505 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3797, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734264 -> initscore=1.016365\n","[LightGBM] [Info] Start training from score 1.016365\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.1, subsample=1.0 will be ignored. Current value: bagging_fraction=0.1\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.1, subsample=1.0 will be ignored. Current value: bagging_fraction=0.1\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.1, subsample=1.0 will be ignored. Current value: bagging_fraction=0.1\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Info] Number of positive: 2788, number of negative: 1008\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002515 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734457 -> initscore=1.017356\n","[LightGBM] [Info] Start training from score 1.017356\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.1, subsample=1.0 will be ignored. Current value: bagging_fraction=0.1\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.1, subsample=1.0 will be ignored. Current value: bagging_fraction=0.1\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.1, subsample=1.0 will be ignored. Current value: bagging_fraction=0.1\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002243 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006\n","[LightGBM] [Info] Start training from score 1.016006\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.1, subsample=1.0 will be ignored. Current value: bagging_fraction=0.1\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.1, subsample=1.0 will be ignored. Current value: bagging_fraction=0.1\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.1, subsample=1.0 will be ignored. Current value: bagging_fraction=0.1\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001373 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006\n","[LightGBM] [Info] Start training from score 1.016006\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.1, subsample=1.0 will be ignored. Current value: bagging_fraction=0.1\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.1, subsample=1.0 will be ignored. Current value: bagging_fraction=0.1\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.1, subsample=1.0 will be ignored. Current value: bagging_fraction=0.1\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001833 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006\n","[LightGBM] [Info] Start training from score 1.016006\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.1, subsample=1.0 will be ignored. Current value: bagging_fraction=0.1\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.1, subsample=1.0 will be ignored. Current value: bagging_fraction=0.1\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.1, subsample=1.0 will be ignored. Current value: bagging_fraction=0.1\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003385 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006\n","[LightGBM] [Info] Start training from score 1.016006\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.1, subsample=1.0 will be ignored. Current value: bagging_fraction=0.1\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.1, subsample=1.0 will be ignored. Current value: bagging_fraction=0.1\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.1, subsample=1.0 will be ignored. Current value: bagging_fraction=0.1\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.033033 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006\n","[LightGBM] [Info] Start training from score 1.016006\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.1, subsample=1.0 will be ignored. Current value: bagging_fraction=0.1\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.1, subsample=1.0 will be ignored. Current value: bagging_fraction=0.1\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.1, subsample=1.0 will be ignored. Current value: bagging_fraction=0.1\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002737 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006\n","[LightGBM] [Info] Start training from score 1.016006\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.1, subsample=1.0 will be ignored. Current value: bagging_fraction=0.1\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.1, subsample=1.0 will be ignored. Current value: bagging_fraction=0.1\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.1, subsample=1.0 will be ignored. Current value: bagging_fraction=0.1\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001626 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006\n","[LightGBM] [Info] Start training from score 1.016006\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.1, subsample=1.0 will be ignored. Current value: bagging_fraction=0.1\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.1, subsample=1.0 will be ignored. Current value: bagging_fraction=0.1\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.1, subsample=1.0 will be ignored. Current value: bagging_fraction=0.1\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Info] Number of positive: 2788, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002901 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3797, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734264 -> initscore=1.016365\n","[LightGBM] [Info] Start training from score 1.016365\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.1, subsample=1.0 will be ignored. Current value: bagging_fraction=0.1\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.1, subsample=1.0 will be ignored. Current value: bagging_fraction=0.1\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.1, subsample=1.0 will be ignored. Current value: bagging_fraction=0.1\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Info] Number of positive: 2788, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001745 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3797, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734264 -> initscore=1.016365\n","[LightGBM] [Info] Start training from score 1.016365\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.1, subsample=1.0 will be ignored. Current value: bagging_fraction=0.1\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Info] Number of positive: 2788, number of negative: 1008\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001694 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734457 -> initscore=1.017356\n","[LightGBM] [Info] Start training from score 1.017356\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.1, subsample=1.0 will be ignored. Current value: bagging_fraction=0.1\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001646 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006\n","[LightGBM] [Info] Start training from score 1.016006\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.1, subsample=1.0 will be ignored. Current value: bagging_fraction=0.1\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002060 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006\n","[LightGBM] [Info] Start training from score 1.016006\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.1, subsample=1.0 will be ignored. Current value: bagging_fraction=0.1\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001554 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006\n","[LightGBM] [Info] Start training from score 1.016006\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.1, subsample=1.0 will be ignored. Current value: bagging_fraction=0.1\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001379 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006\n","[LightGBM] [Info] Start training from score 1.016006\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.1, subsample=1.0 will be ignored. Current value: bagging_fraction=0.1\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.017857 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006\n","[LightGBM] [Info] Start training from score 1.016006\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.1, subsample=1.0 will be ignored. Current value: bagging_fraction=0.1\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001801 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006\n","[LightGBM] [Info] Start training from score 1.016006\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.1, subsample=1.0 will be ignored. Current value: bagging_fraction=0.1\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000975 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006\n","[LightGBM] [Info] Start training from score 1.016006\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.1, subsample=1.0 will be ignored. Current value: bagging_fraction=0.1\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Info] Number of positive: 2788, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001813 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3797, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734264 -> initscore=1.016365\n","[LightGBM] [Info] Start training from score 1.016365\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Info] Number of positive: 2788, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001414 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3797, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734264 -> initscore=1.016365\n","[LightGBM] [Info] Start training from score 1.016365\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.1, subsample=1.0 will be ignored. Current value: bagging_fraction=0.1\n","[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Info] Number of positive: 2788, number of negative: 1008\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001135 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734457 -> initscore=1.017356\n","[LightGBM] [Info] Start training from score 1.017356\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002047 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006\n","[LightGBM] [Info] Start training from score 1.016006\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001975 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006\n","[LightGBM] [Info] Start training from score 1.016006\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012959 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006\n","[LightGBM] [Info] Start training from score 1.016006\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002148 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006\n","[LightGBM] [Info] Start training from score 1.016006\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002851 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006\n","[LightGBM] [Info] Start training from score 1.016006\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001651 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006\n","[LightGBM] [Info] Start training from score 1.016006\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002005 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006\n","[LightGBM] [Info] Start training from score 1.016006\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Info] Number of positive: 2788, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004069 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3797, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734264 -> initscore=1.016365\n","[LightGBM] [Info] Start training from score 1.016365\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Info] Number of positive: 2788, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002293 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3797, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734264 -> initscore=1.016365\n","[LightGBM] [Info] Start training from score 1.016365\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Info] Number of positive: 2788, number of negative: 1008\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005996 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734457 -> initscore=1.017356\n","[LightGBM] [Info] Start training from score 1.017356\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000972 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006\n","[LightGBM] [Info] Start training from score 1.016006\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.212197 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006\n","[LightGBM] [Info] Start training from score 1.016006\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002010 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006\n","[LightGBM] [Info] Start training from score 1.016006\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004093 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006\n","[LightGBM] [Info] Start training from score 1.016006\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004097 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006\n","[LightGBM] [Info] Start training from score 1.016006\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003377 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006\n","[LightGBM] [Info] Start training from score 1.016006\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002300 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006\n","[LightGBM] [Info] Start training from score 1.016006\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Info] Number of positive: 2788, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001881 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3797, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734264 -> initscore=1.016365\n","[LightGBM] [Info] Start training from score 1.016365\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Info] Number of positive: 2788, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001856 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3797, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734264 -> initscore=1.016365\n","[LightGBM] [Info] Start training from score 1.016365\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Info] Number of positive: 2788, number of negative: 1008\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001996 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734457 -> initscore=1.017356\n","[LightGBM] [Info] Start training from score 1.017356\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000946 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006\n","[LightGBM] [Info] Start training from score 1.016006\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002135 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006\n","[LightGBM] [Info] Start training from score 1.016006\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000933 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006\n","[LightGBM] [Info] Start training from score 1.016006\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002089 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006\n","[LightGBM] [Info] Start training from score 1.016006\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002419 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006\n","[LightGBM] [Info] Start training from score 1.016006\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004289 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006\n","[LightGBM] [Info] Start training from score 1.016006\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001595 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006\n","[LightGBM] [Info] Start training from score 1.016006\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Info] Number of positive: 2788, number of negative: 1009\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Info] Number of positive: 2788, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002961 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3797, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734264 -> initscore=1.016365\n","[LightGBM] [Info] Start training from score 1.016365\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.011400 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3797, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734264 -> initscore=1.016365\n","[LightGBM] [Info] Start training from score 1.016365\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Info] Number of positive: 2788, number of negative: 1008\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001857 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734457 -> initscore=1.017356\n","[LightGBM] [Info] Start training from score 1.017356\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000863 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006\n","[LightGBM] [Info] Start training from score 1.016006\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005098 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006\n","[LightGBM] [Info] Start training from score 1.016006\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001073 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006\n","[LightGBM] [Info] Start training from score 1.016006\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003587 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006\n","[LightGBM] [Info] Start training from score 1.016006\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001679 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006\n","[LightGBM] [Info] Start training from score 1.016006\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001899 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006\n","[LightGBM] [Info] Start training from score 1.016006\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002634 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Info] Number of positive: 2788, number of negative: 1009\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006\n","[LightGBM] [Info] Start training from score 1.016006\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000893 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3797, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734264 -> initscore=1.016365\n","[LightGBM] [Info] Start training from score 1.016365\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Info] Number of positive: 2788, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001357 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3797, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734264 -> initscore=1.016365\n","[LightGBM] [Info] Start training from score 1.016365\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n","[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n","[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","[LightGBM] [Info] Number of positive: 3097, number of negative: 1121\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004029 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 4218, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734234 -> initscore=1.016213\n","[LightGBM] [Info] Start training from score 1.016213\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","Runtime:\n","CPU times: user 864 ms, sys: 944 ms, total: 1.81 s\n","Wall time: 1min 46s\n"]}],"source":["%%time\n","\n","lgb_model = lgb.LGBMClassifier(random_state=random_state)\n","\n","lgb_params = {\n","#'objective': ['binary'],\n","'boosting_type': ['gbdt', 'dart', 'rf'],\n","'num_leaves': [1,6,8,12,22,26,28,31,35,40],\n","'learning_rate': [0.001,0.01, 0.05, 0.08, 0.09, 0.1, 0.11, 0.15, 0.2, 0.3, 0.5, 0.7, 0.8, 0.9, 1],\n","'feature_fraction': [0.1, 0.2, 0.5, 0.6, 0.8, 0.9, 1],\n","'max_depth': [1,6,8,12,15,18,22,25,30],\n","'min_data_in_leaf': [20,25,30],\n","'bagging_fraction': [0.1,0.3,0.5,0.7,1],\n","#'num_iterations': [1,6,8,12,20,22,30,35]\n","}\n","\n","lgb_clf = RandomizedSearchCV(lgb_model, lgb_params, scoring='roc_auc', n_jobs=10, cv=cv)\n","lgb_clf.fit(features_train, target_train)\n","# create a variable for the best model\n","best_lgb = lgb_clf.best_estimator_\n","lgb_pred = best_lgb.predict(features_valid)\n","print('Runtime:')"]},{"cell_type":"code","execution_count":130,"metadata":{"execution":{"iopub.execute_input":"2023-11-30T16:54:28.989434Z","iopub.status.busy":"2023-11-30T16:54:28.988544Z","iopub.status.idle":"2023-11-30T16:54:34.691369Z","shell.execute_reply":"2023-11-30T16:54:34.690609Z","shell.execute_reply.started":"2023-11-30T16:54:28.989397Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Number of positive: 2788, number of negative: 1008\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000580 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734457 -> initscore=1.017356\n","[LightGBM] [Info] Start training from score 1.017356\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000600 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006\n","[LightGBM] [Info] Start training from score 1.016006\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000579 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006\n","[LightGBM] [Info] Start training from score 1.016006\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000844 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006\n","[LightGBM] [Info] Start training from score 1.016006\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000588 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006\n","[LightGBM] [Info] Start training from score 1.016006\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000680 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006\n","[LightGBM] [Info] Start training from score 1.016006\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000690 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006\n","[LightGBM] [Info] Start training from score 1.016006\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000543 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006\n","[LightGBM] [Info] Start training from score 1.016006\n","[LightGBM] [Info] Number of positive: 2788, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000649 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3797, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734264 -> initscore=1.016365\n","[LightGBM] [Info] Start training from score 1.016365\n","[LightGBM] [Info] Number of positive: 2788, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000649 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3797, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734264 -> initscore=1.016365\n","[LightGBM] [Info] Start training from score 1.016365\n","[LightGBM] [Info] Number of positive: 2788, number of negative: 1008\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000674 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734457 -> initscore=1.017356\n","[LightGBM] [Info] Start training from score 1.017356\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000618 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006\n","[LightGBM] [Info] Start training from score 1.016006\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000600 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006\n","[LightGBM] [Info] Start training from score 1.016006\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000612 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006\n","[LightGBM] [Info] Start training from score 1.016006\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000639 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006\n","[LightGBM] [Info] Start training from score 1.016006\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000680 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006\n","[LightGBM] [Info] Start training from score 1.016006\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000647 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006\n","[LightGBM] [Info] Start training from score 1.016006\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000599 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006\n","[LightGBM] [Info] Start training from score 1.016006\n","[LightGBM] [Info] Number of positive: 2788, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000535 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3797, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734264 -> initscore=1.016365\n","[LightGBM] [Info] Start training from score 1.016365\n","[LightGBM] [Info] Number of positive: 2788, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000576 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3797, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734264 -> initscore=1.016365\n","[LightGBM] [Info] Start training from score 1.016365\n","[LightGBM] [Info] Number of positive: 2788, number of negative: 1008\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000445 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734457 -> initscore=1.017356\n","[LightGBM] [Info] Start training from score 1.017356\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000627 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006\n","[LightGBM] [Info] Start training from score 1.016006\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000439 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006\n","[LightGBM] [Info] Start training from score 1.016006\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000547 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006\n","[LightGBM] [Info] Start training from score 1.016006\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000570 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006\n","[LightGBM] [Info] Start training from score 1.016006\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000639 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006\n","[LightGBM] [Info] Start training from score 1.016006\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000547 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006\n","[LightGBM] [Info] Start training from score 1.016006\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000504 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006\n","[LightGBM] [Info] Start training from score 1.016006\n","[LightGBM] [Info] Number of positive: 2788, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000818 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3797, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734264 -> initscore=1.016365\n","[LightGBM] [Info] Start training from score 1.016365\n","[LightGBM] [Info] Number of positive: 2788, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000604 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3797, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734264 -> initscore=1.016365\n","[LightGBM] [Info] Start training from score 1.016365\n","[LightGBM] [Info] Number of positive: 2788, number of negative: 1008\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000654 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734457 -> initscore=1.017356\n","[LightGBM] [Info] Start training from score 1.017356\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000677 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006\n","[LightGBM] [Info] Start training from score 1.016006\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000698 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006\n","[LightGBM] [Info] Start training from score 1.016006\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000845 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006\n","[LightGBM] [Info] Start training from score 1.016006\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000631 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006\n","[LightGBM] [Info] Start training from score 1.016006\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000593 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006\n","[LightGBM] [Info] Start training from score 1.016006\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000599 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006\n","[LightGBM] [Info] Start training from score 1.016006\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000583 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006\n","[LightGBM] [Info] Start training from score 1.016006\n","[LightGBM] [Info] Number of positive: 2788, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000765 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3797, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734264 -> initscore=1.016365\n","[LightGBM] [Info] Start training from score 1.016365\n","[LightGBM] [Info] Number of positive: 2788, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000889 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3797, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734264 -> initscore=1.016365\n","[LightGBM] [Info] Start training from score 1.016365\n","[LightGBM] [Info] Number of positive: 2788, number of negative: 1008\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000630 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734457 -> initscore=1.017356\n","[LightGBM] [Info] Start training from score 1.017356\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000686 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006\n","[LightGBM] [Info] Start training from score 1.016006\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000644 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006\n","[LightGBM] [Info] Start training from score 1.016006\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000665 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006\n","[LightGBM] [Info] Start training from score 1.016006\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000601 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006\n","[LightGBM] [Info] Start training from score 1.016006\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000598 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006\n","[LightGBM] [Info] Start training from score 1.016006\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000626 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006\n","[LightGBM] [Info] Start training from score 1.016006\n","[LightGBM] [Info] Number of positive: 2787, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000597 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3796, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734194 -> initscore=1.016006\n","[LightGBM] [Info] Start training from score 1.016006\n","[LightGBM] [Info] Number of positive: 2788, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000603 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3797, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734264 -> initscore=1.016365\n","[LightGBM] [Info] Start training from score 1.016365\n","[LightGBM] [Info] Number of positive: 2788, number of negative: 1009\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000588 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 3797, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734264 -> initscore=1.016365\n","[LightGBM] [Info] Start training from score 1.016365\n","Cross Validation Scores: [0.92287424 0.89749424 0.92220622 0.87747696 0.92615207 0.90812212\n"," 0.89979839 0.89285714 0.92400601 0.90895169 0.87453103 0.88315092\n"," 0.90913018 0.89386521 0.9280818  0.91129032 0.93047235 0.88773041\n"," 0.92782016 0.91452843 0.88936621 0.90967742 0.91998848 0.90691244\n"," 0.896803   0.92540323 0.9203629  0.91627304 0.90531091 0.87956542\n"," 0.90652118 0.88513825 0.91071429 0.92404954 0.92315668 0.93585829\n"," 0.90374424 0.89331797 0.89820273 0.89901179 0.93034911 0.91826037\n"," 0.89017857 0.88680876 0.91751152 0.89363479 0.91687788 0.92707373\n"," 0.90406842 0.91874711]\n"]}],"source":["lgb_scores = cross_val_score(lgb_model, features_train, target_train, cv=cv, scoring='roc_auc')\n","print('Cross Validation Scores: {}'.format(lgb_scores))"]},{"cell_type":"code","execution_count":131,"metadata":{"execution":{"iopub.execute_input":"2023-11-30T16:54:34.697463Z","iopub.status.busy":"2023-11-30T16:54:34.695161Z","iopub.status.idle":"2023-11-30T16:54:34.712956Z","shell.execute_reply":"2023-11-30T16:54:34.712295Z","shell.execute_reply.started":"2023-11-30T16:54:34.697431Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Best hyperparameters: {'num_leaves': 28, 'min_data_in_leaf': 25, 'max_depth': 8, 'learning_rate': 0.15, 'feature_fraction': 0.8, 'boosting_type': 'gbdt', 'bagging_fraction': 0.7}\n","\n","Best score: 0.9063871348829992\n","\n","Average Cross Validation Score: 0.9078685635877259\n","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n","ROC AUC Score - Validation Dataset: 0.9200242272390784\n"]}],"source":["# summary\n","print('Best hyperparameters:',  lgb_clf.best_params_)\n","print()\n","print('Best score:',  lgb_clf.best_score_)\n","print()\n","print('Average Cross Validation Score: {}'.format(lgb_scores.mean()))\n","print()\n","print('ROC AUC Score - Validation Dataset:',  roc_auc_score(target_valid, lgb_clf.predict_proba(features_valid)[:, 1]))"]},{"cell_type":"markdown","metadata":{},"source":["# ROC AUC Curve - LightGBM"]},{"cell_type":"code","execution_count":132,"metadata":{"execution":{"iopub.execute_input":"2023-11-30T16:54:34.716893Z","iopub.status.busy":"2023-11-30T16:54:34.716406Z","iopub.status.idle":"2023-11-30T16:54:34.854502Z","shell.execute_reply":"2023-11-30T16:54:34.853716Z","shell.execute_reply.started":"2023-11-30T16:54:34.716859Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n","[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n","[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n"]},{"data":{"application/vnd.plotly.v1+json":{"config":{"plotlyServerURL":"https://plot.ly"},"data":[{"fillpattern":{"shape":""},"hovertemplate":"False Positive Rate=%{x}<br>True Positive Rate=%{y}<extra></extra>","legendgroup":"","line":{"color":"#636efa"},"marker":{"symbol":"circle"},"mode":"lines","name":"","orientation":"v","showlegend":false,"stackgroup":"1","type":"scatter","x":[0,0,0,0.00267379679144385,0.00267379679144385,0.0053475935828877,0.0053475935828877,0.008021390374331552,0.008021390374331552,0.0106951871657754,0.0106951871657754,0.0106951871657754,0.0106951871657754,0.0106951871657754,0.0106951871657754,0.0106951871657754,0.0106951871657754,0.013368983957219251,0.013368983957219251,0.016042780748663103,0.016042780748663103,0.01871657754010695,0.01871657754010695,0.0213903743315508,0.0213903743315508,0.02406417112299465,0.02406417112299465,0.026737967914438502,0.026737967914438502,0.029411764705882353,0.029411764705882353,0.029411764705882353,0.029411764705882353,0.029411764705882353,0.029411764705882353,0.03208556149732621,0.03208556149732621,0.034759358288770054,0.034759358288770054,0.034759358288770054,0.034759358288770054,0.0374331550802139,0.0374331550802139,0.040106951871657755,0.040106951871657755,0.0427807486631016,0.0427807486631016,0.0481283422459893,0.0481283422459893,0.053475935828877004,0.053475935828877004,0.05614973262032086,0.05614973262032086,0.058823529411764705,0.058823529411764705,0.06149732620320856,0.06149732620320856,0.06417112299465241,0.06417112299465241,0.06684491978609626,0.06684491978609626,0.07219251336898395,0.07219251336898395,0.0748663101604278,0.0748663101604278,0.08021390374331551,0.08021390374331551,0.08288770053475936,0.08288770053475936,0.0855614973262032,0.0855614973262032,0.08823529411764706,0.08823529411764706,0.09358288770053476,0.09358288770053476,0.0962566844919786,0.0962566844919786,0.09893048128342247,0.09893048128342247,0.10160427807486631,0.10160427807486631,0.10427807486631016,0.10427807486631016,0.10695187165775401,0.10695187165775401,0.10962566844919786,0.10962566844919786,0.11229946524064172,0.11229946524064172,0.11497326203208556,0.11497326203208556,0.11764705882352941,0.11764705882352941,0.12032085561497326,0.12032085561497326,0.12299465240641712,0.12299465240641712,0.12566844919786097,0.12566844919786097,0.12834224598930483,0.12834224598930483,0.13101604278074866,0.13101604278074866,0.13368983957219252,0.13368983957219252,0.13636363636363635,0.13636363636363635,0.13903743315508021,0.13903743315508021,0.14171122994652408,0.14171122994652408,0.1443850267379679,0.1443850267379679,0.14705882352941177,0.14705882352941177,0.1497326203208556,0.1497326203208556,0.15240641711229946,0.15240641711229946,0.16042780748663102,0.16042780748663102,0.16310160427807488,0.16310160427807488,0.1657754010695187,0.1657754010695187,0.1711229946524064,0.1711229946524064,0.18181818181818182,0.18181818181818182,0.18716577540106952,0.18716577540106952,0.18983957219251338,0.18983957219251338,0.19518716577540107,0.19518716577540107,0.19786096256684493,0.19786096256684493,0.20053475935828877,0.20053475935828877,0.20320855614973263,0.20320855614973263,0.20855614973262032,0.20855614973262032,0.21122994652406418,0.21122994652406418,0.21390374331550802,0.21390374331550802,0.2192513368983957,0.2192513368983957,0.22192513368983957,0.22192513368983957,0.22459893048128343,0.22459893048128343,0.22727272727272727,0.22727272727272727,0.22994652406417113,0.22994652406417113,0.23529411764705882,0.23529411764705882,0.23796791443850268,0.23796791443850268,0.24064171122994651,0.24064171122994651,0.24331550802139038,0.24331550802139038,0.24598930481283424,0.24598930481283424,0.24866310160427807,0.24866310160427807,0.25133689839572193,0.25133689839572193,0.2540106951871658,0.2540106951871658,0.2620320855614973,0.2620320855614973,0.26737967914438504,0.26737967914438504,0.27540106951871657,0.27540106951871657,0.28342245989304815,0.28342245989304815,0.2887700534759358,0.2887700534759358,0.2914438502673797,0.2914438502673797,0.29411764705882354,0.29411764705882354,0.2967914438502674,0.2967914438502674,0.2994652406417112,0.2994652406417112,0.31016042780748665,0.31016042780748665,0.31283422459893045,0.31283422459893045,0.3155080213903743,0.3155080213903743,0.32085561497326204,0.32085561497326204,0.32887700534759357,0.32887700534759357,0.33689839572192515,0.33689839572192515,0.339572192513369,0.339572192513369,0.3449197860962567,0.3449197860962567,0.34759358288770054,0.34759358288770054,0.3502673796791444,0.3502673796791444,0.35561497326203206,0.35561497326203206,0.3663101604278075,0.3663101604278075,0.3689839572192513,0.3689839572192513,0.37433155080213903,0.37433155080213903,0.3770053475935829,0.3770053475935829,0.37967914438502676,0.37967914438502676,0.38235294117647056,0.38235294117647056,0.3877005347593583,0.3877005347593583,0.393048128342246,0.393048128342246,0.40106951871657753,0.40106951871657753,0.4037433155080214,0.4037433155080214,0.4144385026737968,0.4144385026737968,0.41711229946524064,0.41711229946524064,0.42245989304812837,0.42245989304812837,0.4358288770053476,0.4358288770053476,0.45454545454545453,0.45454545454545453,0.4572192513368984,0.4572192513368984,0.45989304812834225,0.45989304812834225,0.4732620320855615,0.4732620320855615,0.5775401069518716,0.5828877005347594,0.5962566844919787,0.5962566844919787,0.9117647058823529,0.9171122994652406,0.9705882352941176,0.9759358288770054,1],"xaxis":"x","y":[0,0.000968054211035818,0.002904162633107454,0.002904162633107454,0.04259438528557599,0.04259438528557599,0.044530493707647625,0.044530493707647625,0.07744433688286544,0.07744433688286544,0.10648596321393998,0.10842207163601161,0.1181026137463698,0.12003872216844143,0.12487899322362052,0.12681510164569215,0.18102613746369797,0.18102613746369797,0.18586640851887706,0.18586640851887706,0.3117134559535334,0.3117134559535334,0.31461761858664083,0.31461761858664083,0.38818973862536305,0.38818973862536305,0.39399806389157793,0.39399806389157793,0.39496611810261373,0.39496611810261373,0.3978702807357212,0.3998063891577928,0.41045498547918685,0.41239109390125844,0.4269119070667957,0.4269119070667957,0.43272023233301066,0.43272023233301066,0.48209099709583736,0.484027105517909,0.5256534365924492,0.5256534365924492,0.5314617618586641,0.5314617618586641,0.5363020329138432,0.5363020329138432,0.542110358180058,0.542110358180058,0.5750242013552759,0.5750242013552759,0.5847047434656341,0.5847047434656341,0.6001936108422071,0.6001936108422071,0.6050338818973863,0.6050338818973863,0.611810261374637,0.611810261374637,0.616650532429816,0.616650532429816,0.6340755082284608,0.6340755082284608,0.6389157792836399,0.6389157792836399,0.6698935140367861,0.6698935140367861,0.6776379477250726,0.6776379477250726,0.6873184898354308,0.6873184898354308,0.6960309777347532,0.6960309777347532,0.7066795740561471,0.7066795740561471,0.712487899322362,0.712487899322362,0.7260406582768635,0.7260406582768635,0.7279767666989352,0.7279767666989352,0.7454017424975798,0.7454017424975798,0.7492739593417231,0.7492739593417231,0.7637947725072604,0.7637947725072604,0.7657308809293321,0.7657308809293321,0.7725072604065828,0.7725072604065828,0.782187802516941,0.782187802516941,0.7850919651500484,0.7850919651500484,0.7889641819941917,0.7889641819941917,0.7918683446272992,0.7918683446272992,0.8083252662149081,0.8083252662149081,0.8121974830590513,0.8121974830590513,0.814133591481123,0.814133591481123,0.8151016456921588,0.8151016456921588,0.8238141335914811,0.8238141335914811,0.8257502420135527,0.8257502420135527,0.8315585672797676,0.8315585672797676,0.8334946757018393,0.8334946757018393,0.8373668925459826,0.8373668925459826,0.8402710551790901,0.8402710551790901,0.8441432720232332,0.8441432720232332,0.8499515972894482,0.8499515972894482,0.850919651500484,0.850919651500484,0.8557599225556631,0.8557599225556631,0.856727976766699,0.856727976766699,0.8596321393998064,0.8596321393998064,0.861568247821878,0.861568247821878,0.8635043562439496,0.8635043562439496,0.8731848983543078,0.8731848983543078,0.8760890609874153,0.8760890609874153,0.8770571151984511,0.8770571151984511,0.8780251694094869,0.8780251694094869,0.8857696030977735,0.8857696030977735,0.8915779283639884,0.8915779283639884,0.89351403678606,0.89351403678606,0.9031945788964182,0.9031945788964182,0.904162633107454,0.904162633107454,0.9051306873184899,0.9051306873184899,0.9060987415295256,0.9060987415295256,0.9080348499515973,0.9080348499515973,0.9090029041626331,0.9090029041626331,0.9109390125847048,0.9109390125847048,0.914811229428848,0.914811229428848,0.9157792836398838,0.9157792836398838,0.9167473378509197,0.9167473378509197,0.9177153920619555,0.9177153920619555,0.9293320425943853,0.9293320425943853,0.9322362052274927,0.9322362052274927,0.9380445304937076,0.9380445304937076,0.9399806389157793,0.9399806389157793,0.9409486931268151,0.9409486931268151,0.9419167473378509,0.9419167473378509,0.9428848015488868,0.9428848015488868,0.9438528557599225,0.9438528557599225,0.9486931268151017,0.9486931268151017,0.9545014520813165,0.9545014520813165,0.9554695062923524,0.9554695062923524,0.9564375605033882,0.9564375605033882,0.9583736689254598,0.9583736689254598,0.9593417231364957,0.9593417231364957,0.9603097773475314,0.9603097773475314,0.9612778315585673,0.9612778315585673,0.9632139399806389,0.9632139399806389,0.9641819941916747,0.9641819941916747,0.9651500484027106,0.9651500484027106,0.9661181026137464,0.9661181026137464,0.9670861568247822,0.9670861568247822,0.9690222652468539,0.9690222652468539,0.9709583736689255,0.9709583736689255,0.9748305905130688,0.9748305905130688,0.9757986447241046,0.9757986447241046,0.9767666989351403,0.9767666989351403,0.978702807357212,0.978702807357212,0.9796708615682478,0.9796708615682478,0.9806389157792836,0.9806389157792836,0.9816069699903195,0.9816069699903195,0.9845111326234269,0.9845111326234269,0.9864472410454985,0.9864472410454985,0.9874152952565344,0.9874152952565344,0.989351403678606,0.989351403678606,0.9903194578896418,0.9903194578896418,0.9932236205227493,0.9932236205227493,0.9941916747337851,0.9941916747337851,0.995159728944821,0.995159728944821,0.9980638915779284,0.9980638915779284,0.9990319457889641,0.9990319457889641,0.9990319457889641,0.9990319457889641,1,1,1,1,1,1],"yaxis":"y"}],"layout":{"height":600,"legend":{"tracegroupgap":0},"shapes":[{"line":{"dash":"dash"},"type":"line","x0":0,"x1":1,"y0":0,"y1":1}],"showlegend":false,"template":{"data":{"bar":[{"error_x":{"color":"#2a3f5f"},"error_y":{"color":"#2a3f5f"},"marker":{"line":{"color":"white","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"barpolar":[{"marker":{"line":{"color":"white","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"carpet":[{"aaxis":{"endlinecolor":"#2a3f5f","gridcolor":"#C8D4E3","linecolor":"#C8D4E3","minorgridcolor":"#C8D4E3","startlinecolor":"#2a3f5f"},"baxis":{"endlinecolor":"#2a3f5f","gridcolor":"#C8D4E3","linecolor":"#C8D4E3","minorgridcolor":"#C8D4E3","startlinecolor":"#2a3f5f"},"type":"carpet"}],"choropleth":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"choropleth"}],"contour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"contour"}],"contourcarpet":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"contourcarpet"}],"heatmap":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmap"}],"heatmapgl":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmapgl"}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"histogram2d":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2d"}],"histogram2dcontour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2dcontour"}],"mesh3d":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"mesh3d"}],"parcoords":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"parcoords"}],"pie":[{"automargin":true,"type":"pie"}],"scatter":[{"fillpattern":{"fillmode":"overlay","size":10,"solidity":0.2},"type":"scatter"}],"scatter3d":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter3d"}],"scattercarpet":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattercarpet"}],"scattergeo":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergeo"}],"scattergl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergl"}],"scattermapbox":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattermapbox"}],"scatterpolar":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolar"}],"scatterpolargl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolargl"}],"scatterternary":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterternary"}],"surface":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"surface"}],"table":[{"cells":{"fill":{"color":"#EBF0F8"},"line":{"color":"white"}},"header":{"fill":{"color":"#C8D4E3"},"line":{"color":"white"}},"type":"table"}]},"layout":{"annotationdefaults":{"arrowcolor":"#2a3f5f","arrowhead":0,"arrowwidth":1},"autotypenumbers":"strict","coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]],"sequential":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"sequentialminus":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]]},"colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#2a3f5f"},"geo":{"bgcolor":"white","lakecolor":"white","landcolor":"white","showlakes":true,"showland":true,"subunitcolor":"#C8D4E3"},"hoverlabel":{"align":"left"},"hovermode":"closest","mapbox":{"style":"light"},"paper_bgcolor":"white","plot_bgcolor":"white","polar":{"angularaxis":{"gridcolor":"#EBF0F8","linecolor":"#EBF0F8","ticks":""},"bgcolor":"white","radialaxis":{"gridcolor":"#EBF0F8","linecolor":"#EBF0F8","ticks":""}},"scene":{"xaxis":{"backgroundcolor":"white","gridcolor":"#DFE8F3","gridwidth":2,"linecolor":"#EBF0F8","showbackground":true,"ticks":"","zerolinecolor":"#EBF0F8"},"yaxis":{"backgroundcolor":"white","gridcolor":"#DFE8F3","gridwidth":2,"linecolor":"#EBF0F8","showbackground":true,"ticks":"","zerolinecolor":"#EBF0F8"},"zaxis":{"backgroundcolor":"white","gridcolor":"#DFE8F3","gridwidth":2,"linecolor":"#EBF0F8","showbackground":true,"ticks":"","zerolinecolor":"#EBF0F8"}},"shapedefaults":{"line":{"color":"#2a3f5f"}},"ternary":{"aaxis":{"gridcolor":"#DFE8F3","linecolor":"#A2B1C6","ticks":""},"baxis":{"gridcolor":"#DFE8F3","linecolor":"#A2B1C6","ticks":""},"bgcolor":"white","caxis":{"gridcolor":"#DFE8F3","linecolor":"#A2B1C6","ticks":""}},"title":{"x":0.05},"xaxis":{"automargin":true,"gridcolor":"#EBF0F8","linecolor":"#EBF0F8","ticks":"","title":{"standoff":15},"zerolinecolor":"#EBF0F8","zerolinewidth":2},"yaxis":{"automargin":true,"gridcolor":"#EBF0F8","linecolor":"#EBF0F8","ticks":"","title":{"standoff":15},"zerolinecolor":"#EBF0F8","zerolinewidth":2}}},"title":{"text":"ROC Curve (AUC=0.9200)"},"width":800,"xaxis":{"anchor":"y","constrain":"domain","domain":[0,1],"title":{"text":"False Positive Rate"}},"yaxis":{"anchor":"x","domain":[0,1],"scaleanchor":"x","scaleratio":1,"title":{"text":"True Positive Rate"}}}},"text/html":["<div>                            <div id=\"d996e462-81d6-4a51-86df-bc29273e3dcc\" class=\"plotly-graph-div\" style=\"height:600px; width:800px;\"></div>            <script type=\"text/javascript\">                require([\"plotly\"], function(Plotly) {                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"d996e462-81d6-4a51-86df-bc29273e3dcc\")) {                    Plotly.newPlot(                        \"d996e462-81d6-4a51-86df-bc29273e3dcc\",                        [{\"fillpattern\":{\"shape\":\"\"},\"hovertemplate\":\"False Positive Rate=%{x}\\u003cbr\\u003eTrue Positive Rate=%{y}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"\",\"line\":{\"color\":\"#636efa\"},\"marker\":{\"symbol\":\"circle\"},\"mode\":\"lines\",\"name\":\"\",\"orientation\":\"v\",\"showlegend\":false,\"stackgroup\":\"1\",\"x\":[0.0,0.0,0.0,0.00267379679144385,0.00267379679144385,0.0053475935828877,0.0053475935828877,0.008021390374331552,0.008021390374331552,0.0106951871657754,0.0106951871657754,0.0106951871657754,0.0106951871657754,0.0106951871657754,0.0106951871657754,0.0106951871657754,0.0106951871657754,0.013368983957219251,0.013368983957219251,0.016042780748663103,0.016042780748663103,0.01871657754010695,0.01871657754010695,0.0213903743315508,0.0213903743315508,0.02406417112299465,0.02406417112299465,0.026737967914438502,0.026737967914438502,0.029411764705882353,0.029411764705882353,0.029411764705882353,0.029411764705882353,0.029411764705882353,0.029411764705882353,0.03208556149732621,0.03208556149732621,0.034759358288770054,0.034759358288770054,0.034759358288770054,0.034759358288770054,0.0374331550802139,0.0374331550802139,0.040106951871657755,0.040106951871657755,0.0427807486631016,0.0427807486631016,0.0481283422459893,0.0481283422459893,0.053475935828877004,0.053475935828877004,0.05614973262032086,0.05614973262032086,0.058823529411764705,0.058823529411764705,0.06149732620320856,0.06149732620320856,0.06417112299465241,0.06417112299465241,0.06684491978609626,0.06684491978609626,0.07219251336898395,0.07219251336898395,0.0748663101604278,0.0748663101604278,0.08021390374331551,0.08021390374331551,0.08288770053475936,0.08288770053475936,0.0855614973262032,0.0855614973262032,0.08823529411764706,0.08823529411764706,0.09358288770053476,0.09358288770053476,0.0962566844919786,0.0962566844919786,0.09893048128342247,0.09893048128342247,0.10160427807486631,0.10160427807486631,0.10427807486631016,0.10427807486631016,0.10695187165775401,0.10695187165775401,0.10962566844919786,0.10962566844919786,0.11229946524064172,0.11229946524064172,0.11497326203208556,0.11497326203208556,0.11764705882352941,0.11764705882352941,0.12032085561497326,0.12032085561497326,0.12299465240641712,0.12299465240641712,0.12566844919786097,0.12566844919786097,0.12834224598930483,0.12834224598930483,0.13101604278074866,0.13101604278074866,0.13368983957219252,0.13368983957219252,0.13636363636363635,0.13636363636363635,0.13903743315508021,0.13903743315508021,0.14171122994652408,0.14171122994652408,0.1443850267379679,0.1443850267379679,0.14705882352941177,0.14705882352941177,0.1497326203208556,0.1497326203208556,0.15240641711229946,0.15240641711229946,0.16042780748663102,0.16042780748663102,0.16310160427807488,0.16310160427807488,0.1657754010695187,0.1657754010695187,0.1711229946524064,0.1711229946524064,0.18181818181818182,0.18181818181818182,0.18716577540106952,0.18716577540106952,0.18983957219251338,0.18983957219251338,0.19518716577540107,0.19518716577540107,0.19786096256684493,0.19786096256684493,0.20053475935828877,0.20053475935828877,0.20320855614973263,0.20320855614973263,0.20855614973262032,0.20855614973262032,0.21122994652406418,0.21122994652406418,0.21390374331550802,0.21390374331550802,0.2192513368983957,0.2192513368983957,0.22192513368983957,0.22192513368983957,0.22459893048128343,0.22459893048128343,0.22727272727272727,0.22727272727272727,0.22994652406417113,0.22994652406417113,0.23529411764705882,0.23529411764705882,0.23796791443850268,0.23796791443850268,0.24064171122994651,0.24064171122994651,0.24331550802139038,0.24331550802139038,0.24598930481283424,0.24598930481283424,0.24866310160427807,0.24866310160427807,0.25133689839572193,0.25133689839572193,0.2540106951871658,0.2540106951871658,0.2620320855614973,0.2620320855614973,0.26737967914438504,0.26737967914438504,0.27540106951871657,0.27540106951871657,0.28342245989304815,0.28342245989304815,0.2887700534759358,0.2887700534759358,0.2914438502673797,0.2914438502673797,0.29411764705882354,0.29411764705882354,0.2967914438502674,0.2967914438502674,0.2994652406417112,0.2994652406417112,0.31016042780748665,0.31016042780748665,0.31283422459893045,0.31283422459893045,0.3155080213903743,0.3155080213903743,0.32085561497326204,0.32085561497326204,0.32887700534759357,0.32887700534759357,0.33689839572192515,0.33689839572192515,0.339572192513369,0.339572192513369,0.3449197860962567,0.3449197860962567,0.34759358288770054,0.34759358288770054,0.3502673796791444,0.3502673796791444,0.35561497326203206,0.35561497326203206,0.3663101604278075,0.3663101604278075,0.3689839572192513,0.3689839572192513,0.37433155080213903,0.37433155080213903,0.3770053475935829,0.3770053475935829,0.37967914438502676,0.37967914438502676,0.38235294117647056,0.38235294117647056,0.3877005347593583,0.3877005347593583,0.393048128342246,0.393048128342246,0.40106951871657753,0.40106951871657753,0.4037433155080214,0.4037433155080214,0.4144385026737968,0.4144385026737968,0.41711229946524064,0.41711229946524064,0.42245989304812837,0.42245989304812837,0.4358288770053476,0.4358288770053476,0.45454545454545453,0.45454545454545453,0.4572192513368984,0.4572192513368984,0.45989304812834225,0.45989304812834225,0.4732620320855615,0.4732620320855615,0.5775401069518716,0.5828877005347594,0.5962566844919787,0.5962566844919787,0.9117647058823529,0.9171122994652406,0.9705882352941176,0.9759358288770054,1.0],\"xaxis\":\"x\",\"y\":[0.0,0.000968054211035818,0.002904162633107454,0.002904162633107454,0.04259438528557599,0.04259438528557599,0.044530493707647625,0.044530493707647625,0.07744433688286544,0.07744433688286544,0.10648596321393998,0.10842207163601161,0.1181026137463698,0.12003872216844143,0.12487899322362052,0.12681510164569215,0.18102613746369797,0.18102613746369797,0.18586640851887706,0.18586640851887706,0.3117134559535334,0.3117134559535334,0.31461761858664083,0.31461761858664083,0.38818973862536305,0.38818973862536305,0.39399806389157793,0.39399806389157793,0.39496611810261373,0.39496611810261373,0.3978702807357212,0.3998063891577928,0.41045498547918685,0.41239109390125844,0.4269119070667957,0.4269119070667957,0.43272023233301066,0.43272023233301066,0.48209099709583736,0.484027105517909,0.5256534365924492,0.5256534365924492,0.5314617618586641,0.5314617618586641,0.5363020329138432,0.5363020329138432,0.542110358180058,0.542110358180058,0.5750242013552759,0.5750242013552759,0.5847047434656341,0.5847047434656341,0.6001936108422071,0.6001936108422071,0.6050338818973863,0.6050338818973863,0.611810261374637,0.611810261374637,0.616650532429816,0.616650532429816,0.6340755082284608,0.6340755082284608,0.6389157792836399,0.6389157792836399,0.6698935140367861,0.6698935140367861,0.6776379477250726,0.6776379477250726,0.6873184898354308,0.6873184898354308,0.6960309777347532,0.6960309777347532,0.7066795740561471,0.7066795740561471,0.712487899322362,0.712487899322362,0.7260406582768635,0.7260406582768635,0.7279767666989352,0.7279767666989352,0.7454017424975798,0.7454017424975798,0.7492739593417231,0.7492739593417231,0.7637947725072604,0.7637947725072604,0.7657308809293321,0.7657308809293321,0.7725072604065828,0.7725072604065828,0.782187802516941,0.782187802516941,0.7850919651500484,0.7850919651500484,0.7889641819941917,0.7889641819941917,0.7918683446272992,0.7918683446272992,0.8083252662149081,0.8083252662149081,0.8121974830590513,0.8121974830590513,0.814133591481123,0.814133591481123,0.8151016456921588,0.8151016456921588,0.8238141335914811,0.8238141335914811,0.8257502420135527,0.8257502420135527,0.8315585672797676,0.8315585672797676,0.8334946757018393,0.8334946757018393,0.8373668925459826,0.8373668925459826,0.8402710551790901,0.8402710551790901,0.8441432720232332,0.8441432720232332,0.8499515972894482,0.8499515972894482,0.850919651500484,0.850919651500484,0.8557599225556631,0.8557599225556631,0.856727976766699,0.856727976766699,0.8596321393998064,0.8596321393998064,0.861568247821878,0.861568247821878,0.8635043562439496,0.8635043562439496,0.8731848983543078,0.8731848983543078,0.8760890609874153,0.8760890609874153,0.8770571151984511,0.8770571151984511,0.8780251694094869,0.8780251694094869,0.8857696030977735,0.8857696030977735,0.8915779283639884,0.8915779283639884,0.89351403678606,0.89351403678606,0.9031945788964182,0.9031945788964182,0.904162633107454,0.904162633107454,0.9051306873184899,0.9051306873184899,0.9060987415295256,0.9060987415295256,0.9080348499515973,0.9080348499515973,0.9090029041626331,0.9090029041626331,0.9109390125847048,0.9109390125847048,0.914811229428848,0.914811229428848,0.9157792836398838,0.9157792836398838,0.9167473378509197,0.9167473378509197,0.9177153920619555,0.9177153920619555,0.9293320425943853,0.9293320425943853,0.9322362052274927,0.9322362052274927,0.9380445304937076,0.9380445304937076,0.9399806389157793,0.9399806389157793,0.9409486931268151,0.9409486931268151,0.9419167473378509,0.9419167473378509,0.9428848015488868,0.9428848015488868,0.9438528557599225,0.9438528557599225,0.9486931268151017,0.9486931268151017,0.9545014520813165,0.9545014520813165,0.9554695062923524,0.9554695062923524,0.9564375605033882,0.9564375605033882,0.9583736689254598,0.9583736689254598,0.9593417231364957,0.9593417231364957,0.9603097773475314,0.9603097773475314,0.9612778315585673,0.9612778315585673,0.9632139399806389,0.9632139399806389,0.9641819941916747,0.9641819941916747,0.9651500484027106,0.9651500484027106,0.9661181026137464,0.9661181026137464,0.9670861568247822,0.9670861568247822,0.9690222652468539,0.9690222652468539,0.9709583736689255,0.9709583736689255,0.9748305905130688,0.9748305905130688,0.9757986447241046,0.9757986447241046,0.9767666989351403,0.9767666989351403,0.978702807357212,0.978702807357212,0.9796708615682478,0.9796708615682478,0.9806389157792836,0.9806389157792836,0.9816069699903195,0.9816069699903195,0.9845111326234269,0.9845111326234269,0.9864472410454985,0.9864472410454985,0.9874152952565344,0.9874152952565344,0.989351403678606,0.989351403678606,0.9903194578896418,0.9903194578896418,0.9932236205227493,0.9932236205227493,0.9941916747337851,0.9941916747337851,0.995159728944821,0.995159728944821,0.9980638915779284,0.9980638915779284,0.9990319457889641,0.9990319457889641,0.9990319457889641,0.9990319457889641,1.0,1.0,1.0,1.0,1.0,1.0],\"yaxis\":\"y\",\"type\":\"scatter\"}],                        {\"template\":{\"data\":{\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"white\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"white\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"#C8D4E3\",\"linecolor\":\"#C8D4E3\",\"minorgridcolor\":\"#C8D4E3\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"#C8D4E3\",\"linecolor\":\"#C8D4E3\",\"minorgridcolor\":\"#C8D4E3\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"choropleth\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"choropleth\"}],\"contourcarpet\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"contourcarpet\"}],\"contour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"contour\"}],\"heatmapgl\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmapgl\"}],\"heatmap\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmap\"}],\"histogram2dcontour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2dcontour\"}],\"histogram2d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2d\"}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"mesh3d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"mesh3d\"}],\"parcoords\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"parcoords\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}],\"scatter3d\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatter3d\"}],\"scattercarpet\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattercarpet\"}],\"scattergeo\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergeo\"}],\"scattergl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergl\"}],\"scattermapbox\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattermapbox\"}],\"scatterpolargl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolargl\"}],\"scatterpolar\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolar\"}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"scatterternary\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterternary\"}],\"surface\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"surface\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}]},\"layout\":{\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"autotypenumbers\":\"strict\",\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]],\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]},\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"geo\":{\"bgcolor\":\"white\",\"lakecolor\":\"white\",\"landcolor\":\"white\",\"showlakes\":true,\"showland\":true,\"subunitcolor\":\"#C8D4E3\"},\"hoverlabel\":{\"align\":\"left\"},\"hovermode\":\"closest\",\"mapbox\":{\"style\":\"light\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"white\",\"polar\":{\"angularaxis\":{\"gridcolor\":\"#EBF0F8\",\"linecolor\":\"#EBF0F8\",\"ticks\":\"\"},\"bgcolor\":\"white\",\"radialaxis\":{\"gridcolor\":\"#EBF0F8\",\"linecolor\":\"#EBF0F8\",\"ticks\":\"\"}},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"white\",\"gridcolor\":\"#DFE8F3\",\"gridwidth\":2,\"linecolor\":\"#EBF0F8\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"#EBF0F8\"},\"yaxis\":{\"backgroundcolor\":\"white\",\"gridcolor\":\"#DFE8F3\",\"gridwidth\":2,\"linecolor\":\"#EBF0F8\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"#EBF0F8\"},\"zaxis\":{\"backgroundcolor\":\"white\",\"gridcolor\":\"#DFE8F3\",\"gridwidth\":2,\"linecolor\":\"#EBF0F8\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"#EBF0F8\"}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"ternary\":{\"aaxis\":{\"gridcolor\":\"#DFE8F3\",\"linecolor\":\"#A2B1C6\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"#DFE8F3\",\"linecolor\":\"#A2B1C6\",\"ticks\":\"\"},\"bgcolor\":\"white\",\"caxis\":{\"gridcolor\":\"#DFE8F3\",\"linecolor\":\"#A2B1C6\",\"ticks\":\"\"}},\"title\":{\"x\":0.05},\"xaxis\":{\"automargin\":true,\"gridcolor\":\"#EBF0F8\",\"linecolor\":\"#EBF0F8\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"#EBF0F8\",\"zerolinewidth\":2},\"yaxis\":{\"automargin\":true,\"gridcolor\":\"#EBF0F8\",\"linecolor\":\"#EBF0F8\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"#EBF0F8\",\"zerolinewidth\":2}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"False Positive Rate\"},\"constrain\":\"domain\"},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"True Positive Rate\"},\"scaleanchor\":\"x\",\"scaleratio\":1},\"legend\":{\"tracegroupgap\":0},\"title\":{\"text\":\"ROC Curve (AUC=0.9200)\"},\"height\":600,\"width\":800,\"shapes\":[{\"line\":{\"dash\":\"dash\"},\"type\":\"line\",\"x0\":0,\"x1\":1,\"y0\":0,\"y1\":1}],\"showlegend\":false},                        {\"responsive\": true}                    ).then(function(){\n","                            \n","var gd = document.getElementById('d996e462-81d6-4a51-86df-bc29273e3dcc');\n","var x = new MutationObserver(function (mutations, observer) {{\n","        var display = window.getComputedStyle(gd).display;\n","        if (!display || display === 'none') {{\n","            console.log([gd, 'removed!']);\n","            Plotly.purge(gd);\n","            observer.disconnect();\n","        }}\n","}});\n","\n","// Listen for the removal of the full notebook cells\n","var notebookContainer = gd.closest('#notebook-container');\n","if (notebookContainer) {{\n","    x.observe(notebookContainer, {childList: true});\n","}}\n","\n","// Listen for the clearing of the current output cell\n","var outputEl = gd.closest('.output');\n","if (outputEl) {{\n","    x.observe(outputEl, {childList: true});\n","}}\n","\n","                        })                };                });            </script>        </div>"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.plotly.v1+json":{"config":{"plotlyServerURL":"https://plot.ly"},"data":[{"fillpattern":{"shape":""},"hovertemplate":"Recall=%{x}<br>Precision=%{y}<extra></extra>","legendgroup":"","line":{"color":"#636efa"},"marker":{"symbol":"circle"},"mode":"lines","name":"","orientation":"v","showlegend":false,"stackgroup":"1","type":"scatter","x":[1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,0.9990319457889641,0.9990319457889641,0.9990319457889641,0.9990319457889641,0.9990319457889641,0.9990319457889641,0.9990319457889641,0.9990319457889641,0.9990319457889641,0.9990319457889641,0.9990319457889641,0.9990319457889641,0.9990319457889641,0.9990319457889641,0.9990319457889641,0.9990319457889641,0.9990319457889641,0.9990319457889641,0.9990319457889641,0.9990319457889641,0.9990319457889641,0.9990319457889641,0.9990319457889641,0.9990319457889641,0.9990319457889641,0.9990319457889641,0.9990319457889641,0.9990319457889641,0.9990319457889641,0.9990319457889641,0.9990319457889641,0.9990319457889641,0.9990319457889641,0.9990319457889641,0.9990319457889641,0.9990319457889641,0.9990319457889641,0.9990319457889641,0.9990319457889641,0.9990319457889641,0.9990319457889641,0.9990319457889641,0.9990319457889641,0.9990319457889641,0.9990319457889641,0.9990319457889641,0.9980638915779284,0.9980638915779284,0.9980638915779284,0.9980638915779284,0.9980638915779284,0.9980638915779284,0.9970958373668926,0.9961277831558567,0.995159728944821,0.995159728944821,0.9941916747337851,0.9941916747337851,0.9932236205227493,0.9932236205227493,0.9932236205227493,0.9932236205227493,0.9932236205227493,0.9932236205227493,0.9932236205227493,0.9932236205227493,0.9922555663117134,0.9912875121006777,0.9903194578896418,0.9903194578896418,0.9903194578896418,0.9903194578896418,0.9903194578896418,0.9903194578896418,0.989351403678606,0.989351403678606,0.989351403678606,0.9883833494675702,0.9874152952565344,0.9874152952565344,0.9864472410454985,0.9864472410454985,0.9864472410454985,0.9864472410454985,0.9864472410454985,0.9854791868344628,0.9845111326234269,0.9845111326234269,0.9835430784123911,0.9825750242013552,0.9816069699903195,0.9816069699903195,0.9816069699903195,0.9816069699903195,0.9806389157792836,0.9806389157792836,0.9806389157792836,0.9796708615682478,0.9796708615682478,0.9796708615682478,0.978702807357212,0.978702807357212,0.9777347531461762,0.9767666989351403,0.9767666989351403,0.9757986447241046,0.9757986447241046,0.9748305905130688,0.9748305905130688,0.9748305905130688,0.9738625363020329,0.972894482090997,0.9719264278799613,0.9709583736689255,0.9709583736689255,0.9699903194578896,0.9690222652468539,0.9690222652468539,0.9690222652468539,0.9690222652468539,0.9690222652468539,0.968054211035818,0.9670861568247822,0.9670861568247822,0.9670861568247822,0.9661181026137464,0.9661181026137464,0.9651500484027106,0.9651500484027106,0.9641819941916747,0.9641819941916747,0.9641819941916747,0.9632139399806389,0.9632139399806389,0.9622458857696031,0.9612778315585673,0.9612778315585673,0.9612778315585673,0.9612778315585673,0.9603097773475314,0.9603097773475314,0.9603097773475314,0.9603097773475314,0.9593417231364957,0.9593417231364957,0.9593417231364957,0.9583736689254598,0.9583736689254598,0.957405614714424,0.9564375605033882,0.9564375605033882,0.9554695062923524,0.9554695062923524,0.9554695062923524,0.9554695062923524,0.9554695062923524,0.9545014520813165,0.9545014520813165,0.9535333978702807,0.952565343659245,0.9515972894482091,0.9506292352371732,0.9496611810261375,0.9486931268151017,0.9486931268151017,0.9477250726040658,0.9467570183930301,0.9457889641819942,0.9448209099709584,0.9438528557599225,0.9438528557599225,0.9428848015488868,0.9428848015488868,0.9419167473378509,0.9419167473378509,0.9419167473378509,0.9409486931268151,0.9409486931268151,0.9409486931268151,0.9409486931268151,0.9399806389157793,0.9399806389157793,0.9399806389157793,0.9399806389157793,0.9390125847047435,0.9380445304937076,0.9380445304937076,0.9380445304937076,0.9370764762826719,0.936108422071636,0.9351403678606002,0.9341723136495643,0.9332042594385286,0.9322362052274927,0.9322362052274927,0.9322362052274927,0.9322362052274927,0.9312681510164569,0.9303000968054211,0.9293320425943853,0.9293320425943853,0.9283639883833494,0.9273959341723137,0.9264278799612778,0.925459825750242,0.9244917715392061,0.9235237173281704,0.9225556631171346,0.9215876089060987,0.920619554695063,0.9196515004840271,0.9186834462729913,0.9177153920619555,0.9177153920619555,0.9167473378509197,0.9167473378509197,0.9157792836398838,0.9157792836398838,0.914811229428848,0.914811229428848,0.9138431752178122,0.9128751210067764,0.9119070667957405,0.9109390125847048,0.9109390125847048,0.9099709583736689,0.9090029041626331,0.9090029041626331,0.9080348499515973,0.9080348499515973,0.9080348499515973,0.9070667957405615,0.9060987415295256,0.9060987415295256,0.9051306873184899,0.9051306873184899,0.904162633107454,0.904162633107454,0.9031945788964182,0.9031945788964182,0.9022265246853823,0.9012584704743466,0.9002904162633107,0.8993223620522749,0.8983543078412392,0.8973862536302033,0.8964181994191674,0.8954501452081317,0.8944820909970959,0.89351403678606,0.89351403678606,0.89351403678606,0.8925459825750242,0.8915779283639884,0.8915779283639884,0.8906098741529526,0.8896418199419167,0.888673765730881,0.8877057115198451,0.8867376573088093,0.8857696030977735,0.8857696030977735,0.8848015488867377,0.8838334946757018,0.882865440464666,0.8818973862536302,0.8809293320425944,0.8799612778315585,0.8789932236205228,0.8780251694094869,0.8780251694094869,0.8780251694094869,0.8770571151984511,0.8770571151984511,0.8760890609874153,0.8760890609874153,0.8751210067763795,0.8741529525653436,0.8731848983543078,0.8731848983543078,0.872216844143272,0.8712487899322362,0.8702807357212003,0.8693126815101646,0.8683446272991288,0.8673765730880929,0.8664085188770572,0.8654404646660213,0.8644724104549855,0.8635043562439496,0.8635043562439496,0.8635043562439496,0.8625363020329139,0.861568247821878,0.861568247821878,0.8606001936108422,0.8596321393998064,0.8596321393998064,0.8596321393998064,0.8586640851887706,0.8576960309777347,0.856727976766699,0.856727976766699,0.856727976766699,0.856727976766699,0.856727976766699,0.8557599225556631,0.8557599225556631,0.8557599225556631,0.8547918683446273,0.8538238141335914,0.8528557599225557,0.8518877057115198,0.850919651500484,0.850919651500484,0.8499515972894482,0.8499515972894482,0.8489835430784124,0.8480154888673765,0.8470474346563408,0.846079380445305,0.8451113262342691,0.8441432720232332,0.8441432720232332,0.8441432720232332,0.8441432720232332,0.8431752178121975,0.8422071636011617,0.8412391093901258,0.8402710551790901,0.8402710551790901,0.8393030009680542,0.8383349467570184,0.8373668925459826,0.8373668925459826,0.8363988383349468,0.8354307841239109,0.8344627299128751,0.8334946757018393,0.8334946757018393,0.8325266214908035,0.8315585672797676,0.8315585672797676,0.8305905130687319,0.829622458857696,0.8286544046466602,0.8276863504356244,0.8267182962245886,0.8257502420135527,0.8257502420135527,0.8247821878025169,0.8238141335914811,0.8238141335914811,0.8228460793804453,0.8218780251694094,0.8209099709583737,0.8199419167473379,0.818973862536302,0.8180058083252663,0.8170377541142304,0.8160696999031946,0.8151016456921588,0.8151016456921588,0.814133591481123,0.814133591481123,0.8131655372700871,0.8121974830590513,0.8121974830590513,0.8112294288480155,0.8102613746369797,0.8092933204259438,0.8083252662149081,0.8083252662149081,0.8073572120038722,0.8063891577928364,0.8054211035818006,0.8044530493707648,0.8034849951597289,0.8025169409486931,0.8015488867376573,0.8005808325266215,0.7996127783155856,0.7986447241045499,0.797676669893514,0.7967086156824782,0.7957405614714425,0.7947725072604066,0.7938044530493708,0.7928363988383349,0.7918683446272992,0.7918683446272992,0.7909002904162633,0.7899322362052275,0.7889641819941917,0.7889641819941917,0.7879961277831559,0.78702807357212,0.7860600193610843,0.7850919651500484,0.7850919651500484,0.7841239109390126,0.7831558567279767,0.782187802516941,0.782187802516941,0.7812197483059051,0.7802516940948693,0.7792836398838335,0.7783155856727977,0.7773475314617618,0.7763794772507261,0.7754114230396902,0.7744433688286544,0.7734753146176185,0.7725072604065828,0.7725072604065828,0.771539206195547,0.7705711519845111,0.7696030977734754,0.7686350435624395,0.7676669893514037,0.7666989351403679,0.7657308809293321,0.7657308809293321,0.7647628267182962,0.7637947725072604,0.7637947725072604,0.7628267182962246,0.7618586640851888,0.7608906098741529,0.7599225556631172,0.7589545014520813,0.7579864472410455,0.7570183930300097,0.7560503388189739,0.755082284607938,0.7541142303969022,0.7531461761858664,0.7521781219748306,0.7512100677637947,0.750242013552759,0.7492739593417231,0.7492739593417231,0.7483059051306873,0.7473378509196515,0.7463697967086157,0.7454017424975798,0.7454017424975798,0.744433688286544,0.7434656340755083,0.7424975798644724,0.7415295256534365,0.7405614714424008,0.739593417231365,0.7386253630203291,0.7376573088092934,0.7366892545982575,0.7357212003872217,0.7347531461761858,0.7337850919651501,0.7328170377541142,0.7318489835430784,0.7308809293320426,0.7299128751210068,0.7289448209099709,0.7279767666989352,0.7279767666989352,0.7270087124878993,0.7260406582768635,0.7260406582768635,0.7250726040658277,0.7241045498547919,0.723136495643756,0.7221684414327202,0.7212003872216844,0.7202323330106486,0.7192642787996127,0.718296224588577,0.7173281703775412,0.7163601161665053,0.7153920619554696,0.7144240077444337,0.7134559535333979,0.712487899322362,0.712487899322362,0.7115198451113263,0.7105517909002904,0.7095837366892546,0.7086156824782188,0.707647628267183,0.7066795740561471,0.7066795740561471,0.7066795740561471,0.7057115198451114,0.7047434656340755,0.7037754114230397,0.7028073572120038,0.7018393030009681,0.7008712487899322,0.6999031945788964,0.6989351403678606,0.6979670861568248,0.6969990319457889,0.6960309777347532,0.6960309777347532,0.6950629235237173,0.6940948693126815,0.6931268151016456,0.6921587608906099,0.691190706679574,0.6902226524685382,0.6892545982575025,0.6882865440464666,0.6873184898354308,0.6873184898354308,0.686350435624395,0.6853823814133592,0.6844143272023233,0.6834462729912875,0.6824782187802517,0.6815101645692159,0.68054211035818,0.6795740561471443,0.6786060019361084,0.6776379477250726,0.6776379477250726,0.6766698935140368,0.675701839303001,0.6747337850919651,0.6737657308809293,0.6727976766698935,0.6718296224588577,0.6708615682478218,0.6698935140367861,0.6698935140367861,0.6698935140367861,0.6689254598257502,0.6679574056147144,0.6669893514036787,0.6660212971926428,0.665053242981607,0.6640851887705711,0.6631171345595354,0.6621490803484995,0.6611810261374637,0.6602129719264279,0.6592449177153921,0.6582768635043562,0.6573088092933205,0.6563407550822846,0.6553727008712488,0.6544046466602129,0.6534365924491772,0.6524685382381413,0.6515004840271055,0.6505324298160697,0.6495643756050339,0.648596321393998,0.6476282671829623,0.6466602129719264,0.6456921587608906,0.6447241045498547,0.643756050338819,0.6427879961277831,0.6418199419167473,0.6408518877057116,0.6398838334946757,0.6389157792836399,0.6389157792836399,0.6379477250726041,0.6369796708615683,0.6360116166505324,0.6350435624394967,0.6340755082284608,0.6340755082284608,0.6340755082284608,0.633107454017425,0.6321393998063891,0.6311713455953534,0.6302032913843175,0.6292352371732817,0.6282671829622459,0.6272991287512101,0.6263310745401742,0.6253630203291385,0.6243949661181026,0.6234269119070668,0.6224588576960309,0.6214908034849952,0.6205227492739593,0.6195546950629235,0.6185866408518877,0.6176185866408519,0.616650532429816,0.616650532429816,0.6156824782187803,0.6147144240077445,0.6137463697967086,0.6127783155856728,0.611810261374637,0.611810261374637,0.6108422071636012,0.6098741529525653,0.6089060987415296,0.6079380445304937,0.6069699903194579,0.6060019361084221,0.6050338818973863,0.6050338818973863,0.6040658276863504,0.6030977734753146,0.6021297192642788,0.601161665053243,0.6001936108422071,0.6001936108422071,0.5992255566311714,0.5982575024201355,0.5972894482090997,0.5963213939980639,0.5953533397870281,0.5943852855759922,0.5934172313649564,0.5924491771539206,0.5914811229428848,0.590513068731849,0.5895450145208132,0.5885769603097774,0.5876089060987415,0.5866408518877058,0.5856727976766699,0.5847047434656341,0.5847047434656341,0.5837366892545982,0.5827686350435625,0.5818005808325266,0.5808325266214908,0.579864472410455,0.5788964181994192,0.5779283639883833,0.5769603097773476,0.5759922555663117,0.5750242013552759,0.5750242013552759,0.5750242013552759,0.57405614714424,0.5730880929332043,0.5721200387221684,0.5711519845111326,0.5701839303000968,0.569215876089061,0.5682478218780251,0.5672797676669894,0.5663117134559535,0.5653436592449177,0.5643756050338818,0.5634075508228461,0.5624394966118103,0.5614714424007744,0.5605033881897387,0.5595353339787028,0.558567279767667,0.5575992255566312,0.5566311713455954,0.5556631171345595,0.5546950629235237,0.5537270087124879,0.5527589545014521,0.5517909002904162,0.5508228460793805,0.5498547918683446,0.5488867376573088,0.547918683446273,0.5469506292352372,0.5459825750242013,0.5450145208131656,0.5440464666021297,0.5430784123910939,0.542110358180058,0.542110358180058,0.542110358180058,0.5411423039690223,0.5401742497579864,0.5392061955469506,0.5382381413359149,0.537270087124879,0.5363020329138432,0.5363020329138432,0.5353339787028074,0.5343659244917716,0.5333978702807357,0.5324298160696999,0.5314617618586641,0.5314617618586641,0.5304937076476283,0.5295256534365924,0.5285575992255567,0.5275895450145208,0.526621490803485,0.5256534365924492,0.5256534365924492,0.5246853823814134,0.5237173281703775,0.5227492739593417,0.5217812197483059,0.5208131655372701,0.5198451113262342,0.5188770571151985,0.5179090029041626,0.5169409486931268,0.515972894482091,0.5150048402710552,0.5140367860600193,0.5130687318489835,0.5121006776379478,0.5111326234269119,0.510164569215876,0.5091965150048403,0.5082284607938045,0.5072604065827686,0.5062923523717329,0.505324298160697,0.5043562439496612,0.5033881897386253,0.5024201355275896,0.5014520813165537,0.5004840271055179,0.4995159728944821,0.4985479186834463,0.4975798644724105,0.49661181026137463,0.49564375605033884,0.494675701839303,0.4937076476282672,0.4927395934172314,0.49177153920619554,0.49080348499515974,0.4898354307841239,0.4888673765730881,0.4878993223620523,0.48693126815101645,0.48596321393998065,0.4849951597289448,0.484027105517909,0.48209099709583736,0.48112294288480156,0.4801548886737657,0.4791868344627299,0.4782187802516941,0.47725072604065827,0.4762826718296225,0.4753146176185866,0.4743465634075508,0.47337850919651503,0.4724104549854792,0.4714424007744434,0.47047434656340753,0.46950629235237173,0.46853823814133594,0.4675701839303001,0.4666021297192643,0.46563407550822844,0.46466602129719264,0.46369796708615685,0.462729912875121,0.4617618586640852,0.46079380445304935,0.45982575024201355,0.45885769603097776,0.4578896418199419,0.4569215876089061,0.45595353339787026,0.45498547918683446,0.45401742497579867,0.4530493707647628,0.452081316553727,0.45111326234269117,0.45014520813165537,0.4491771539206196,0.4482090997095837,0.44724104549854793,0.4462729912875121,0.4453049370764763,0.4443368828654405,0.44336882865440463,0.44240077444336884,0.441432720232333,0.4404646660212972,0.4394966118102614,0.43852855759922554,0.43756050338818975,0.4365924491771539,0.4356243949661181,0.4346563407550823,0.43368828654404645,0.43272023233301066,0.43272023233301066,0.4317521781219748,0.430784123910939,0.4298160696999032,0.42884801548886736,0.42787996127783157,0.4269119070667957,0.4269119070667957,0.4259438528557599,0.4249757986447241,0.42400774443368827,0.4230396902226525,0.4220716360116166,0.42110358180058083,0.42013552758954503,0.4191674733785092,0.4181994191674734,0.41723136495643753,0.41626331074540174,0.41529525653436594,0.4143272023233301,0.4133591481122943,0.41239109390125844,0.41045498547918685,0.409486931268151,0.4085188770571152,0.4075508228460794,0.40658276863504356,0.40561471442400776,0.4046466602129719,0.4036786060019361,0.4027105517909003,0.40174249757986447,0.40077444336882867,0.3998063891577928,0.3978702807357212,0.3969022265246854,0.3959341723136496,0.39496611810261373,0.39496611810261373,0.39399806389157793,0.39399806389157793,0.39303000968054214,0.3920619554695063,0.3910939012584705,0.39012584704743464,0.38915779283639884,0.38818973862536305,0.38818973862536305,0.3872216844143272,0.3862536302032914,0.38528557599225555,0.38431752178121975,0.38334946757018395,0.3823814133591481,0.3814133591481123,0.38044530493707646,0.37947725072604066,0.37850919651500486,0.377541142303969,0.3765730880929332,0.37560503388189737,0.37463697967086157,0.3736689254598258,0.3727008712487899,0.3717328170377541,0.3707647628267183,0.3697967086156825,0.3688286544046467,0.36786060019361083,0.36689254598257504,0.3659244917715392,0.3649564375605034,0.3639883833494676,0.36302032913843174,0.36205227492739595,0.3610842207163601,0.3601161665053243,0.3591481122942885,0.35818005808325265,0.35721200387221685,0.356243949661181,0.3552758954501452,0.3543078412391094,0.35333978702807356,0.35237173281703776,0.3514036786060019,0.3504356243949661,0.3494675701839303,0.34849951597289447,0.3475314617618587,0.3465634075508228,0.345595353339787,0.34462729912875123,0.3436592449177154,0.3426911907066796,0.34172313649564373,0.34075508228460794,0.33978702807357214,0.3388189738625363,0.3378509196515005,0.33688286544046464,0.33591481122942884,0.33494675701839305,0.3339787028073572,0.3330106485963214,0.33204259438528555,0.33107454017424975,0.33010648596321396,0.3291384317521781,0.3281703775411423,0.32720232333010646,0.32623426911907066,0.32526621490803487,0.324298160696999,0.3233301064859632,0.32236205227492737,0.3213939980638916,0.3204259438528558,0.3194578896418199,0.31848983543078413,0.31752178121974833,0.3165537270087125,0.3155856727976767,0.31461761858664083,0.31461761858664083,0.31364956437560504,0.31268151016456924,0.3117134559535334,0.3117134559535334,0.3107454017424976,0.30977734753146174,0.30880929332042595,0.30784123910939015,0.3068731848983543,0.3059051306873185,0.30493707647628265,0.30396902226524686,0.30300096805421106,0.3020329138431752,0.3010648596321394,0.30009680542110356,0.29912875121006777,0.29816069699903197,0.2971926427879961,0.2962245885769603,0.2952565343659245,0.2942884801548887,0.2933204259438529,0.29235237173281703,0.29138431752178123,0.2904162633107454,0.2894482090997096,0.2884801548886738,0.28751210067763794,0.28654404646660214,0.2855759922555663,0.2846079380445305,0.2836398838334947,0.28267182962245885,0.28170377541142305,0.2807357212003872,0.2797676669893514,0.2787996127783156,0.27783155856727976,0.27686350435624396,0.2758954501452081,0.2749273959341723,0.2739593417231365,0.27299128751210067,0.27202323330106487,0.271055179090029,0.2700871248789932,0.2691190706679574,0.2681510164569216,0.2671829622458858,0.26621490803484993,0.26524685382381413,0.26427879961277834,0.2633107454017425,0.2623426911907067,0.26137463697967084,0.26040658276863504,0.25943852855759925,0.2584704743465634,0.2575024201355276,0.25653436592449175,0.25556631171345595,0.25459825750242016,0.2536302032913843,0.2526621490803485,0.25169409486931266,0.25072604065827686,0.24975798644724104,0.24878993223620524,0.24782187802516942,0.2468538238141336,0.24588576960309777,0.24491771539206195,0.24394966118102615,0.24298160696999033,0.2420135527589545,0.24104549854791868,0.24007744433688286,0.23910939012584706,0.23814133591481124,0.2371732817037754,0.2362052274927396,0.23523717328170377,0.23426911907066797,0.23330106485963215,0.23233301064859632,0.2313649564375605,0.23039690222652467,0.22942884801548888,0.22846079380445306,0.22749273959341723,0.2265246853823814,0.22555663117134558,0.2245885769603098,0.22362052274927396,0.22265246853823814,0.22168441432720232,0.2207163601161665,0.2197483059051307,0.21878025169409487,0.21781219748305905,0.21684414327202323,0.2158760890609874,0.2149080348499516,0.21393998063891578,0.21297192642787996,0.21200387221684414,0.2110358180058083,0.21006776379477252,0.2090997095837367,0.20813165537270087,0.20716360116166505,0.20619554695062922,0.20522749273959343,0.2042594385285576,0.20329138431752178,0.20232333010648595,0.20135527589545016,0.20038722168441434,0.1994191674733785,0.1984511132623427,0.19748305905130686,0.19651500484027107,0.19554695062923524,0.19457889641819942,0.1936108422071636,0.19264278799612777,0.19167473378509198,0.19070667957405615,0.18973862536302033,0.1887705711519845,0.18780251694094868,0.1868344627299129,0.18586640851887706,0.18586640851887706,0.18489835430784124,0.18393030009680542,0.1829622458857696,0.1819941916747338,0.18102613746369797,0.18102613746369797,0.18005808325266215,0.17909002904162633,0.1781219748305905,0.1771539206195547,0.17618586640851888,0.17521781219748306,0.17424975798644723,0.1732817037754114,0.17231364956437561,0.1713455953533398,0.17037754114230397,0.16940948693126814,0.16844143272023232,0.16747337850919652,0.1665053242981607,0.16553727008712488,0.16456921587608905,0.16360116166505323,0.16263310745401743,0.1616650532429816,0.1606969990319458,0.15972894482090996,0.15876089060987417,0.15779283639883834,0.15682478218780252,0.1558567279767667,0.15488867376573087,0.15392061955469508,0.15295256534365925,0.15198451113262343,0.1510164569215876,0.15004840271055178,0.14908034849951599,0.14811229428848016,0.14714424007744434,0.14617618586640851,0.1452081316553727,0.1442400774443369,0.14327202323330107,0.14230396902226525,0.14133591481122942,0.1403678606001936,0.1393998063891578,0.13843175217812198,0.13746369796708616,0.13649564375605033,0.1355275895450145,0.1345595353339787,0.1335914811229429,0.13262342691190707,0.13165537270087124,0.13068731848983542,0.12971926427879962,0.1287512100677638,0.12778315585672798,0.12681510164569215,0.12487899322362052,0.12391093901258471,0.12294288480154889,0.12197483059051308,0.12100677637947725,0.12003872216844143,0.1181026137463698,0.11713455953533398,0.11616650532429816,0.11519845111326234,0.11423039690222653,0.1132623426911907,0.1122942884801549,0.11132623426911907,0.11035818005808325,0.10939012584704744,0.10842207163601161,0.10648596321393998,0.10551790900290416,0.10454985479186835,0.10358180058083252,0.10261374636979671,0.10164569215876089,0.10067763794772508,0.09970958373668926,0.09874152952565343,0.09777347531461762,0.0968054211035818,0.09583736689254599,0.09486931268151017,0.09390125847047434,0.09293320425943853,0.09196515004840271,0.0909970958373669,0.09002904162633107,0.08906098741529525,0.08809293320425944,0.08712487899322362,0.08615682478218781,0.08518877057115198,0.08422071636011616,0.08325266214908035,0.08228460793804453,0.08131655372700872,0.0803484995159729,0.07938044530493708,0.07841239109390126,0.07744433688286544,0.07744433688286544,0.07647628267182963,0.0755082284607938,0.07454017424975799,0.07357212003872217,0.07260406582768635,0.07163601161665054,0.07066795740561471,0.0696999031945789,0.06873184898354308,0.06776379477250725,0.06679574056147145,0.06582768635043562,0.06485963213939981,0.06389157792836399,0.06292352371732816,0.061955469506292354,0.06098741529525654,0.060019361084220714,0.0590513068731849,0.05808325266214908,0.057115198451113264,0.05614714424007745,0.05517909002904162,0.05421103581800581,0.05324298160696999,0.05227492739593417,0.051306873184898356,0.05033881897386254,0.049370764762826716,0.0484027105517909,0.04743465634075508,0.046466602129719266,0.04549854791868345,0.044530493707647625,0.044530493707647625,0.04356243949661181,0.04259438528557599,0.04259438528557599,0.041626331074540175,0.04065827686350436,0.03969022265246854,0.03872216844143272,0.0377541142303969,0.036786060019361085,0.03581800580832527,0.03484995159728945,0.03388189738625363,0.03291384317521781,0.031945788964181994,0.030977734753146177,0.030009680542110357,0.02904162633107454,0.028073572120038724,0.027105517909002903,0.026137463697967087,0.02516940948693127,0.02420135527589545,0.023233301064859633,0.022265246853823813,0.021297192642787996,0.02032913843175218,0.01936108422071636,0.018393030009680542,0.017424975798644726,0.016456921587608905,0.015488867376573089,0.01452081316553727,0.013552758954501452,0.012584704743465635,0.011616650532429816,0.010648596321393998,0.00968054211035818,0.008712487899322363,0.007744433688286544,0.006776379477250726,0.005808325266214908,0.00484027105517909,0.003872216844143272,0.002904162633107454,0.002904162633107454,0.001936108422071636,0.000968054211035818,0],"xaxis":"x","y":[0.7341862117981521,0.7347083926031295,0.7352313167259786,0.7357549857549858,0.7362794012829651,0.7368045649072753,0.7373304782298359,0.7378571428571429,0.7383845604002859,0.7389127324749643,0.7399713467048711,0.7405017921146954,0.7410329985652798,0.741564967695621,0.7420977011494253,0.7426312005751258,0.7431654676258993,0.7437005039596832,0.7442363112391931,0.7447728911319395,0.7453102453102453,0.7458483754512636,0.7463872832369942,0.7469269703543022,0.7474674384949349,0.7480086893555394,0.7485507246376811,0.7490935460478607,0.7496371552975326,0.7501815541031227,0.7507267441860465,0.7518195050946143,0.752367079388201,0.7529154518950437,0.7534646243617797,0.754014598540146,0.7545653761869978,0.7551169590643275,0.7556693489392831,0.7562225475841874,0.7567765567765568,0.7573313782991202,0.7578870139398386,0.7584434654919237,0.7590007347538574,0.7595588235294117,0.7601177336276674,0.7606774668630338,0.7612380250552689,0.7617994100294986,0.7623616236162362,0.7629246676514032,0.7634885439763488,0.7640532544378699,0.764618800888231,0.7651851851851852,0.765752409191994,0.766320474777448,0.7668893838158871,0.7674591381872214,0.7680297397769517,0.7686011904761905,0.7691734921816828,0.7697466467958272,0.7703206562266965,0.7708955223880597,0.7714712471994025,0.7720478325859492,0.7726252804786836,0.7732035928143712,0.7737827715355805,0.7743628185907047,0.7749437359339835,0.7755255255255256,0.7761081893313299,0.7766917293233083,0.7772761474793077,0.7778614457831325,0.7784476262245666,0.7790346907993967,0.779622641509434,0.7802114803625377,0.780801209372638,0.7813918305597579,0.7819833459500378,0.7825757575757576,0.7831690674753601,0.783763277693475,0.7843583902809416,0.7849544072948328,0.785551330798479,0.7861491628614916,0.7867479055597868,0.7873475609756098,0.7879481311975591,0.7885496183206107,0.7891520244461421,0.7897553516819572,0.7903596021423106,0.7909647779479326,0.7915708812260537,0.7921779141104295,0.7927858787413661,0.793394777265745,0.7940046118370484,0.7946153846153846,0.7952270977675134,0.7958397534668721,0.7964533538936006,0.7970679012345679,0.7976833976833977,0.7982998454404946,0.7989172467130704,0.7995356037151703,0.8001549186676995,0.8007751937984496,0.8013964313421257,0.8020186335403726,0.8026418026418026,0.8032659409020217,0.8038910505836576,0.8045171339563862,0.8051441932969603,0.8057722308892356,0.8064012490241999,0.80703125,0.8076622361219703,0.8082942097026604,0.8089271730618638,0.8095611285266457,0.8101960784313725,0.8108320251177394,0.8114689709347996,0.8121069182389937,0.8127458693941778,0.8133858267716535,0.814026792750197,0.8146687697160884,0.8153117600631413,0.815955766192733,0.8166007905138339,0.817246835443038,0.8178939034045922,0.8185419968304279,0.8191911181601903,0.8198412698412698,0.8204924543288324,0.8211446740858506,0.8217979315831344,0.822452229299363,0.8223107569721115,0.8229665071770335,0.8236233040702314,0.8242811501597445,0.8249400479616307,0.8256,0.8269230769230769,0.8275862068965517,0.8282504012841091,0.8289156626506025,0.8295819935691319,0.830249396621078,0.8309178743961353,0.8315874294923449,0.832258064516129,0.8329297820823245,0.8336025848142165,0.8342764753435732,0.8349514563106796,0.8356275303643724,0.8363047001620746,0.8369829683698297,0.8376623376623377,0.8383428107229894,0.8390243902439024,0.8397070789259561,0.8403908794788274,0.8410757946210269,0.8417618270799347,0.8424489795918367,0.8431372549019608,0.8438266557645135,0.8445171849427169,0.8452088452088452,0.8459016393442623,0.8465955701394585,0.8472906403940886,0.847986852917009,0.8486842105263158,0.8493827160493828,0.8500823723228995,0.8507831821929102,0.8514851485148515,0.8521882741535921,0.8528925619834711,0.8535980148883374,0.8534768211920529,0.8541839270919636,0.8548922056384743,0.8556016597510373,0.856312292358804,0.857024106400665,0.8569051580698835,0.8567860116569526,0.8566666666666667,0.8573811509591326,0.8572621035058431,0.8579782790309106,0.8578595317725752,0.8585774058577406,0.8592964824120602,0.8600167644593462,0.860738255033557,0.8614609571788413,0.8621848739495799,0.862910008410429,0.8627946127946128,0.8626790227464195,0.8625632377740303,0.8632911392405064,0.8640202702702703,0.8647506339814032,0.8654822335025381,0.8662150719729044,0.8661016949152542,0.8668363019508057,0.8675721561969439,0.8674596431605778,0.8673469387755102,0.8680851063829788,0.8679727427597955,0.8687127024722933,0.8694539249146758,0.8701964133219471,0.870940170940171,0.8708297690333618,0.8707191780821918,0.87146529562982,0.8713550600343053,0.871244635193133,0.8711340206185567,0.8718830610490111,0.8726333907056799,0.8733850129198967,0.8732758620689656,0.8740293356341674,0.8747841105354058,0.874675885911841,0.8754325259515571,0.8761904761904762,0.8760831889081456,0.8768430182133564,0.8767361111111112,0.8766290182450044,0.8773913043478261,0.8772845953002611,0.8780487804878049,0.8779424585876199,0.8787085514834206,0.879475982532751,0.8793706293706294,0.8792650918635171,0.8791593695271454,0.8790534618755478,0.8798245614035087,0.8797190517998245,0.8796133567662566,0.8803869832893579,0.8811619718309859,0.8819383259911894,0.8827160493827161,0.8826125330979699,0.8825088339222615,0.883289124668435,0.8840707964601769,0.8839681133746679,0.8847517730496454,0.8846495119787046,0.8854351687388987,0.8853333333333333,0.8861209964412812,0.8869100623330365,0.8868092691622104,0.8876003568242641,0.8875,0.8873994638069705,0.8881932021466905,0.8889883616830797,0.8897849462365591,0.8896860986547085,0.8904847396768402,0.8912848158131177,0.8920863309352518,0.891989198919892,0.8927927927927928,0.8935978358881875,0.8935018050541517,0.8943089430894309,0.8942133815551537,0.8941176470588236,0.894927536231884,0.8948322756119673,0.895644283121597,0.896457765667575,0.8972727272727272,0.8980891719745223,0.8979963570127505,0.8988149498632635,0.8987226277372263,0.8986301369863013,0.8985374771480804,0.898444647758463,0.8983516483516484,0.8982584784601283,0.8990825688073395,0.898989898989899,0.8988970588235294,0.8988040478380864,0.8987108655616943,0.8986175115207373,0.8994464944649446,0.8993536472760849,0.9001848428835489,0.9000925069380203,0.9009259259259259,0.901760889712697,0.9016697588126159,0.9025069637883009,0.9033457249070632,0.9041860465116279,0.904096834264432,0.9049394221808015,0.9057835820895522,0.9066293183940243,0.9065420560747663,0.9064546304957904,0.9073033707865169,0.9081537019681349,0.9080675422138836,0.907981220657277,0.9078947368421053,0.9078080903104422,0.9077212806026366,0.9076343072573044,0.9084905660377358,0.9093484419263456,0.9102079395085066,0.9101229895931883,0.9100378787878788,0.909952606635071,0.9108159392789373,0.9107312440645774,0.9106463878326996,0.9105613701236918,0.9104761904761904,0.9103908484270734,0.9103053435114504,0.9102196752626552,0.9101338432122371,0.9100478468899521,0.9099616858237548,0.909875359539789,0.9097888675623801,0.9106628242074928,0.9105769230769231,0.9114533205004812,0.9113680154142582,0.9122468659594986,0.9121621621621622,0.9130434782608695,0.9129593810444874,0.9128751210067764,0.9127906976744186,0.9127061105722599,0.9135922330097087,0.9135082604470359,0.9134241245136187,0.9143135345666992,0.9142300194931774,0.9151219512195122,0.916015625,0.9159335288367546,0.9158512720156555,0.9167482859941234,0.9166666666666666,0.9175662414131501,0.9174852652259332,0.9183874139626352,0.9183070866141733,0.9192118226600985,0.9191321499013807,0.9190523198420533,0.9189723320158103,0.9188921859545005,0.9188118811881189,0.9187314172447968,0.9186507936507936,0.9185700099304865,0.9184890656063618,0.918407960199005,0.9193227091633466,0.9202392821535393,0.9201596806387226,0.9200799200799201,0.921,0.9209209209209209,0.9208416833667334,0.9207622868605817,0.9206827309236948,0.9206030150753769,0.920523138832998,0.9214501510574018,0.9213709677419355,0.9212916246215943,0.9212121212121213,0.9211324570273003,0.9210526315789473,0.9209726443768997,0.920892494929006,0.9208121827411168,0.9217479674796748,0.9226856561546287,0.9226069246435845,0.9235474006116208,0.923469387755102,0.9244126659856997,0.9243353783231084,0.9242579324462641,0.9241803278688525,0.9251282051282051,0.9250513347022588,0.9249743062692704,0.9248971193415638,0.9248197734294542,0.9247422680412372,0.9246646026831785,0.9245867768595041,0.9245087900723888,0.9244306418219461,0.9243523316062177,0.9253112033195021,0.9262720664589823,0.9261954261954262,0.9261186264308012,0.9270833333333334,0.927007299270073,0.9269311064718163,0.9278996865203761,0.9288702928870293,0.9287958115183246,0.9287211740041929,0.9286463798530955,0.9296218487394958,0.9305993690851735,0.9315789473684211,0.9325605900948367,0.9324894514767933,0.9334741288278775,0.9344608879492601,0.9343915343915344,0.934322033898305,0.9342523860021209,0.9341825902335457,0.9341126461211477,0.9351063829787234,0.9350372736954207,0.9360341151385928,0.935965848452508,0.9358974358974359,0.9358288770053476,0.9357601713062098,0.9356913183279743,0.9356223175965666,0.9366272824919442,0.9376344086021505,0.9386437029063509,0.9385775862068966,0.9385113268608414,0.9384449244060475,0.9383783783783783,0.9393939393939394,0.9393282773564464,0.9392624728850325,0.9391965255157437,0.9402173913043478,0.940152339499456,0.9400871459694989,0.9400218102508179,0.9399563318777293,0.940983606557377,0.9409190371991247,0.940854326396495,0.9418859649122807,0.9418221734357849,0.9417582417582417,0.9416941694169417,0.9416299559471366,0.9415656008820287,0.9415011037527594,0.9425414364640884,0.9424778761061947,0.9424141749723145,0.9434589800443459,0.9433962264150944,0.9433333333333334,0.9432703003337041,0.9432071269487751,0.9431438127090301,0.9430803571428571,0.9430167597765363,0.9429530201342282,0.9428891377379619,0.9439461883408071,0.9438832772166106,0.9449438202247191,0.9448818897637795,0.9448198198198198,0.9458850056369785,0.945823927765237,0.9457627118644067,0.9457013574660633,0.9456398640996603,0.9467120181405896,0.9466515323496028,0.946590909090909,0.9465301478953356,0.9464692482915718,0.9464082098061574,0.9463470319634704,0.9462857142857143,0.9462242562929062,0.9461626575028637,0.9461009174311926,0.9460390355912744,0.9459770114942528,0.9459148446490219,0.945852534562212,0.9457900807381776,0.9457274826789839,0.945664739884393,0.9467592592592593,0.9466975666280417,0.9466357308584686,0.9465737514518002,0.9476744186046512,0.9476135040745053,0.9475524475524476,0.9474912485414235,0.947429906542056,0.9485380116959065,0.9484777517564403,0.9484173505275498,0.9483568075117371,0.9494712103407755,0.9494117647058824,0.9493521790341578,0.9492924528301887,0.9492325855962219,0.9491725768321513,0.9491124260355029,0.9490521327014217,0.9489916963226572,0.9489311163895487,0.9488703923900119,0.95,0.9499404052443385,0.9498806682577565,0.9498207885304659,0.9497607655502392,0.9497005988023952,0.9496402877697842,0.9495798319327731,0.9507211538461539,0.950661853188929,0.9506024096385542,0.9517490952955368,0.9516908212560387,0.9516324062877872,0.9515738498789347,0.9515151515151515,0.9514563106796117,0.951397326852977,0.9513381995133819,0.951278928136419,0.9512195121951219,0.9511599511599511,0.9511002444987775,0.9510403916768666,0.9509803921568627,0.950920245398773,0.9508599508599509,0.9520295202952029,0.9519704433497537,0.9519112207151664,0.9518518518518518,0.9517923362175525,0.9529702970297029,0.952912019826518,0.9528535980148883,0.9527950310559006,0.9527363184079602,0.9526774595267746,0.9526184538653366,0.9525593008739076,0.9525,0.9524405506883604,0.9523809523809523,0.9523212045169385,0.9522613065326633,0.9522012578616352,0.9521410579345088,0.9520807061790668,0.952020202020202,0.9519595448798989,0.9518987341772152,0.9531051964512041,0.9530456852791879,0.9529860228716646,0.9541984732824428,0.954140127388535,0.9540816326530612,0.9540229885057471,0.9539641943734015,0.9539052496798975,0.9538461538461539,0.9537869062901155,0.9537275064267352,0.9536679536679536,0.9536082474226805,0.9535483870967741,0.9534883720930233,0.9534282018111255,0.9533678756476683,0.9546044098573282,0.9545454545454546,0.9544863459037711,0.9544270833333334,0.954367666232073,0.9543080939947781,0.954248366013072,0.9554973821989529,0.9567496723460026,0.9566929133858267,0.9566360052562418,0.9565789473684211,0.9565217391304348,0.9564643799472295,0.9564068692206077,0.9563492063492064,0.9562913907284768,0.9562334217506632,0.9561752988047809,0.9561170212765957,0.9573901464713716,0.9573333333333334,0.9572763684913218,0.9572192513368984,0.9571619812583668,0.9571045576407506,0.9570469798657718,0.956989247311828,0.9569313593539704,0.9568733153638814,0.9581646423751687,0.9581081081081081,0.9580514208389715,0.9579945799457995,0.9579375848032564,0.9578804347826086,0.9578231292517007,0.9577656675749319,0.9577080491132333,0.9576502732240437,0.957592339261286,0.958904109589041,0.9588477366255144,0.9587912087912088,0.9587345254470426,0.9586776859504132,0.9586206896551724,0.9585635359116023,0.9585062240663901,0.9584487534626038,0.9597780859916782,0.9611111111111111,0.96105702364395,0.9610027855153204,0.9609483960948396,0.9608938547486033,0.9608391608391609,0.9607843137254902,0.9607293127629734,0.9606741573033708,0.960618846694796,0.9605633802816902,0.9605077574047954,0.96045197740113,0.9603960396039604,0.9603399433427762,0.9602836879432625,0.9602272727272727,0.9601706970128022,0.9601139601139601,0.9600570613409415,0.96,0.9599427753934192,0.9598853868194842,0.9598278335724534,0.9597701149425287,0.9597122302158273,0.9596541786743515,0.9595959595959596,0.9595375722543352,0.959479015918958,0.9594202898550724,0.9593613933236574,0.9593023255813954,0.9606986899563319,0.9606413994169096,0.9605839416058394,0.9605263157894737,0.9604685212298683,0.9604105571847508,0.9618208516886931,0.9632352941176471,0.9631811487481591,0.9631268436578171,0.9630723781388478,0.9630177514792899,0.9629629629629629,0.9629080118694362,0.962852897473997,0.9627976190476191,0.96274217585693,0.9626865671641791,0.9626307922272048,0.9625748502994012,0.9625187406296851,0.9624624624624625,0.9624060150375939,0.9623493975903614,0.9622926093514329,0.9622356495468278,0.9636913767019667,0.9636363636363636,0.9635811836115327,0.9635258358662614,0.9634703196347032,0.9634146341463414,0.9648854961832061,0.9648318042813455,0.9647779479326187,0.9647239263803681,0.9646697388632872,0.9646153846153847,0.9645608628659477,0.9645061728395061,0.9659969088098919,0.9659442724458205,0.9658914728682171,0.9658385093167702,0.9657853810264385,0.9657320872274143,0.9672386895475819,0.9671875,0.9671361502347418,0.9670846394984326,0.967032967032967,0.9669811320754716,0.9669291338582677,0.9668769716088328,0.966824644549763,0.9667721518987342,0.9667194928684627,0.9666666666666667,0.9666136724960255,0.9665605095541401,0.9665071770334929,0.9664536741214057,0.9664,0.967948717948718,0.9678972712680578,0.9678456591639871,0.9677938808373591,0.967741935483871,0.9676898222940227,0.9676375404530745,0.9675850891410048,0.9675324675324676,0.967479674796748,0.9674267100977199,0.9690048939641109,0.9705882352941176,0.9705400981996727,0.9704918032786886,0.9704433497536946,0.9703947368421053,0.9703459637561779,0.9702970297029703,0.9702479338842975,0.9701986754966887,0.9701492537313433,0.9700996677740864,0.9700499168053245,0.97,0.9699499165275459,0.9698996655518395,0.9698492462311558,0.9697986577181208,0.9697478991596639,0.9696969696969697,0.96964586846543,0.9695945945945946,0.9695431472081218,0.9694915254237289,0.969439728353141,0.9693877551020408,0.969335604770017,0.9692832764505119,0.9692307692307692,0.9691780821917808,0.9691252144082333,0.9690721649484536,0.9690189328743546,0.9689655172413794,0.9689119170984456,0.9688581314878892,0.9705372616984402,0.9722222222222222,0.9721739130434782,0.9721254355400697,0.9720767888307156,0.972027972027972,0.9719789842381786,0.9719298245614035,0.9736379613356766,0.9735915492957746,0.9735449735449735,0.9734982332155477,0.9734513274336283,0.973404255319149,0.9751332149200711,0.9750889679715302,0.9750445632798574,0.975,0.9749552772808586,0.974910394265233,0.9748653500897666,0.9766187050359713,0.9765765765765766,0.9765342960288809,0.976491862567812,0.9764492753623188,0.9764065335753176,0.9763636363636363,0.97632058287796,0.9762773722627737,0.9762340036563071,0.9761904761904762,0.9761467889908257,0.9761029411764706,0.9760589318600368,0.9760147601476015,0.9759704251386322,0.975925925925926,0.9758812615955473,0.9758364312267658,0.9757914338919925,0.9757462686567164,0.9757009345794393,0.9756554307116105,0.975609756097561,0.9755639097744361,0.975517890772128,0.9754716981132076,0.9754253308128544,0.9753787878787878,0.9753320683111955,0.9752851711026616,0.9752380952380952,0.9751908396946565,0.9751434034416826,0.975095785440613,0.9750479846449136,0.975,0.9749518304431599,0.974903474903475,0.9748549323017408,0.9748062015503876,0.974757281553398,0.9747081712062257,0.9746588693957114,0.974559686888454,0.9745098039215686,0.9744597249508841,0.9744094488188977,0.9743589743589743,0.974308300395257,0.9742574257425742,0.9742063492063492,0.974155069582505,0.9741035856573705,0.9740518962075848,0.974,0.9739478957915831,0.9738955823293173,0.9738430583501007,0.9737903225806451,0.9737373737373738,0.9736842105263158,0.973630831643002,0.9735772357723578,0.9735234215885947,0.9734693877551021,0.9734151329243353,0.9733606557377049,0.973305954825462,0.9732510288065843,0.9731958762886598,0.9731404958677686,0.9730848861283644,0.9730290456431535,0.972972972972973,0.9729166666666667,0.9728601252609603,0.9728033472803347,0.9727463312368972,0.9726890756302521,0.9726315789473684,0.9725738396624473,0.9725158562367865,0.972457627118644,0.9723991507430998,0.9723404255319149,0.9722814498933902,0.9722222222222222,0.9721627408993576,0.9721030042918455,0.9720430107526882,0.9719827586206896,0.9719222462203023,0.9718614718614719,0.9718004338394793,0.9717391304347827,0.9738562091503268,0.9737991266375546,0.973741794310722,0.9736842105263158,0.9736263736263736,0.973568281938326,0.9735099337748344,0.9756637168141593,0.975609756097561,0.9755555555555555,0.9755011135857461,0.9754464285714286,0.9753914988814317,0.9753363228699552,0.9752808988764045,0.9752252252252253,0.9751693002257337,0.9751131221719457,0.9750566893424036,0.975,0.9749430523917996,0.9748858447488584,0.9748283752860412,0.9747126436781609,0.9746543778801844,0.9745958429561201,0.9745370370370371,0.974477958236659,0.9744186046511628,0.9743589743589743,0.9742990654205608,0.9742388758782201,0.9741784037558685,0.9741176470588235,0.9740566037735849,0.9739336492890995,0.9738717339667459,0.9738095238095238,0.9737470167064439,0.9760765550239234,0.9760191846522782,0.9783653846153846,0.9783132530120482,0.9782608695652174,0.9782082324455206,0.9781553398058253,0.9781021897810219,0.9780487804878049,0.980440097799511,0.9803921568627451,0.9803439803439803,0.9802955665024631,0.980246913580247,0.9801980198019802,0.9801488833746899,0.9800995024875622,0.9800498753117207,0.98,0.9799498746867168,0.9798994974874372,0.9798488664987406,0.9797979797979798,0.979746835443038,0.9796954314720813,0.9796437659033079,0.9795918367346939,0.979539641943734,0.9794871794871794,0.9794344473007712,0.979381443298969,0.979328165374677,0.9792746113989638,0.9792207792207792,0.9791666666666666,0.97911227154047,0.9790575916230366,0.979002624671916,0.9789473684210527,0.978891820580475,0.9788359788359788,0.9787798408488063,0.9787234042553191,0.9786666666666667,0.9786096256684492,0.9785522788203753,0.978494623655914,0.9784366576819407,0.9783783783783784,0.978319783197832,0.9782608695652174,0.9782016348773842,0.9781420765027322,0.9780821917808219,0.978021978021978,0.977961432506887,0.9779005524861878,0.9778393351800554,0.9777777777777777,0.9777158774373259,0.9776536312849162,0.9775910364145658,0.9775280898876404,0.9774647887323944,0.9774011299435028,0.9773371104815864,0.9772727272727273,0.9772079772079773,0.9771428571428571,0.9770773638968482,0.9770114942528736,0.9769452449567724,0.976878612716763,0.9768115942028985,0.9767441860465116,0.9766763848396501,0.9766081871345029,0.9765395894428153,0.9764705882352941,0.976401179941003,0.9763313609467456,0.9762611275964391,0.9761904761904762,0.9761194029850746,0.9760479041916168,0.975975975975976,0.9789156626506024,0.9788519637462235,0.9787878787878788,0.9787234042553191,0.9817073170731707,0.981651376146789,0.9815950920245399,0.9815384615384616,0.9814814814814815,0.9814241486068112,0.9813664596273292,0.9813084112149533,0.98125,0.9811912225705329,0.9811320754716981,0.9810725552050473,0.9810126582278481,0.9809523809523809,0.9808917197452229,0.9808306709265175,0.9807692307692307,0.9807073954983923,0.9806451612903225,0.9805825242718447,0.9805194805194806,0.9804560260586319,0.9803921568627451,0.980327868852459,0.9802631578947368,0.9801980198019802,0.9801324503311258,0.9800664451827242,0.98,0.979933110367893,0.9798657718120806,0.9797979797979798,0.9797297297297297,0.9796610169491525,0.9795918367346939,0.9795221843003413,0.9794520547945206,0.979381443298969,0.9793103448275862,0.9792387543252595,0.9791666666666666,0.9790940766550522,0.9790209790209791,0.9789473684210527,0.9788732394366197,0.9787985865724381,0.9787234042553191,0.9786476868327402,0.9785714285714285,0.978494623655914,0.9784172661870504,0.9783393501805054,0.9782608695652174,0.9781818181818182,0.9781021897810219,0.978021978021978,0.9779411764705882,0.977859778597786,0.9777777777777777,0.9776951672862454,0.9776119402985075,0.9775280898876404,0.9774436090225563,0.9773584905660377,0.9772727272727273,0.9771863117870723,0.9770992366412213,0.9770114942528736,0.9769230769230769,0.9768339768339769,0.9767441860465116,0.9766536964980544,0.9765625,0.9764705882352941,0.9763779527559056,0.9762845849802372,0.9761904761904762,0.9760956175298805,0.976,0.9759036144578314,0.9758064516129032,0.9757085020242915,0.975609756097561,0.9755102040816327,0.9754098360655737,0.9753086419753086,0.9752066115702479,0.975103734439834,0.975,0.9748953974895398,0.9747899159663865,0.9746835443037974,0.9745762711864406,0.9744680851063829,0.9743589743589743,0.9742489270386266,0.9741379310344828,0.974025974025974,0.9739130434782609,0.9737991266375546,0.9736842105263158,0.973568281938326,0.9734513274336283,0.9733333333333334,0.9732142857142857,0.9730941704035875,0.972972972972973,0.9728506787330317,0.9727272727272728,0.9726027397260274,0.9724770642201835,0.9723502304147466,0.9722222222222222,0.9720930232558139,0.9719626168224299,0.971830985915493,0.9716981132075472,0.9715639810426541,0.9714285714285714,0.9712918660287081,0.9711538461538461,0.9710144927536232,0.970873786407767,0.9707317073170731,0.9705882352941176,0.9704433497536946,0.9702970297029703,0.9701492537313433,0.97,0.9698492462311558,0.9696969696969697,0.9746192893401016,0.9744897959183674,0.9743589743589743,0.9742268041237113,0.9740932642487047,0.9739583333333334,0.9790575916230366,0.9789473684210527,0.9788359788359788,0.9787234042553191,0.9786096256684492,0.978494623655914,0.9783783783783784,0.9782608695652174,0.9781420765027322,0.978021978021978,0.9779005524861878,0.9777777777777777,0.9776536312849162,0.9775280898876404,0.9774011299435028,0.9772727272727273,0.9771428571428571,0.9770114942528736,0.976878612716763,0.9767441860465116,0.9766081871345029,0.9764705882352941,0.9763313609467456,0.9761904761904762,0.9760479041916168,0.9759036144578314,0.9757575757575757,0.975609756097561,0.9754601226993865,0.9753086419753086,0.9751552795031055,0.975,0.9748427672955975,0.9746835443037974,0.9745222929936306,0.9743589743589743,0.9741935483870968,0.974025974025974,0.9738562091503268,0.9736842105263158,0.9735099337748344,0.9733333333333334,0.9731543624161074,0.972972972972973,0.9727891156462585,0.9726027397260274,0.9724137931034482,0.9722222222222222,0.972027972027972,0.971830985915493,0.9716312056737588,0.9714285714285714,0.9712230215827338,0.9710144927536232,0.9708029197080292,0.9705882352941176,0.9703703703703703,0.9699248120300752,0.9696969696969697,0.9694656488549618,0.9692307692307692,0.9689922480620154,0.96875,0.9682539682539683,0.968,0.967741935483871,0.967479674796748,0.9672131147540983,0.9669421487603306,0.9666666666666667,0.9663865546218487,0.9661016949152542,0.9658119658119658,0.9655172413793104,0.9649122807017544,0.9646017699115044,0.9642857142857143,0.963963963963964,0.9636363636363636,0.963302752293578,0.9629629629629629,0.9626168224299065,0.9622641509433962,0.9619047619047619,0.9615384615384616,0.9611650485436893,0.9607843137254902,0.9603960396039604,0.96,0.9595959595959596,0.9591836734693877,0.9587628865979382,0.9583333333333334,0.9578947368421052,0.9574468085106383,0.956989247311828,0.9565217391304348,0.9560439560439561,0.9555555555555556,0.9550561797752809,0.9545454545454546,0.9540229885057471,0.9534883720930233,0.9529411764705882,0.9523809523809523,0.963855421686747,0.9634146341463414,0.9629629629629629,0.9625,0.9620253164556962,0.9615384615384616,0.961038961038961,0.9605263157894737,0.96,0.9594594594594594,0.958904109589041,0.9583333333333334,0.9577464788732394,0.9571428571428572,0.9565217391304348,0.9558823529411765,0.9552238805970149,0.9545454545454546,0.9538461538461539,0.953125,0.9523809523809523,0.9516129032258065,0.9508196721311475,0.95,0.9491525423728814,0.9482758620689655,0.9473684210526315,0.9464285714285714,0.9454545454545454,0.9444444444444444,0.9433962264150944,0.9423076923076923,0.9411764705882353,0.94,0.9387755102040817,0.9583333333333334,0.9574468085106383,0.9565217391304348,0.9777777777777777,0.9772727272727273,0.9767441860465116,0.9761904761904762,0.975609756097561,0.975,0.9743589743589743,0.9736842105263158,0.972972972972973,0.9722222222222222,0.9714285714285714,0.9705882352941176,0.9696969696969697,0.96875,0.967741935483871,0.9666666666666667,0.9655172413793104,0.9642857142857143,0.9629629629629629,0.9615384615384616,0.96,0.9583333333333334,0.9565217391304348,0.9545454545454546,0.9523809523809523,0.95,0.9473684210526315,0.9444444444444444,0.9411764705882353,0.9375,0.9333333333333333,0.9285714285714286,0.9230769230769231,0.9166666666666666,0.9090909090909091,0.9,0.8888888888888888,0.875,0.8571428571428571,0.8333333333333334,0.8,0.75,1,1,1,1],"yaxis":"y"}],"layout":{"height":600,"legend":{"tracegroupgap":0},"shapes":[{"line":{"dash":"dash"},"type":"line","x0":0,"x1":1,"y0":1,"y1":0}],"showlegend":false,"template":{"data":{"bar":[{"error_x":{"color":"#2a3f5f"},"error_y":{"color":"#2a3f5f"},"marker":{"line":{"color":"white","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"barpolar":[{"marker":{"line":{"color":"white","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"carpet":[{"aaxis":{"endlinecolor":"#2a3f5f","gridcolor":"#C8D4E3","linecolor":"#C8D4E3","minorgridcolor":"#C8D4E3","startlinecolor":"#2a3f5f"},"baxis":{"endlinecolor":"#2a3f5f","gridcolor":"#C8D4E3","linecolor":"#C8D4E3","minorgridcolor":"#C8D4E3","startlinecolor":"#2a3f5f"},"type":"carpet"}],"choropleth":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"choropleth"}],"contour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"contour"}],"contourcarpet":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"contourcarpet"}],"heatmap":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmap"}],"heatmapgl":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmapgl"}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"histogram2d":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2d"}],"histogram2dcontour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2dcontour"}],"mesh3d":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"mesh3d"}],"parcoords":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"parcoords"}],"pie":[{"automargin":true,"type":"pie"}],"scatter":[{"fillpattern":{"fillmode":"overlay","size":10,"solidity":0.2},"type":"scatter"}],"scatter3d":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter3d"}],"scattercarpet":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattercarpet"}],"scattergeo":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergeo"}],"scattergl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergl"}],"scattermapbox":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattermapbox"}],"scatterpolar":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolar"}],"scatterpolargl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolargl"}],"scatterternary":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterternary"}],"surface":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"surface"}],"table":[{"cells":{"fill":{"color":"#EBF0F8"},"line":{"color":"white"}},"header":{"fill":{"color":"#C8D4E3"},"line":{"color":"white"}},"type":"table"}]},"layout":{"annotationdefaults":{"arrowcolor":"#2a3f5f","arrowhead":0,"arrowwidth":1},"autotypenumbers":"strict","coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]],"sequential":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"sequentialminus":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]]},"colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#2a3f5f"},"geo":{"bgcolor":"white","lakecolor":"white","landcolor":"white","showlakes":true,"showland":true,"subunitcolor":"#C8D4E3"},"hoverlabel":{"align":"left"},"hovermode":"closest","mapbox":{"style":"light"},"paper_bgcolor":"white","plot_bgcolor":"white","polar":{"angularaxis":{"gridcolor":"#EBF0F8","linecolor":"#EBF0F8","ticks":""},"bgcolor":"white","radialaxis":{"gridcolor":"#EBF0F8","linecolor":"#EBF0F8","ticks":""}},"scene":{"xaxis":{"backgroundcolor":"white","gridcolor":"#DFE8F3","gridwidth":2,"linecolor":"#EBF0F8","showbackground":true,"ticks":"","zerolinecolor":"#EBF0F8"},"yaxis":{"backgroundcolor":"white","gridcolor":"#DFE8F3","gridwidth":2,"linecolor":"#EBF0F8","showbackground":true,"ticks":"","zerolinecolor":"#EBF0F8"},"zaxis":{"backgroundcolor":"white","gridcolor":"#DFE8F3","gridwidth":2,"linecolor":"#EBF0F8","showbackground":true,"ticks":"","zerolinecolor":"#EBF0F8"}},"shapedefaults":{"line":{"color":"#2a3f5f"}},"ternary":{"aaxis":{"gridcolor":"#DFE8F3","linecolor":"#A2B1C6","ticks":""},"baxis":{"gridcolor":"#DFE8F3","linecolor":"#A2B1C6","ticks":""},"bgcolor":"white","caxis":{"gridcolor":"#DFE8F3","linecolor":"#A2B1C6","ticks":""}},"title":{"x":0.05},"xaxis":{"automargin":true,"gridcolor":"#EBF0F8","linecolor":"#EBF0F8","ticks":"","title":{"standoff":15},"zerolinecolor":"#EBF0F8","zerolinewidth":2},"yaxis":{"automargin":true,"gridcolor":"#EBF0F8","linecolor":"#EBF0F8","ticks":"","title":{"standoff":15},"zerolinecolor":"#EBF0F8","zerolinewidth":2}}},"title":{"text":"Precision-Recall Curve (AUC=0.9200)"},"width":800,"xaxis":{"anchor":"y","constrain":"domain","domain":[0,1],"title":{"text":"Recall"}},"yaxis":{"anchor":"x","domain":[0,1],"scaleanchor":"x","scaleratio":1,"title":{"text":"Precision"}}}},"text/html":["<div>                            <div id=\"ba7dfd8c-dddf-4e78-b45a-122a7a763c4c\" class=\"plotly-graph-div\" style=\"height:600px; width:800px;\"></div>            <script type=\"text/javascript\">                require([\"plotly\"], function(Plotly) {                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"ba7dfd8c-dddf-4e78-b45a-122a7a763c4c\")) {                    Plotly.newPlot(                        \"ba7dfd8c-dddf-4e78-b45a-122a7a763c4c\",                        [{\"fillpattern\":{\"shape\":\"\"},\"hovertemplate\":\"Recall=%{x}\\u003cbr\\u003ePrecision=%{y}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"\",\"line\":{\"color\":\"#636efa\"},\"marker\":{\"symbol\":\"circle\"},\"mode\":\"lines\",\"name\":\"\",\"orientation\":\"v\",\"showlegend\":false,\"stackgroup\":\"1\",\"x\":[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,0.9990319457889641,0.9990319457889641,0.9990319457889641,0.9990319457889641,0.9990319457889641,0.9990319457889641,0.9990319457889641,0.9990319457889641,0.9990319457889641,0.9990319457889641,0.9990319457889641,0.9990319457889641,0.9990319457889641,0.9990319457889641,0.9990319457889641,0.9990319457889641,0.9990319457889641,0.9990319457889641,0.9990319457889641,0.9990319457889641,0.9990319457889641,0.9990319457889641,0.9990319457889641,0.9990319457889641,0.9990319457889641,0.9990319457889641,0.9990319457889641,0.9990319457889641,0.9990319457889641,0.9990319457889641,0.9990319457889641,0.9990319457889641,0.9990319457889641,0.9990319457889641,0.9990319457889641,0.9990319457889641,0.9990319457889641,0.9990319457889641,0.9990319457889641,0.9990319457889641,0.9990319457889641,0.9990319457889641,0.9990319457889641,0.9990319457889641,0.9990319457889641,0.9990319457889641,0.9980638915779284,0.9980638915779284,0.9980638915779284,0.9980638915779284,0.9980638915779284,0.9980638915779284,0.9970958373668926,0.9961277831558567,0.995159728944821,0.995159728944821,0.9941916747337851,0.9941916747337851,0.9932236205227493,0.9932236205227493,0.9932236205227493,0.9932236205227493,0.9932236205227493,0.9932236205227493,0.9932236205227493,0.9932236205227493,0.9922555663117134,0.9912875121006777,0.9903194578896418,0.9903194578896418,0.9903194578896418,0.9903194578896418,0.9903194578896418,0.9903194578896418,0.989351403678606,0.989351403678606,0.989351403678606,0.9883833494675702,0.9874152952565344,0.9874152952565344,0.9864472410454985,0.9864472410454985,0.9864472410454985,0.9864472410454985,0.9864472410454985,0.9854791868344628,0.9845111326234269,0.9845111326234269,0.9835430784123911,0.9825750242013552,0.9816069699903195,0.9816069699903195,0.9816069699903195,0.9816069699903195,0.9806389157792836,0.9806389157792836,0.9806389157792836,0.9796708615682478,0.9796708615682478,0.9796708615682478,0.978702807357212,0.978702807357212,0.9777347531461762,0.9767666989351403,0.9767666989351403,0.9757986447241046,0.9757986447241046,0.9748305905130688,0.9748305905130688,0.9748305905130688,0.9738625363020329,0.972894482090997,0.9719264278799613,0.9709583736689255,0.9709583736689255,0.9699903194578896,0.9690222652468539,0.9690222652468539,0.9690222652468539,0.9690222652468539,0.9690222652468539,0.968054211035818,0.9670861568247822,0.9670861568247822,0.9670861568247822,0.9661181026137464,0.9661181026137464,0.9651500484027106,0.9651500484027106,0.9641819941916747,0.9641819941916747,0.9641819941916747,0.9632139399806389,0.9632139399806389,0.9622458857696031,0.9612778315585673,0.9612778315585673,0.9612778315585673,0.9612778315585673,0.9603097773475314,0.9603097773475314,0.9603097773475314,0.9603097773475314,0.9593417231364957,0.9593417231364957,0.9593417231364957,0.9583736689254598,0.9583736689254598,0.957405614714424,0.9564375605033882,0.9564375605033882,0.9554695062923524,0.9554695062923524,0.9554695062923524,0.9554695062923524,0.9554695062923524,0.9545014520813165,0.9545014520813165,0.9535333978702807,0.952565343659245,0.9515972894482091,0.9506292352371732,0.9496611810261375,0.9486931268151017,0.9486931268151017,0.9477250726040658,0.9467570183930301,0.9457889641819942,0.9448209099709584,0.9438528557599225,0.9438528557599225,0.9428848015488868,0.9428848015488868,0.9419167473378509,0.9419167473378509,0.9419167473378509,0.9409486931268151,0.9409486931268151,0.9409486931268151,0.9409486931268151,0.9399806389157793,0.9399806389157793,0.9399806389157793,0.9399806389157793,0.9390125847047435,0.9380445304937076,0.9380445304937076,0.9380445304937076,0.9370764762826719,0.936108422071636,0.9351403678606002,0.9341723136495643,0.9332042594385286,0.9322362052274927,0.9322362052274927,0.9322362052274927,0.9322362052274927,0.9312681510164569,0.9303000968054211,0.9293320425943853,0.9293320425943853,0.9283639883833494,0.9273959341723137,0.9264278799612778,0.925459825750242,0.9244917715392061,0.9235237173281704,0.9225556631171346,0.9215876089060987,0.920619554695063,0.9196515004840271,0.9186834462729913,0.9177153920619555,0.9177153920619555,0.9167473378509197,0.9167473378509197,0.9157792836398838,0.9157792836398838,0.914811229428848,0.914811229428848,0.9138431752178122,0.9128751210067764,0.9119070667957405,0.9109390125847048,0.9109390125847048,0.9099709583736689,0.9090029041626331,0.9090029041626331,0.9080348499515973,0.9080348499515973,0.9080348499515973,0.9070667957405615,0.9060987415295256,0.9060987415295256,0.9051306873184899,0.9051306873184899,0.904162633107454,0.904162633107454,0.9031945788964182,0.9031945788964182,0.9022265246853823,0.9012584704743466,0.9002904162633107,0.8993223620522749,0.8983543078412392,0.8973862536302033,0.8964181994191674,0.8954501452081317,0.8944820909970959,0.89351403678606,0.89351403678606,0.89351403678606,0.8925459825750242,0.8915779283639884,0.8915779283639884,0.8906098741529526,0.8896418199419167,0.888673765730881,0.8877057115198451,0.8867376573088093,0.8857696030977735,0.8857696030977735,0.8848015488867377,0.8838334946757018,0.882865440464666,0.8818973862536302,0.8809293320425944,0.8799612778315585,0.8789932236205228,0.8780251694094869,0.8780251694094869,0.8780251694094869,0.8770571151984511,0.8770571151984511,0.8760890609874153,0.8760890609874153,0.8751210067763795,0.8741529525653436,0.8731848983543078,0.8731848983543078,0.872216844143272,0.8712487899322362,0.8702807357212003,0.8693126815101646,0.8683446272991288,0.8673765730880929,0.8664085188770572,0.8654404646660213,0.8644724104549855,0.8635043562439496,0.8635043562439496,0.8635043562439496,0.8625363020329139,0.861568247821878,0.861568247821878,0.8606001936108422,0.8596321393998064,0.8596321393998064,0.8596321393998064,0.8586640851887706,0.8576960309777347,0.856727976766699,0.856727976766699,0.856727976766699,0.856727976766699,0.856727976766699,0.8557599225556631,0.8557599225556631,0.8557599225556631,0.8547918683446273,0.8538238141335914,0.8528557599225557,0.8518877057115198,0.850919651500484,0.850919651500484,0.8499515972894482,0.8499515972894482,0.8489835430784124,0.8480154888673765,0.8470474346563408,0.846079380445305,0.8451113262342691,0.8441432720232332,0.8441432720232332,0.8441432720232332,0.8441432720232332,0.8431752178121975,0.8422071636011617,0.8412391093901258,0.8402710551790901,0.8402710551790901,0.8393030009680542,0.8383349467570184,0.8373668925459826,0.8373668925459826,0.8363988383349468,0.8354307841239109,0.8344627299128751,0.8334946757018393,0.8334946757018393,0.8325266214908035,0.8315585672797676,0.8315585672797676,0.8305905130687319,0.829622458857696,0.8286544046466602,0.8276863504356244,0.8267182962245886,0.8257502420135527,0.8257502420135527,0.8247821878025169,0.8238141335914811,0.8238141335914811,0.8228460793804453,0.8218780251694094,0.8209099709583737,0.8199419167473379,0.818973862536302,0.8180058083252663,0.8170377541142304,0.8160696999031946,0.8151016456921588,0.8151016456921588,0.814133591481123,0.814133591481123,0.8131655372700871,0.8121974830590513,0.8121974830590513,0.8112294288480155,0.8102613746369797,0.8092933204259438,0.8083252662149081,0.8083252662149081,0.8073572120038722,0.8063891577928364,0.8054211035818006,0.8044530493707648,0.8034849951597289,0.8025169409486931,0.8015488867376573,0.8005808325266215,0.7996127783155856,0.7986447241045499,0.797676669893514,0.7967086156824782,0.7957405614714425,0.7947725072604066,0.7938044530493708,0.7928363988383349,0.7918683446272992,0.7918683446272992,0.7909002904162633,0.7899322362052275,0.7889641819941917,0.7889641819941917,0.7879961277831559,0.78702807357212,0.7860600193610843,0.7850919651500484,0.7850919651500484,0.7841239109390126,0.7831558567279767,0.782187802516941,0.782187802516941,0.7812197483059051,0.7802516940948693,0.7792836398838335,0.7783155856727977,0.7773475314617618,0.7763794772507261,0.7754114230396902,0.7744433688286544,0.7734753146176185,0.7725072604065828,0.7725072604065828,0.771539206195547,0.7705711519845111,0.7696030977734754,0.7686350435624395,0.7676669893514037,0.7666989351403679,0.7657308809293321,0.7657308809293321,0.7647628267182962,0.7637947725072604,0.7637947725072604,0.7628267182962246,0.7618586640851888,0.7608906098741529,0.7599225556631172,0.7589545014520813,0.7579864472410455,0.7570183930300097,0.7560503388189739,0.755082284607938,0.7541142303969022,0.7531461761858664,0.7521781219748306,0.7512100677637947,0.750242013552759,0.7492739593417231,0.7492739593417231,0.7483059051306873,0.7473378509196515,0.7463697967086157,0.7454017424975798,0.7454017424975798,0.744433688286544,0.7434656340755083,0.7424975798644724,0.7415295256534365,0.7405614714424008,0.739593417231365,0.7386253630203291,0.7376573088092934,0.7366892545982575,0.7357212003872217,0.7347531461761858,0.7337850919651501,0.7328170377541142,0.7318489835430784,0.7308809293320426,0.7299128751210068,0.7289448209099709,0.7279767666989352,0.7279767666989352,0.7270087124878993,0.7260406582768635,0.7260406582768635,0.7250726040658277,0.7241045498547919,0.723136495643756,0.7221684414327202,0.7212003872216844,0.7202323330106486,0.7192642787996127,0.718296224588577,0.7173281703775412,0.7163601161665053,0.7153920619554696,0.7144240077444337,0.7134559535333979,0.712487899322362,0.712487899322362,0.7115198451113263,0.7105517909002904,0.7095837366892546,0.7086156824782188,0.707647628267183,0.7066795740561471,0.7066795740561471,0.7066795740561471,0.7057115198451114,0.7047434656340755,0.7037754114230397,0.7028073572120038,0.7018393030009681,0.7008712487899322,0.6999031945788964,0.6989351403678606,0.6979670861568248,0.6969990319457889,0.6960309777347532,0.6960309777347532,0.6950629235237173,0.6940948693126815,0.6931268151016456,0.6921587608906099,0.691190706679574,0.6902226524685382,0.6892545982575025,0.6882865440464666,0.6873184898354308,0.6873184898354308,0.686350435624395,0.6853823814133592,0.6844143272023233,0.6834462729912875,0.6824782187802517,0.6815101645692159,0.68054211035818,0.6795740561471443,0.6786060019361084,0.6776379477250726,0.6776379477250726,0.6766698935140368,0.675701839303001,0.6747337850919651,0.6737657308809293,0.6727976766698935,0.6718296224588577,0.6708615682478218,0.6698935140367861,0.6698935140367861,0.6698935140367861,0.6689254598257502,0.6679574056147144,0.6669893514036787,0.6660212971926428,0.665053242981607,0.6640851887705711,0.6631171345595354,0.6621490803484995,0.6611810261374637,0.6602129719264279,0.6592449177153921,0.6582768635043562,0.6573088092933205,0.6563407550822846,0.6553727008712488,0.6544046466602129,0.6534365924491772,0.6524685382381413,0.6515004840271055,0.6505324298160697,0.6495643756050339,0.648596321393998,0.6476282671829623,0.6466602129719264,0.6456921587608906,0.6447241045498547,0.643756050338819,0.6427879961277831,0.6418199419167473,0.6408518877057116,0.6398838334946757,0.6389157792836399,0.6389157792836399,0.6379477250726041,0.6369796708615683,0.6360116166505324,0.6350435624394967,0.6340755082284608,0.6340755082284608,0.6340755082284608,0.633107454017425,0.6321393998063891,0.6311713455953534,0.6302032913843175,0.6292352371732817,0.6282671829622459,0.6272991287512101,0.6263310745401742,0.6253630203291385,0.6243949661181026,0.6234269119070668,0.6224588576960309,0.6214908034849952,0.6205227492739593,0.6195546950629235,0.6185866408518877,0.6176185866408519,0.616650532429816,0.616650532429816,0.6156824782187803,0.6147144240077445,0.6137463697967086,0.6127783155856728,0.611810261374637,0.611810261374637,0.6108422071636012,0.6098741529525653,0.6089060987415296,0.6079380445304937,0.6069699903194579,0.6060019361084221,0.6050338818973863,0.6050338818973863,0.6040658276863504,0.6030977734753146,0.6021297192642788,0.601161665053243,0.6001936108422071,0.6001936108422071,0.5992255566311714,0.5982575024201355,0.5972894482090997,0.5963213939980639,0.5953533397870281,0.5943852855759922,0.5934172313649564,0.5924491771539206,0.5914811229428848,0.590513068731849,0.5895450145208132,0.5885769603097774,0.5876089060987415,0.5866408518877058,0.5856727976766699,0.5847047434656341,0.5847047434656341,0.5837366892545982,0.5827686350435625,0.5818005808325266,0.5808325266214908,0.579864472410455,0.5788964181994192,0.5779283639883833,0.5769603097773476,0.5759922555663117,0.5750242013552759,0.5750242013552759,0.5750242013552759,0.57405614714424,0.5730880929332043,0.5721200387221684,0.5711519845111326,0.5701839303000968,0.569215876089061,0.5682478218780251,0.5672797676669894,0.5663117134559535,0.5653436592449177,0.5643756050338818,0.5634075508228461,0.5624394966118103,0.5614714424007744,0.5605033881897387,0.5595353339787028,0.558567279767667,0.5575992255566312,0.5566311713455954,0.5556631171345595,0.5546950629235237,0.5537270087124879,0.5527589545014521,0.5517909002904162,0.5508228460793805,0.5498547918683446,0.5488867376573088,0.547918683446273,0.5469506292352372,0.5459825750242013,0.5450145208131656,0.5440464666021297,0.5430784123910939,0.542110358180058,0.542110358180058,0.542110358180058,0.5411423039690223,0.5401742497579864,0.5392061955469506,0.5382381413359149,0.537270087124879,0.5363020329138432,0.5363020329138432,0.5353339787028074,0.5343659244917716,0.5333978702807357,0.5324298160696999,0.5314617618586641,0.5314617618586641,0.5304937076476283,0.5295256534365924,0.5285575992255567,0.5275895450145208,0.526621490803485,0.5256534365924492,0.5256534365924492,0.5246853823814134,0.5237173281703775,0.5227492739593417,0.5217812197483059,0.5208131655372701,0.5198451113262342,0.5188770571151985,0.5179090029041626,0.5169409486931268,0.515972894482091,0.5150048402710552,0.5140367860600193,0.5130687318489835,0.5121006776379478,0.5111326234269119,0.510164569215876,0.5091965150048403,0.5082284607938045,0.5072604065827686,0.5062923523717329,0.505324298160697,0.5043562439496612,0.5033881897386253,0.5024201355275896,0.5014520813165537,0.5004840271055179,0.4995159728944821,0.4985479186834463,0.4975798644724105,0.49661181026137463,0.49564375605033884,0.494675701839303,0.4937076476282672,0.4927395934172314,0.49177153920619554,0.49080348499515974,0.4898354307841239,0.4888673765730881,0.4878993223620523,0.48693126815101645,0.48596321393998065,0.4849951597289448,0.484027105517909,0.48209099709583736,0.48112294288480156,0.4801548886737657,0.4791868344627299,0.4782187802516941,0.47725072604065827,0.4762826718296225,0.4753146176185866,0.4743465634075508,0.47337850919651503,0.4724104549854792,0.4714424007744434,0.47047434656340753,0.46950629235237173,0.46853823814133594,0.4675701839303001,0.4666021297192643,0.46563407550822844,0.46466602129719264,0.46369796708615685,0.462729912875121,0.4617618586640852,0.46079380445304935,0.45982575024201355,0.45885769603097776,0.4578896418199419,0.4569215876089061,0.45595353339787026,0.45498547918683446,0.45401742497579867,0.4530493707647628,0.452081316553727,0.45111326234269117,0.45014520813165537,0.4491771539206196,0.4482090997095837,0.44724104549854793,0.4462729912875121,0.4453049370764763,0.4443368828654405,0.44336882865440463,0.44240077444336884,0.441432720232333,0.4404646660212972,0.4394966118102614,0.43852855759922554,0.43756050338818975,0.4365924491771539,0.4356243949661181,0.4346563407550823,0.43368828654404645,0.43272023233301066,0.43272023233301066,0.4317521781219748,0.430784123910939,0.4298160696999032,0.42884801548886736,0.42787996127783157,0.4269119070667957,0.4269119070667957,0.4259438528557599,0.4249757986447241,0.42400774443368827,0.4230396902226525,0.4220716360116166,0.42110358180058083,0.42013552758954503,0.4191674733785092,0.4181994191674734,0.41723136495643753,0.41626331074540174,0.41529525653436594,0.4143272023233301,0.4133591481122943,0.41239109390125844,0.41045498547918685,0.409486931268151,0.4085188770571152,0.4075508228460794,0.40658276863504356,0.40561471442400776,0.4046466602129719,0.4036786060019361,0.4027105517909003,0.40174249757986447,0.40077444336882867,0.3998063891577928,0.3978702807357212,0.3969022265246854,0.3959341723136496,0.39496611810261373,0.39496611810261373,0.39399806389157793,0.39399806389157793,0.39303000968054214,0.3920619554695063,0.3910939012584705,0.39012584704743464,0.38915779283639884,0.38818973862536305,0.38818973862536305,0.3872216844143272,0.3862536302032914,0.38528557599225555,0.38431752178121975,0.38334946757018395,0.3823814133591481,0.3814133591481123,0.38044530493707646,0.37947725072604066,0.37850919651500486,0.377541142303969,0.3765730880929332,0.37560503388189737,0.37463697967086157,0.3736689254598258,0.3727008712487899,0.3717328170377541,0.3707647628267183,0.3697967086156825,0.3688286544046467,0.36786060019361083,0.36689254598257504,0.3659244917715392,0.3649564375605034,0.3639883833494676,0.36302032913843174,0.36205227492739595,0.3610842207163601,0.3601161665053243,0.3591481122942885,0.35818005808325265,0.35721200387221685,0.356243949661181,0.3552758954501452,0.3543078412391094,0.35333978702807356,0.35237173281703776,0.3514036786060019,0.3504356243949661,0.3494675701839303,0.34849951597289447,0.3475314617618587,0.3465634075508228,0.345595353339787,0.34462729912875123,0.3436592449177154,0.3426911907066796,0.34172313649564373,0.34075508228460794,0.33978702807357214,0.3388189738625363,0.3378509196515005,0.33688286544046464,0.33591481122942884,0.33494675701839305,0.3339787028073572,0.3330106485963214,0.33204259438528555,0.33107454017424975,0.33010648596321396,0.3291384317521781,0.3281703775411423,0.32720232333010646,0.32623426911907066,0.32526621490803487,0.324298160696999,0.3233301064859632,0.32236205227492737,0.3213939980638916,0.3204259438528558,0.3194578896418199,0.31848983543078413,0.31752178121974833,0.3165537270087125,0.3155856727976767,0.31461761858664083,0.31461761858664083,0.31364956437560504,0.31268151016456924,0.3117134559535334,0.3117134559535334,0.3107454017424976,0.30977734753146174,0.30880929332042595,0.30784123910939015,0.3068731848983543,0.3059051306873185,0.30493707647628265,0.30396902226524686,0.30300096805421106,0.3020329138431752,0.3010648596321394,0.30009680542110356,0.29912875121006777,0.29816069699903197,0.2971926427879961,0.2962245885769603,0.2952565343659245,0.2942884801548887,0.2933204259438529,0.29235237173281703,0.29138431752178123,0.2904162633107454,0.2894482090997096,0.2884801548886738,0.28751210067763794,0.28654404646660214,0.2855759922555663,0.2846079380445305,0.2836398838334947,0.28267182962245885,0.28170377541142305,0.2807357212003872,0.2797676669893514,0.2787996127783156,0.27783155856727976,0.27686350435624396,0.2758954501452081,0.2749273959341723,0.2739593417231365,0.27299128751210067,0.27202323330106487,0.271055179090029,0.2700871248789932,0.2691190706679574,0.2681510164569216,0.2671829622458858,0.26621490803484993,0.26524685382381413,0.26427879961277834,0.2633107454017425,0.2623426911907067,0.26137463697967084,0.26040658276863504,0.25943852855759925,0.2584704743465634,0.2575024201355276,0.25653436592449175,0.25556631171345595,0.25459825750242016,0.2536302032913843,0.2526621490803485,0.25169409486931266,0.25072604065827686,0.24975798644724104,0.24878993223620524,0.24782187802516942,0.2468538238141336,0.24588576960309777,0.24491771539206195,0.24394966118102615,0.24298160696999033,0.2420135527589545,0.24104549854791868,0.24007744433688286,0.23910939012584706,0.23814133591481124,0.2371732817037754,0.2362052274927396,0.23523717328170377,0.23426911907066797,0.23330106485963215,0.23233301064859632,0.2313649564375605,0.23039690222652467,0.22942884801548888,0.22846079380445306,0.22749273959341723,0.2265246853823814,0.22555663117134558,0.2245885769603098,0.22362052274927396,0.22265246853823814,0.22168441432720232,0.2207163601161665,0.2197483059051307,0.21878025169409487,0.21781219748305905,0.21684414327202323,0.2158760890609874,0.2149080348499516,0.21393998063891578,0.21297192642787996,0.21200387221684414,0.2110358180058083,0.21006776379477252,0.2090997095837367,0.20813165537270087,0.20716360116166505,0.20619554695062922,0.20522749273959343,0.2042594385285576,0.20329138431752178,0.20232333010648595,0.20135527589545016,0.20038722168441434,0.1994191674733785,0.1984511132623427,0.19748305905130686,0.19651500484027107,0.19554695062923524,0.19457889641819942,0.1936108422071636,0.19264278799612777,0.19167473378509198,0.19070667957405615,0.18973862536302033,0.1887705711519845,0.18780251694094868,0.1868344627299129,0.18586640851887706,0.18586640851887706,0.18489835430784124,0.18393030009680542,0.1829622458857696,0.1819941916747338,0.18102613746369797,0.18102613746369797,0.18005808325266215,0.17909002904162633,0.1781219748305905,0.1771539206195547,0.17618586640851888,0.17521781219748306,0.17424975798644723,0.1732817037754114,0.17231364956437561,0.1713455953533398,0.17037754114230397,0.16940948693126814,0.16844143272023232,0.16747337850919652,0.1665053242981607,0.16553727008712488,0.16456921587608905,0.16360116166505323,0.16263310745401743,0.1616650532429816,0.1606969990319458,0.15972894482090996,0.15876089060987417,0.15779283639883834,0.15682478218780252,0.1558567279767667,0.15488867376573087,0.15392061955469508,0.15295256534365925,0.15198451113262343,0.1510164569215876,0.15004840271055178,0.14908034849951599,0.14811229428848016,0.14714424007744434,0.14617618586640851,0.1452081316553727,0.1442400774443369,0.14327202323330107,0.14230396902226525,0.14133591481122942,0.1403678606001936,0.1393998063891578,0.13843175217812198,0.13746369796708616,0.13649564375605033,0.1355275895450145,0.1345595353339787,0.1335914811229429,0.13262342691190707,0.13165537270087124,0.13068731848983542,0.12971926427879962,0.1287512100677638,0.12778315585672798,0.12681510164569215,0.12487899322362052,0.12391093901258471,0.12294288480154889,0.12197483059051308,0.12100677637947725,0.12003872216844143,0.1181026137463698,0.11713455953533398,0.11616650532429816,0.11519845111326234,0.11423039690222653,0.1132623426911907,0.1122942884801549,0.11132623426911907,0.11035818005808325,0.10939012584704744,0.10842207163601161,0.10648596321393998,0.10551790900290416,0.10454985479186835,0.10358180058083252,0.10261374636979671,0.10164569215876089,0.10067763794772508,0.09970958373668926,0.09874152952565343,0.09777347531461762,0.0968054211035818,0.09583736689254599,0.09486931268151017,0.09390125847047434,0.09293320425943853,0.09196515004840271,0.0909970958373669,0.09002904162633107,0.08906098741529525,0.08809293320425944,0.08712487899322362,0.08615682478218781,0.08518877057115198,0.08422071636011616,0.08325266214908035,0.08228460793804453,0.08131655372700872,0.0803484995159729,0.07938044530493708,0.07841239109390126,0.07744433688286544,0.07744433688286544,0.07647628267182963,0.0755082284607938,0.07454017424975799,0.07357212003872217,0.07260406582768635,0.07163601161665054,0.07066795740561471,0.0696999031945789,0.06873184898354308,0.06776379477250725,0.06679574056147145,0.06582768635043562,0.06485963213939981,0.06389157792836399,0.06292352371732816,0.061955469506292354,0.06098741529525654,0.060019361084220714,0.0590513068731849,0.05808325266214908,0.057115198451113264,0.05614714424007745,0.05517909002904162,0.05421103581800581,0.05324298160696999,0.05227492739593417,0.051306873184898356,0.05033881897386254,0.049370764762826716,0.0484027105517909,0.04743465634075508,0.046466602129719266,0.04549854791868345,0.044530493707647625,0.044530493707647625,0.04356243949661181,0.04259438528557599,0.04259438528557599,0.041626331074540175,0.04065827686350436,0.03969022265246854,0.03872216844143272,0.0377541142303969,0.036786060019361085,0.03581800580832527,0.03484995159728945,0.03388189738625363,0.03291384317521781,0.031945788964181994,0.030977734753146177,0.030009680542110357,0.02904162633107454,0.028073572120038724,0.027105517909002903,0.026137463697967087,0.02516940948693127,0.02420135527589545,0.023233301064859633,0.022265246853823813,0.021297192642787996,0.02032913843175218,0.01936108422071636,0.018393030009680542,0.017424975798644726,0.016456921587608905,0.015488867376573089,0.01452081316553727,0.013552758954501452,0.012584704743465635,0.011616650532429816,0.010648596321393998,0.00968054211035818,0.008712487899322363,0.007744433688286544,0.006776379477250726,0.005808325266214908,0.00484027105517909,0.003872216844143272,0.002904162633107454,0.002904162633107454,0.001936108422071636,0.000968054211035818,0.0],\"xaxis\":\"x\",\"y\":[0.7341862117981521,0.7347083926031295,0.7352313167259786,0.7357549857549858,0.7362794012829651,0.7368045649072753,0.7373304782298359,0.7378571428571429,0.7383845604002859,0.7389127324749643,0.7399713467048711,0.7405017921146954,0.7410329985652798,0.741564967695621,0.7420977011494253,0.7426312005751258,0.7431654676258993,0.7437005039596832,0.7442363112391931,0.7447728911319395,0.7453102453102453,0.7458483754512636,0.7463872832369942,0.7469269703543022,0.7474674384949349,0.7480086893555394,0.7485507246376811,0.7490935460478607,0.7496371552975326,0.7501815541031227,0.7507267441860465,0.7518195050946143,0.752367079388201,0.7529154518950437,0.7534646243617797,0.754014598540146,0.7545653761869978,0.7551169590643275,0.7556693489392831,0.7562225475841874,0.7567765567765568,0.7573313782991202,0.7578870139398386,0.7584434654919237,0.7590007347538574,0.7595588235294117,0.7601177336276674,0.7606774668630338,0.7612380250552689,0.7617994100294986,0.7623616236162362,0.7629246676514032,0.7634885439763488,0.7640532544378699,0.764618800888231,0.7651851851851852,0.765752409191994,0.766320474777448,0.7668893838158871,0.7674591381872214,0.7680297397769517,0.7686011904761905,0.7691734921816828,0.7697466467958272,0.7703206562266965,0.7708955223880597,0.7714712471994025,0.7720478325859492,0.7726252804786836,0.7732035928143712,0.7737827715355805,0.7743628185907047,0.7749437359339835,0.7755255255255256,0.7761081893313299,0.7766917293233083,0.7772761474793077,0.7778614457831325,0.7784476262245666,0.7790346907993967,0.779622641509434,0.7802114803625377,0.780801209372638,0.7813918305597579,0.7819833459500378,0.7825757575757576,0.7831690674753601,0.783763277693475,0.7843583902809416,0.7849544072948328,0.785551330798479,0.7861491628614916,0.7867479055597868,0.7873475609756098,0.7879481311975591,0.7885496183206107,0.7891520244461421,0.7897553516819572,0.7903596021423106,0.7909647779479326,0.7915708812260537,0.7921779141104295,0.7927858787413661,0.793394777265745,0.7940046118370484,0.7946153846153846,0.7952270977675134,0.7958397534668721,0.7964533538936006,0.7970679012345679,0.7976833976833977,0.7982998454404946,0.7989172467130704,0.7995356037151703,0.8001549186676995,0.8007751937984496,0.8013964313421257,0.8020186335403726,0.8026418026418026,0.8032659409020217,0.8038910505836576,0.8045171339563862,0.8051441932969603,0.8057722308892356,0.8064012490241999,0.80703125,0.8076622361219703,0.8082942097026604,0.8089271730618638,0.8095611285266457,0.8101960784313725,0.8108320251177394,0.8114689709347996,0.8121069182389937,0.8127458693941778,0.8133858267716535,0.814026792750197,0.8146687697160884,0.8153117600631413,0.815955766192733,0.8166007905138339,0.817246835443038,0.8178939034045922,0.8185419968304279,0.8191911181601903,0.8198412698412698,0.8204924543288324,0.8211446740858506,0.8217979315831344,0.822452229299363,0.8223107569721115,0.8229665071770335,0.8236233040702314,0.8242811501597445,0.8249400479616307,0.8256,0.8269230769230769,0.8275862068965517,0.8282504012841091,0.8289156626506025,0.8295819935691319,0.830249396621078,0.8309178743961353,0.8315874294923449,0.832258064516129,0.8329297820823245,0.8336025848142165,0.8342764753435732,0.8349514563106796,0.8356275303643724,0.8363047001620746,0.8369829683698297,0.8376623376623377,0.8383428107229894,0.8390243902439024,0.8397070789259561,0.8403908794788274,0.8410757946210269,0.8417618270799347,0.8424489795918367,0.8431372549019608,0.8438266557645135,0.8445171849427169,0.8452088452088452,0.8459016393442623,0.8465955701394585,0.8472906403940886,0.847986852917009,0.8486842105263158,0.8493827160493828,0.8500823723228995,0.8507831821929102,0.8514851485148515,0.8521882741535921,0.8528925619834711,0.8535980148883374,0.8534768211920529,0.8541839270919636,0.8548922056384743,0.8556016597510373,0.856312292358804,0.857024106400665,0.8569051580698835,0.8567860116569526,0.8566666666666667,0.8573811509591326,0.8572621035058431,0.8579782790309106,0.8578595317725752,0.8585774058577406,0.8592964824120602,0.8600167644593462,0.860738255033557,0.8614609571788413,0.8621848739495799,0.862910008410429,0.8627946127946128,0.8626790227464195,0.8625632377740303,0.8632911392405064,0.8640202702702703,0.8647506339814032,0.8654822335025381,0.8662150719729044,0.8661016949152542,0.8668363019508057,0.8675721561969439,0.8674596431605778,0.8673469387755102,0.8680851063829788,0.8679727427597955,0.8687127024722933,0.8694539249146758,0.8701964133219471,0.870940170940171,0.8708297690333618,0.8707191780821918,0.87146529562982,0.8713550600343053,0.871244635193133,0.8711340206185567,0.8718830610490111,0.8726333907056799,0.8733850129198967,0.8732758620689656,0.8740293356341674,0.8747841105354058,0.874675885911841,0.8754325259515571,0.8761904761904762,0.8760831889081456,0.8768430182133564,0.8767361111111112,0.8766290182450044,0.8773913043478261,0.8772845953002611,0.8780487804878049,0.8779424585876199,0.8787085514834206,0.879475982532751,0.8793706293706294,0.8792650918635171,0.8791593695271454,0.8790534618755478,0.8798245614035087,0.8797190517998245,0.8796133567662566,0.8803869832893579,0.8811619718309859,0.8819383259911894,0.8827160493827161,0.8826125330979699,0.8825088339222615,0.883289124668435,0.8840707964601769,0.8839681133746679,0.8847517730496454,0.8846495119787046,0.8854351687388987,0.8853333333333333,0.8861209964412812,0.8869100623330365,0.8868092691622104,0.8876003568242641,0.8875,0.8873994638069705,0.8881932021466905,0.8889883616830797,0.8897849462365591,0.8896860986547085,0.8904847396768402,0.8912848158131177,0.8920863309352518,0.891989198919892,0.8927927927927928,0.8935978358881875,0.8935018050541517,0.8943089430894309,0.8942133815551537,0.8941176470588236,0.894927536231884,0.8948322756119673,0.895644283121597,0.896457765667575,0.8972727272727272,0.8980891719745223,0.8979963570127505,0.8988149498632635,0.8987226277372263,0.8986301369863013,0.8985374771480804,0.898444647758463,0.8983516483516484,0.8982584784601283,0.8990825688073395,0.898989898989899,0.8988970588235294,0.8988040478380864,0.8987108655616943,0.8986175115207373,0.8994464944649446,0.8993536472760849,0.9001848428835489,0.9000925069380203,0.9009259259259259,0.901760889712697,0.9016697588126159,0.9025069637883009,0.9033457249070632,0.9041860465116279,0.904096834264432,0.9049394221808015,0.9057835820895522,0.9066293183940243,0.9065420560747663,0.9064546304957904,0.9073033707865169,0.9081537019681349,0.9080675422138836,0.907981220657277,0.9078947368421053,0.9078080903104422,0.9077212806026366,0.9076343072573044,0.9084905660377358,0.9093484419263456,0.9102079395085066,0.9101229895931883,0.9100378787878788,0.909952606635071,0.9108159392789373,0.9107312440645774,0.9106463878326996,0.9105613701236918,0.9104761904761904,0.9103908484270734,0.9103053435114504,0.9102196752626552,0.9101338432122371,0.9100478468899521,0.9099616858237548,0.909875359539789,0.9097888675623801,0.9106628242074928,0.9105769230769231,0.9114533205004812,0.9113680154142582,0.9122468659594986,0.9121621621621622,0.9130434782608695,0.9129593810444874,0.9128751210067764,0.9127906976744186,0.9127061105722599,0.9135922330097087,0.9135082604470359,0.9134241245136187,0.9143135345666992,0.9142300194931774,0.9151219512195122,0.916015625,0.9159335288367546,0.9158512720156555,0.9167482859941234,0.9166666666666666,0.9175662414131501,0.9174852652259332,0.9183874139626352,0.9183070866141733,0.9192118226600985,0.9191321499013807,0.9190523198420533,0.9189723320158103,0.9188921859545005,0.9188118811881189,0.9187314172447968,0.9186507936507936,0.9185700099304865,0.9184890656063618,0.918407960199005,0.9193227091633466,0.9202392821535393,0.9201596806387226,0.9200799200799201,0.921,0.9209209209209209,0.9208416833667334,0.9207622868605817,0.9206827309236948,0.9206030150753769,0.920523138832998,0.9214501510574018,0.9213709677419355,0.9212916246215943,0.9212121212121213,0.9211324570273003,0.9210526315789473,0.9209726443768997,0.920892494929006,0.9208121827411168,0.9217479674796748,0.9226856561546287,0.9226069246435845,0.9235474006116208,0.923469387755102,0.9244126659856997,0.9243353783231084,0.9242579324462641,0.9241803278688525,0.9251282051282051,0.9250513347022588,0.9249743062692704,0.9248971193415638,0.9248197734294542,0.9247422680412372,0.9246646026831785,0.9245867768595041,0.9245087900723888,0.9244306418219461,0.9243523316062177,0.9253112033195021,0.9262720664589823,0.9261954261954262,0.9261186264308012,0.9270833333333334,0.927007299270073,0.9269311064718163,0.9278996865203761,0.9288702928870293,0.9287958115183246,0.9287211740041929,0.9286463798530955,0.9296218487394958,0.9305993690851735,0.9315789473684211,0.9325605900948367,0.9324894514767933,0.9334741288278775,0.9344608879492601,0.9343915343915344,0.934322033898305,0.9342523860021209,0.9341825902335457,0.9341126461211477,0.9351063829787234,0.9350372736954207,0.9360341151385928,0.935965848452508,0.9358974358974359,0.9358288770053476,0.9357601713062098,0.9356913183279743,0.9356223175965666,0.9366272824919442,0.9376344086021505,0.9386437029063509,0.9385775862068966,0.9385113268608414,0.9384449244060475,0.9383783783783783,0.9393939393939394,0.9393282773564464,0.9392624728850325,0.9391965255157437,0.9402173913043478,0.940152339499456,0.9400871459694989,0.9400218102508179,0.9399563318777293,0.940983606557377,0.9409190371991247,0.940854326396495,0.9418859649122807,0.9418221734357849,0.9417582417582417,0.9416941694169417,0.9416299559471366,0.9415656008820287,0.9415011037527594,0.9425414364640884,0.9424778761061947,0.9424141749723145,0.9434589800443459,0.9433962264150944,0.9433333333333334,0.9432703003337041,0.9432071269487751,0.9431438127090301,0.9430803571428571,0.9430167597765363,0.9429530201342282,0.9428891377379619,0.9439461883408071,0.9438832772166106,0.9449438202247191,0.9448818897637795,0.9448198198198198,0.9458850056369785,0.945823927765237,0.9457627118644067,0.9457013574660633,0.9456398640996603,0.9467120181405896,0.9466515323496028,0.946590909090909,0.9465301478953356,0.9464692482915718,0.9464082098061574,0.9463470319634704,0.9462857142857143,0.9462242562929062,0.9461626575028637,0.9461009174311926,0.9460390355912744,0.9459770114942528,0.9459148446490219,0.945852534562212,0.9457900807381776,0.9457274826789839,0.945664739884393,0.9467592592592593,0.9466975666280417,0.9466357308584686,0.9465737514518002,0.9476744186046512,0.9476135040745053,0.9475524475524476,0.9474912485414235,0.947429906542056,0.9485380116959065,0.9484777517564403,0.9484173505275498,0.9483568075117371,0.9494712103407755,0.9494117647058824,0.9493521790341578,0.9492924528301887,0.9492325855962219,0.9491725768321513,0.9491124260355029,0.9490521327014217,0.9489916963226572,0.9489311163895487,0.9488703923900119,0.95,0.9499404052443385,0.9498806682577565,0.9498207885304659,0.9497607655502392,0.9497005988023952,0.9496402877697842,0.9495798319327731,0.9507211538461539,0.950661853188929,0.9506024096385542,0.9517490952955368,0.9516908212560387,0.9516324062877872,0.9515738498789347,0.9515151515151515,0.9514563106796117,0.951397326852977,0.9513381995133819,0.951278928136419,0.9512195121951219,0.9511599511599511,0.9511002444987775,0.9510403916768666,0.9509803921568627,0.950920245398773,0.9508599508599509,0.9520295202952029,0.9519704433497537,0.9519112207151664,0.9518518518518518,0.9517923362175525,0.9529702970297029,0.952912019826518,0.9528535980148883,0.9527950310559006,0.9527363184079602,0.9526774595267746,0.9526184538653366,0.9525593008739076,0.9525,0.9524405506883604,0.9523809523809523,0.9523212045169385,0.9522613065326633,0.9522012578616352,0.9521410579345088,0.9520807061790668,0.952020202020202,0.9519595448798989,0.9518987341772152,0.9531051964512041,0.9530456852791879,0.9529860228716646,0.9541984732824428,0.954140127388535,0.9540816326530612,0.9540229885057471,0.9539641943734015,0.9539052496798975,0.9538461538461539,0.9537869062901155,0.9537275064267352,0.9536679536679536,0.9536082474226805,0.9535483870967741,0.9534883720930233,0.9534282018111255,0.9533678756476683,0.9546044098573282,0.9545454545454546,0.9544863459037711,0.9544270833333334,0.954367666232073,0.9543080939947781,0.954248366013072,0.9554973821989529,0.9567496723460026,0.9566929133858267,0.9566360052562418,0.9565789473684211,0.9565217391304348,0.9564643799472295,0.9564068692206077,0.9563492063492064,0.9562913907284768,0.9562334217506632,0.9561752988047809,0.9561170212765957,0.9573901464713716,0.9573333333333334,0.9572763684913218,0.9572192513368984,0.9571619812583668,0.9571045576407506,0.9570469798657718,0.956989247311828,0.9569313593539704,0.9568733153638814,0.9581646423751687,0.9581081081081081,0.9580514208389715,0.9579945799457995,0.9579375848032564,0.9578804347826086,0.9578231292517007,0.9577656675749319,0.9577080491132333,0.9576502732240437,0.957592339261286,0.958904109589041,0.9588477366255144,0.9587912087912088,0.9587345254470426,0.9586776859504132,0.9586206896551724,0.9585635359116023,0.9585062240663901,0.9584487534626038,0.9597780859916782,0.9611111111111111,0.96105702364395,0.9610027855153204,0.9609483960948396,0.9608938547486033,0.9608391608391609,0.9607843137254902,0.9607293127629734,0.9606741573033708,0.960618846694796,0.9605633802816902,0.9605077574047954,0.96045197740113,0.9603960396039604,0.9603399433427762,0.9602836879432625,0.9602272727272727,0.9601706970128022,0.9601139601139601,0.9600570613409415,0.96,0.9599427753934192,0.9598853868194842,0.9598278335724534,0.9597701149425287,0.9597122302158273,0.9596541786743515,0.9595959595959596,0.9595375722543352,0.959479015918958,0.9594202898550724,0.9593613933236574,0.9593023255813954,0.9606986899563319,0.9606413994169096,0.9605839416058394,0.9605263157894737,0.9604685212298683,0.9604105571847508,0.9618208516886931,0.9632352941176471,0.9631811487481591,0.9631268436578171,0.9630723781388478,0.9630177514792899,0.9629629629629629,0.9629080118694362,0.962852897473997,0.9627976190476191,0.96274217585693,0.9626865671641791,0.9626307922272048,0.9625748502994012,0.9625187406296851,0.9624624624624625,0.9624060150375939,0.9623493975903614,0.9622926093514329,0.9622356495468278,0.9636913767019667,0.9636363636363636,0.9635811836115327,0.9635258358662614,0.9634703196347032,0.9634146341463414,0.9648854961832061,0.9648318042813455,0.9647779479326187,0.9647239263803681,0.9646697388632872,0.9646153846153847,0.9645608628659477,0.9645061728395061,0.9659969088098919,0.9659442724458205,0.9658914728682171,0.9658385093167702,0.9657853810264385,0.9657320872274143,0.9672386895475819,0.9671875,0.9671361502347418,0.9670846394984326,0.967032967032967,0.9669811320754716,0.9669291338582677,0.9668769716088328,0.966824644549763,0.9667721518987342,0.9667194928684627,0.9666666666666667,0.9666136724960255,0.9665605095541401,0.9665071770334929,0.9664536741214057,0.9664,0.967948717948718,0.9678972712680578,0.9678456591639871,0.9677938808373591,0.967741935483871,0.9676898222940227,0.9676375404530745,0.9675850891410048,0.9675324675324676,0.967479674796748,0.9674267100977199,0.9690048939641109,0.9705882352941176,0.9705400981996727,0.9704918032786886,0.9704433497536946,0.9703947368421053,0.9703459637561779,0.9702970297029703,0.9702479338842975,0.9701986754966887,0.9701492537313433,0.9700996677740864,0.9700499168053245,0.97,0.9699499165275459,0.9698996655518395,0.9698492462311558,0.9697986577181208,0.9697478991596639,0.9696969696969697,0.96964586846543,0.9695945945945946,0.9695431472081218,0.9694915254237289,0.969439728353141,0.9693877551020408,0.969335604770017,0.9692832764505119,0.9692307692307692,0.9691780821917808,0.9691252144082333,0.9690721649484536,0.9690189328743546,0.9689655172413794,0.9689119170984456,0.9688581314878892,0.9705372616984402,0.9722222222222222,0.9721739130434782,0.9721254355400697,0.9720767888307156,0.972027972027972,0.9719789842381786,0.9719298245614035,0.9736379613356766,0.9735915492957746,0.9735449735449735,0.9734982332155477,0.9734513274336283,0.973404255319149,0.9751332149200711,0.9750889679715302,0.9750445632798574,0.975,0.9749552772808586,0.974910394265233,0.9748653500897666,0.9766187050359713,0.9765765765765766,0.9765342960288809,0.976491862567812,0.9764492753623188,0.9764065335753176,0.9763636363636363,0.97632058287796,0.9762773722627737,0.9762340036563071,0.9761904761904762,0.9761467889908257,0.9761029411764706,0.9760589318600368,0.9760147601476015,0.9759704251386322,0.975925925925926,0.9758812615955473,0.9758364312267658,0.9757914338919925,0.9757462686567164,0.9757009345794393,0.9756554307116105,0.975609756097561,0.9755639097744361,0.975517890772128,0.9754716981132076,0.9754253308128544,0.9753787878787878,0.9753320683111955,0.9752851711026616,0.9752380952380952,0.9751908396946565,0.9751434034416826,0.975095785440613,0.9750479846449136,0.975,0.9749518304431599,0.974903474903475,0.9748549323017408,0.9748062015503876,0.974757281553398,0.9747081712062257,0.9746588693957114,0.974559686888454,0.9745098039215686,0.9744597249508841,0.9744094488188977,0.9743589743589743,0.974308300395257,0.9742574257425742,0.9742063492063492,0.974155069582505,0.9741035856573705,0.9740518962075848,0.974,0.9739478957915831,0.9738955823293173,0.9738430583501007,0.9737903225806451,0.9737373737373738,0.9736842105263158,0.973630831643002,0.9735772357723578,0.9735234215885947,0.9734693877551021,0.9734151329243353,0.9733606557377049,0.973305954825462,0.9732510288065843,0.9731958762886598,0.9731404958677686,0.9730848861283644,0.9730290456431535,0.972972972972973,0.9729166666666667,0.9728601252609603,0.9728033472803347,0.9727463312368972,0.9726890756302521,0.9726315789473684,0.9725738396624473,0.9725158562367865,0.972457627118644,0.9723991507430998,0.9723404255319149,0.9722814498933902,0.9722222222222222,0.9721627408993576,0.9721030042918455,0.9720430107526882,0.9719827586206896,0.9719222462203023,0.9718614718614719,0.9718004338394793,0.9717391304347827,0.9738562091503268,0.9737991266375546,0.973741794310722,0.9736842105263158,0.9736263736263736,0.973568281938326,0.9735099337748344,0.9756637168141593,0.975609756097561,0.9755555555555555,0.9755011135857461,0.9754464285714286,0.9753914988814317,0.9753363228699552,0.9752808988764045,0.9752252252252253,0.9751693002257337,0.9751131221719457,0.9750566893424036,0.975,0.9749430523917996,0.9748858447488584,0.9748283752860412,0.9747126436781609,0.9746543778801844,0.9745958429561201,0.9745370370370371,0.974477958236659,0.9744186046511628,0.9743589743589743,0.9742990654205608,0.9742388758782201,0.9741784037558685,0.9741176470588235,0.9740566037735849,0.9739336492890995,0.9738717339667459,0.9738095238095238,0.9737470167064439,0.9760765550239234,0.9760191846522782,0.9783653846153846,0.9783132530120482,0.9782608695652174,0.9782082324455206,0.9781553398058253,0.9781021897810219,0.9780487804878049,0.980440097799511,0.9803921568627451,0.9803439803439803,0.9802955665024631,0.980246913580247,0.9801980198019802,0.9801488833746899,0.9800995024875622,0.9800498753117207,0.98,0.9799498746867168,0.9798994974874372,0.9798488664987406,0.9797979797979798,0.979746835443038,0.9796954314720813,0.9796437659033079,0.9795918367346939,0.979539641943734,0.9794871794871794,0.9794344473007712,0.979381443298969,0.979328165374677,0.9792746113989638,0.9792207792207792,0.9791666666666666,0.97911227154047,0.9790575916230366,0.979002624671916,0.9789473684210527,0.978891820580475,0.9788359788359788,0.9787798408488063,0.9787234042553191,0.9786666666666667,0.9786096256684492,0.9785522788203753,0.978494623655914,0.9784366576819407,0.9783783783783784,0.978319783197832,0.9782608695652174,0.9782016348773842,0.9781420765027322,0.9780821917808219,0.978021978021978,0.977961432506887,0.9779005524861878,0.9778393351800554,0.9777777777777777,0.9777158774373259,0.9776536312849162,0.9775910364145658,0.9775280898876404,0.9774647887323944,0.9774011299435028,0.9773371104815864,0.9772727272727273,0.9772079772079773,0.9771428571428571,0.9770773638968482,0.9770114942528736,0.9769452449567724,0.976878612716763,0.9768115942028985,0.9767441860465116,0.9766763848396501,0.9766081871345029,0.9765395894428153,0.9764705882352941,0.976401179941003,0.9763313609467456,0.9762611275964391,0.9761904761904762,0.9761194029850746,0.9760479041916168,0.975975975975976,0.9789156626506024,0.9788519637462235,0.9787878787878788,0.9787234042553191,0.9817073170731707,0.981651376146789,0.9815950920245399,0.9815384615384616,0.9814814814814815,0.9814241486068112,0.9813664596273292,0.9813084112149533,0.98125,0.9811912225705329,0.9811320754716981,0.9810725552050473,0.9810126582278481,0.9809523809523809,0.9808917197452229,0.9808306709265175,0.9807692307692307,0.9807073954983923,0.9806451612903225,0.9805825242718447,0.9805194805194806,0.9804560260586319,0.9803921568627451,0.980327868852459,0.9802631578947368,0.9801980198019802,0.9801324503311258,0.9800664451827242,0.98,0.979933110367893,0.9798657718120806,0.9797979797979798,0.9797297297297297,0.9796610169491525,0.9795918367346939,0.9795221843003413,0.9794520547945206,0.979381443298969,0.9793103448275862,0.9792387543252595,0.9791666666666666,0.9790940766550522,0.9790209790209791,0.9789473684210527,0.9788732394366197,0.9787985865724381,0.9787234042553191,0.9786476868327402,0.9785714285714285,0.978494623655914,0.9784172661870504,0.9783393501805054,0.9782608695652174,0.9781818181818182,0.9781021897810219,0.978021978021978,0.9779411764705882,0.977859778597786,0.9777777777777777,0.9776951672862454,0.9776119402985075,0.9775280898876404,0.9774436090225563,0.9773584905660377,0.9772727272727273,0.9771863117870723,0.9770992366412213,0.9770114942528736,0.9769230769230769,0.9768339768339769,0.9767441860465116,0.9766536964980544,0.9765625,0.9764705882352941,0.9763779527559056,0.9762845849802372,0.9761904761904762,0.9760956175298805,0.976,0.9759036144578314,0.9758064516129032,0.9757085020242915,0.975609756097561,0.9755102040816327,0.9754098360655737,0.9753086419753086,0.9752066115702479,0.975103734439834,0.975,0.9748953974895398,0.9747899159663865,0.9746835443037974,0.9745762711864406,0.9744680851063829,0.9743589743589743,0.9742489270386266,0.9741379310344828,0.974025974025974,0.9739130434782609,0.9737991266375546,0.9736842105263158,0.973568281938326,0.9734513274336283,0.9733333333333334,0.9732142857142857,0.9730941704035875,0.972972972972973,0.9728506787330317,0.9727272727272728,0.9726027397260274,0.9724770642201835,0.9723502304147466,0.9722222222222222,0.9720930232558139,0.9719626168224299,0.971830985915493,0.9716981132075472,0.9715639810426541,0.9714285714285714,0.9712918660287081,0.9711538461538461,0.9710144927536232,0.970873786407767,0.9707317073170731,0.9705882352941176,0.9704433497536946,0.9702970297029703,0.9701492537313433,0.97,0.9698492462311558,0.9696969696969697,0.9746192893401016,0.9744897959183674,0.9743589743589743,0.9742268041237113,0.9740932642487047,0.9739583333333334,0.9790575916230366,0.9789473684210527,0.9788359788359788,0.9787234042553191,0.9786096256684492,0.978494623655914,0.9783783783783784,0.9782608695652174,0.9781420765027322,0.978021978021978,0.9779005524861878,0.9777777777777777,0.9776536312849162,0.9775280898876404,0.9774011299435028,0.9772727272727273,0.9771428571428571,0.9770114942528736,0.976878612716763,0.9767441860465116,0.9766081871345029,0.9764705882352941,0.9763313609467456,0.9761904761904762,0.9760479041916168,0.9759036144578314,0.9757575757575757,0.975609756097561,0.9754601226993865,0.9753086419753086,0.9751552795031055,0.975,0.9748427672955975,0.9746835443037974,0.9745222929936306,0.9743589743589743,0.9741935483870968,0.974025974025974,0.9738562091503268,0.9736842105263158,0.9735099337748344,0.9733333333333334,0.9731543624161074,0.972972972972973,0.9727891156462585,0.9726027397260274,0.9724137931034482,0.9722222222222222,0.972027972027972,0.971830985915493,0.9716312056737588,0.9714285714285714,0.9712230215827338,0.9710144927536232,0.9708029197080292,0.9705882352941176,0.9703703703703703,0.9699248120300752,0.9696969696969697,0.9694656488549618,0.9692307692307692,0.9689922480620154,0.96875,0.9682539682539683,0.968,0.967741935483871,0.967479674796748,0.9672131147540983,0.9669421487603306,0.9666666666666667,0.9663865546218487,0.9661016949152542,0.9658119658119658,0.9655172413793104,0.9649122807017544,0.9646017699115044,0.9642857142857143,0.963963963963964,0.9636363636363636,0.963302752293578,0.9629629629629629,0.9626168224299065,0.9622641509433962,0.9619047619047619,0.9615384615384616,0.9611650485436893,0.9607843137254902,0.9603960396039604,0.96,0.9595959595959596,0.9591836734693877,0.9587628865979382,0.9583333333333334,0.9578947368421052,0.9574468085106383,0.956989247311828,0.9565217391304348,0.9560439560439561,0.9555555555555556,0.9550561797752809,0.9545454545454546,0.9540229885057471,0.9534883720930233,0.9529411764705882,0.9523809523809523,0.963855421686747,0.9634146341463414,0.9629629629629629,0.9625,0.9620253164556962,0.9615384615384616,0.961038961038961,0.9605263157894737,0.96,0.9594594594594594,0.958904109589041,0.9583333333333334,0.9577464788732394,0.9571428571428572,0.9565217391304348,0.9558823529411765,0.9552238805970149,0.9545454545454546,0.9538461538461539,0.953125,0.9523809523809523,0.9516129032258065,0.9508196721311475,0.95,0.9491525423728814,0.9482758620689655,0.9473684210526315,0.9464285714285714,0.9454545454545454,0.9444444444444444,0.9433962264150944,0.9423076923076923,0.9411764705882353,0.94,0.9387755102040817,0.9583333333333334,0.9574468085106383,0.9565217391304348,0.9777777777777777,0.9772727272727273,0.9767441860465116,0.9761904761904762,0.975609756097561,0.975,0.9743589743589743,0.9736842105263158,0.972972972972973,0.9722222222222222,0.9714285714285714,0.9705882352941176,0.9696969696969697,0.96875,0.967741935483871,0.9666666666666667,0.9655172413793104,0.9642857142857143,0.9629629629629629,0.9615384615384616,0.96,0.9583333333333334,0.9565217391304348,0.9545454545454546,0.9523809523809523,0.95,0.9473684210526315,0.9444444444444444,0.9411764705882353,0.9375,0.9333333333333333,0.9285714285714286,0.9230769230769231,0.9166666666666666,0.9090909090909091,0.9,0.8888888888888888,0.875,0.8571428571428571,0.8333333333333334,0.8,0.75,1.0,1.0,1.0,1.0],\"yaxis\":\"y\",\"type\":\"scatter\"}],                        {\"template\":{\"data\":{\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"white\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"white\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"#C8D4E3\",\"linecolor\":\"#C8D4E3\",\"minorgridcolor\":\"#C8D4E3\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"#C8D4E3\",\"linecolor\":\"#C8D4E3\",\"minorgridcolor\":\"#C8D4E3\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"choropleth\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"choropleth\"}],\"contourcarpet\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"contourcarpet\"}],\"contour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"contour\"}],\"heatmapgl\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmapgl\"}],\"heatmap\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmap\"}],\"histogram2dcontour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2dcontour\"}],\"histogram2d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2d\"}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"mesh3d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"mesh3d\"}],\"parcoords\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"parcoords\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}],\"scatter3d\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatter3d\"}],\"scattercarpet\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattercarpet\"}],\"scattergeo\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergeo\"}],\"scattergl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergl\"}],\"scattermapbox\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattermapbox\"}],\"scatterpolargl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolargl\"}],\"scatterpolar\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolar\"}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"scatterternary\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterternary\"}],\"surface\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"surface\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}]},\"layout\":{\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"autotypenumbers\":\"strict\",\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]],\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]},\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"geo\":{\"bgcolor\":\"white\",\"lakecolor\":\"white\",\"landcolor\":\"white\",\"showlakes\":true,\"showland\":true,\"subunitcolor\":\"#C8D4E3\"},\"hoverlabel\":{\"align\":\"left\"},\"hovermode\":\"closest\",\"mapbox\":{\"style\":\"light\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"white\",\"polar\":{\"angularaxis\":{\"gridcolor\":\"#EBF0F8\",\"linecolor\":\"#EBF0F8\",\"ticks\":\"\"},\"bgcolor\":\"white\",\"radialaxis\":{\"gridcolor\":\"#EBF0F8\",\"linecolor\":\"#EBF0F8\",\"ticks\":\"\"}},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"white\",\"gridcolor\":\"#DFE8F3\",\"gridwidth\":2,\"linecolor\":\"#EBF0F8\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"#EBF0F8\"},\"yaxis\":{\"backgroundcolor\":\"white\",\"gridcolor\":\"#DFE8F3\",\"gridwidth\":2,\"linecolor\":\"#EBF0F8\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"#EBF0F8\"},\"zaxis\":{\"backgroundcolor\":\"white\",\"gridcolor\":\"#DFE8F3\",\"gridwidth\":2,\"linecolor\":\"#EBF0F8\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"#EBF0F8\"}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"ternary\":{\"aaxis\":{\"gridcolor\":\"#DFE8F3\",\"linecolor\":\"#A2B1C6\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"#DFE8F3\",\"linecolor\":\"#A2B1C6\",\"ticks\":\"\"},\"bgcolor\":\"white\",\"caxis\":{\"gridcolor\":\"#DFE8F3\",\"linecolor\":\"#A2B1C6\",\"ticks\":\"\"}},\"title\":{\"x\":0.05},\"xaxis\":{\"automargin\":true,\"gridcolor\":\"#EBF0F8\",\"linecolor\":\"#EBF0F8\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"#EBF0F8\",\"zerolinewidth\":2},\"yaxis\":{\"automargin\":true,\"gridcolor\":\"#EBF0F8\",\"linecolor\":\"#EBF0F8\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"#EBF0F8\",\"zerolinewidth\":2}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"Recall\"},\"constrain\":\"domain\"},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"Precision\"},\"scaleanchor\":\"x\",\"scaleratio\":1},\"legend\":{\"tracegroupgap\":0},\"title\":{\"text\":\"Precision-Recall Curve (AUC=0.9200)\"},\"height\":600,\"width\":800,\"shapes\":[{\"line\":{\"dash\":\"dash\"},\"type\":\"line\",\"x0\":0,\"x1\":1,\"y0\":1,\"y1\":0}],\"showlegend\":false},                        {\"responsive\": true}                    ).then(function(){\n","                            \n","var gd = document.getElementById('ba7dfd8c-dddf-4e78-b45a-122a7a763c4c');\n","var x = new MutationObserver(function (mutations, observer) {{\n","        var display = window.getComputedStyle(gd).display;\n","        if (!display || display === 'none') {{\n","            console.log([gd, 'removed!']);\n","            Plotly.purge(gd);\n","            observer.disconnect();\n","        }}\n","}});\n","\n","// Listen for the removal of the full notebook cells\n","var notebookContainer = gd.closest('#notebook-container');\n","if (notebookContainer) {{\n","    x.observe(notebookContainer, {childList: true});\n","}}\n","\n","// Listen for the clearing of the current output cell\n","var outputEl = gd.closest('.output');\n","if (outputEl) {{\n","    x.observe(outputEl, {childList: true});\n","}}\n","\n","                        })                };                });            </script>        </div>"]},"metadata":{},"output_type":"display_data"}],"source":["target_score = lgb_clf.predict_proba(features_valid)[:, 1]\n","\n","fpr, tpr, thresholds = roc_curve(target_valid, target_score)\n","\n","fig = px.area(\n","    x=fpr, y=tpr,\n","    title=f'ROC Curve (AUC={auc(fpr, tpr):.4f})',\n","    labels=dict(x='False Positive Rate', y='True Positive Rate'),\n","    width=800, height=600\n",")\n","fig.add_shape(\n","    type='line', line=dict(dash='dash'),\n","    x0=0, x1=1, y0=0, y1=1\n",")\n","\n","fig.update_yaxes(scaleanchor=\"x\", scaleratio=1)\n","fig.update_xaxes(constrain='domain')\n","fig.update_layout(showlegend=False)\n","fig.show()\n","\n","precision, recall, thresholds = precision_recall_curve(target_valid, target_score)\n","\n","fig = px.area(\n","    x=recall, y=precision,\n","    title=f'Precision-Recall Curve (AUC={auc(fpr, tpr):.4f})',\n","    labels=dict(x='Recall', y='Precision'),\n","    width=800, height=600\n",")\n","fig.add_shape(\n","    type='line', line=dict(dash='dash'),\n","    x0=0, x1=1, y0=1, y1=0\n",")\n","fig.update_yaxes(scaleanchor=\"x\", scaleratio=1)\n","fig.update_xaxes(constrain='domain')\n","fig.update_layout(showlegend=False)\n","fig.show()"]},{"cell_type":"markdown","metadata":{},"source":["--------"]},{"cell_type":"markdown","metadata":{},"source":["# XGBoost"]},{"cell_type":"code","execution_count":133,"metadata":{"execution":{"iopub.execute_input":"2023-11-30T16:54:34.856327Z","iopub.status.busy":"2023-11-30T16:54:34.855760Z","iopub.status.idle":"2023-11-30T17:05:01.539192Z","shell.execute_reply":"2023-11-30T17:05:01.538397Z","shell.execute_reply.started":"2023-11-30T16:54:34.856295Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Runtime:\n","CPU times: user 10.6 s, sys: 2.16 s, total: 12.7 s\n","Wall time: 1min 24s\n"]}],"source":["%%time\n","\n","xgb_model = xgb.XGBClassifier(random_state=random_state) # fix encoding if issues arise\n","\n","xgb_params = {\n","#'min_child_weight': [1, 5, 10],\n","#'gamma': [0.5, 1, 1.5, 2, 5],\n","#'subsample': [0.6, 0.8, 1.0],\n","#'colsample_bytree': [0.6, 0.8, 1.0],\n","'booster': ['dart', 'gblinear', 'gbtree'],\n","#'max_leaves': [0,6,12,16,25,35],\n","'learning_rate': [0.01, 0.05, 0.08, 0.1, 0.15, 0.2],\n","#'max_depth': [1,,6,8,12,15,18,20],\n","'eval_metric': ['auc'],\n","}\n","\n","xgb_clf = RandomizedSearchCV(xgb_model, xgb_params, scoring='roc_auc', n_jobs=-1, cv=cv)\n","xgb_clf.fit(features_train, target_train)\n","# create a variable for the best model\n","best_xgb = xgb_clf.best_estimator_\n","xgb_pred = best_xgb.predict(features_valid)\n","print('Runtime:')"]},{"cell_type":"code","execution_count":134,"metadata":{"execution":{"iopub.execute_input":"2023-11-30T17:05:01.545140Z","iopub.status.busy":"2023-11-30T17:05:01.543025Z","iopub.status.idle":"2023-11-30T17:05:07.446876Z","shell.execute_reply":"2023-11-30T17:05:07.446006Z","shell.execute_reply.started":"2023-11-30T17:05:01.545108Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Cross Validation Scores: [0.92986224 0.91388249 0.92603687 0.87318548 0.93358295 0.90967742\n"," 0.90014401 0.89451325 0.90941401 0.90311489 0.88544262 0.88245968\n"," 0.92200461 0.8890841  0.93078917 0.91131912 0.93355415 0.89343318\n"," 0.9351884  0.9088939  0.89475041 0.92108295 0.9141129  0.91382488\n"," 0.89634217 0.93257488 0.92623848 0.90460829 0.91146556 0.88060564\n"," 0.91150442 0.89634217 0.90573157 0.93508065 0.92088134 0.92635369\n"," 0.90869816 0.90216014 0.88869626 0.90242141 0.92808661 0.92684332\n"," 0.88461982 0.88338134 0.91131912 0.8750576  0.91926843 0.92413594\n"," 0.8925393  0.91525081]\n"]}],"source":["xgb_scores = cross_val_score(xgb_model, features_train, target_train, cv=cv, scoring='roc_auc')\n","print('Cross Validation Scores: {}'.format(xgb_scores))"]},{"cell_type":"code","execution_count":135,"metadata":{"execution":{"iopub.execute_input":"2023-11-30T17:05:07.453675Z","iopub.status.busy":"2023-11-30T17:05:07.451234Z","iopub.status.idle":"2023-11-30T17:05:07.470522Z","shell.execute_reply":"2023-11-30T17:05:07.469855Z","shell.execute_reply.started":"2023-11-30T17:05:07.453628Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Best hyperparameters: {'learning_rate': 0.2, 'eval_metric': 'auc', 'booster': 'dart'}\n","\n","Best score: 0.910742213150237\n","\n","Average Cross Validation Score: 0.908791215308284\n","\n","ROC AUC Score - Validation Dataset: 0.9211217004622846\n"]}],"source":["# summary\n","print('Best hyperparameters:',  xgb_clf.best_params_)\n","print()\n","print('Best score:',  xgb_clf.best_score_)\n","print()\n","print('Average Cross Validation Score: {}'.format(xgb_scores.mean()))\n","print()\n","print('ROC AUC Score - Validation Dataset:',  roc_auc_score(target_valid, xgb_clf.predict_proba(features_valid)[:, 1]))"]},{"cell_type":"markdown","metadata":{},"source":["# ROC AUC Curve - XGBoost"]},{"cell_type":"code","execution_count":136,"metadata":{"execution":{"iopub.execute_input":"2023-11-30T17:05:07.476704Z","iopub.status.busy":"2023-11-30T17:05:07.474433Z","iopub.status.idle":"2023-11-30T17:05:07.619121Z","shell.execute_reply":"2023-11-30T17:05:07.618202Z","shell.execute_reply.started":"2023-11-30T17:05:07.476645Z"},"trusted":true},"outputs":[{"data":{"application/vnd.plotly.v1+json":{"config":{"plotlyServerURL":"https://plot.ly"},"data":[{"fillpattern":{"shape":""},"hovertemplate":"False Positive Rate=%{x}<br>True Positive Rate=%{y}<extra></extra>","legendgroup":"","line":{"color":"#636efa"},"marker":{"symbol":"circle"},"mode":"lines","name":"","orientation":"v","showlegend":false,"stackgroup":"1","type":"scatter","x":[0,0,0,0.00267379679144385,0.00267379679144385,0.00267379679144385,0.00267379679144385,0.0053475935828877,0.0053475935828877,0.008021390374331552,0.008021390374331552,0.013368983957219251,0.013368983957219251,0.013368983957219251,0.013368983957219251,0.016042780748663103,0.016042780748663103,0.016042780748663103,0.016042780748663103,0.01871657754010695,0.01871657754010695,0.01871657754010695,0.01871657754010695,0.0213903743315508,0.0213903743315508,0.02406417112299465,0.02406417112299465,0.02406417112299465,0.02406417112299465,0.02406417112299465,0.02406417112299465,0.026737967914438502,0.026737967914438502,0.029411764705882353,0.029411764705882353,0.03208556149732621,0.03208556149732621,0.034759358288770054,0.034759358288770054,0.0374331550802139,0.0374331550802139,0.040106951871657755,0.040106951871657755,0.0427807486631016,0.0427807486631016,0.045454545454545456,0.045454545454545456,0.0481283422459893,0.0481283422459893,0.05080213903743316,0.05080213903743316,0.053475935828877004,0.053475935828877004,0.05614973262032086,0.05614973262032086,0.058823529411764705,0.058823529411764705,0.06149732620320856,0.06149732620320856,0.06417112299465241,0.06417112299465241,0.06684491978609626,0.06684491978609626,0.06951871657754011,0.06951871657754011,0.07219251336898395,0.07219251336898395,0.0748663101604278,0.0748663101604278,0.07754010695187166,0.07754010695187166,0.08021390374331551,0.08021390374331551,0.08288770053475936,0.08288770053475936,0.0855614973262032,0.0855614973262032,0.08823529411764706,0.08823529411764706,0.09090909090909091,0.09090909090909091,0.0962566844919786,0.0962566844919786,0.09893048128342247,0.09893048128342247,0.10160427807486631,0.10160427807486631,0.10695187165775401,0.10695187165775401,0.10962566844919786,0.10962566844919786,0.11229946524064172,0.11229946524064172,0.11497326203208556,0.11497326203208556,0.11764705882352941,0.11764705882352941,0.12032085561497326,0.12032085561497326,0.12566844919786097,0.12566844919786097,0.12834224598930483,0.12834224598930483,0.13101604278074866,0.13101604278074866,0.13368983957219252,0.13368983957219252,0.13636363636363635,0.13636363636363635,0.13903743315508021,0.13903743315508021,0.14171122994652408,0.14171122994652408,0.1443850267379679,0.1443850267379679,0.14705882352941177,0.14705882352941177,0.1497326203208556,0.1497326203208556,0.15240641711229946,0.15240641711229946,0.15508021390374332,0.15508021390374332,0.15775401069518716,0.15775401069518716,0.16042780748663102,0.16042780748663102,0.16310160427807488,0.16310160427807488,0.1657754010695187,0.1657754010695187,0.1711229946524064,0.1711229946524064,0.17379679144385027,0.17379679144385027,0.18449197860962566,0.18449197860962566,0.18983957219251338,0.18983957219251338,0.1925133689839572,0.1925133689839572,0.19786096256684493,0.19786096256684493,0.20053475935828877,0.20053475935828877,0.20320855614973263,0.20320855614973263,0.20588235294117646,0.20588235294117646,0.21122994652406418,0.21122994652406418,0.21390374331550802,0.21390374331550802,0.21657754010695188,0.21657754010695188,0.2192513368983957,0.2192513368983957,0.22192513368983957,0.22192513368983957,0.22459893048128343,0.22459893048128343,0.22727272727272727,0.22727272727272727,0.22994652406417113,0.22994652406417113,0.23796791443850268,0.23796791443850268,0.24064171122994651,0.24064171122994651,0.24331550802139038,0.24331550802139038,0.24598930481283424,0.24598930481283424,0.24866310160427807,0.24866310160427807,0.2540106951871658,0.2540106951871658,0.2620320855614973,0.2620320855614973,0.2647058823529412,0.2647058823529412,0.27540106951871657,0.27540106951871657,0.27807486631016043,0.27807486631016043,0.2807486631016043,0.2807486631016043,0.28342245989304815,0.28342245989304815,0.28609625668449196,0.28609625668449196,0.29411764705882354,0.29411764705882354,0.30213903743315507,0.30213903743315507,0.3155080213903743,0.3155080213903743,0.32085561497326204,0.32085561497326204,0.3235294117647059,0.3235294117647059,0.339572192513369,0.339572192513369,0.3422459893048128,0.3422459893048128,0.34759358288770054,0.34759358288770054,0.3502673796791444,0.3502673796791444,0.36363636363636365,0.36363636363636365,0.3663101604278075,0.3663101604278075,0.37967914438502676,0.37967914438502676,0.3850267379679144,0.3850267379679144,0.3877005347593583,0.3877005347593583,0.39037433155080214,0.39037433155080214,0.393048128342246,0.393048128342246,0.39572192513368987,0.39572192513368987,0.40106951871657753,0.40106951871657753,0.40641711229946526,0.40641711229946526,0.42780748663101603,0.42780748663101603,0.4304812834224599,0.4304812834224599,0.44385026737967914,0.44385026737967914,0.45454545454545453,0.45454545454545453,0.4893048128342246,0.4893048128342246,0.5026737967914439,0.5026737967914439,0.5080213903743316,0.5080213903743316,0.5294117647058824,0.5347593582887701,0.5588235294117647,0.5588235294117647,1],"xaxis":"x","y":[0,0.000968054211035818,0.015488867376573089,0.015488867376573089,0.027105517909002903,0.02904162633107454,0.031945788964181994,0.031945788964181994,0.044530493707647625,0.044530493707647625,0.05517909002904162,0.05517909002904162,0.08615682478218781,0.08809293320425944,0.14133591481122942,0.14133591481122942,0.2042594385285576,0.20619554695062922,0.22749273959341723,0.22749273959341723,0.27783155856727976,0.2797676669893514,0.2971926427879961,0.2971926427879961,0.3388189738625363,0.3388189738625363,0.38528557599225555,0.3872216844143272,0.38818973862536305,0.39012584704743464,0.3978702807357212,0.3978702807357212,0.42400774443368827,0.42400774443368827,0.4453049370764763,0.4453049370764763,0.462729912875121,0.462729912875121,0.46950629235237173,0.46950629235237173,0.48596321393998065,0.48596321393998065,0.5014520813165537,0.5014520813165537,0.5091965150048403,0.5091965150048403,0.5450145208131656,0.5450145208131656,0.547918683446273,0.547918683446273,0.5682478218780251,0.5682478218780251,0.5750242013552759,0.5750242013552759,0.5808325266214908,0.5808325266214908,0.643756050338819,0.643756050338819,0.6563407550822846,0.6563407550822846,0.665053242981607,0.665053242981607,0.6766698935140368,0.6766698935140368,0.6795740561471443,0.6795740561471443,0.68054211035818,0.68054211035818,0.6844143272023233,0.6844143272023233,0.6892545982575025,0.6892545982575025,0.6931268151016456,0.6931268151016456,0.6940948693126815,0.6940948693126815,0.7057115198451114,0.7057115198451114,0.707647628267183,0.707647628267183,0.7144240077444337,0.7144240077444337,0.7192642787996127,0.7192642787996127,0.7337850919651501,0.7337850919651501,0.739593417231365,0.739593417231365,0.7473378509196515,0.7473378509196515,0.7512100677637947,0.7512100677637947,0.755082284607938,0.755082284607938,0.7608906098741529,0.7608906098741529,0.7783155856727977,0.7783155856727977,0.7889641819941917,0.7889641819941917,0.7947725072604066,0.7947725072604066,0.7996127783155856,0.7996127783155856,0.8073572120038722,0.8073572120038722,0.8092933204259438,0.8092933204259438,0.814133591481123,0.814133591481123,0.8151016456921588,0.8151016456921588,0.8160696999031946,0.8160696999031946,0.8354307841239109,0.8354307841239109,0.8363988383349468,0.8363988383349468,0.8383349467570184,0.8383349467570184,0.8402710551790901,0.8402710551790901,0.8412391093901258,0.8412391093901258,0.8480154888673765,0.8480154888673765,0.856727976766699,0.856727976766699,0.8586640851887706,0.8586640851887706,0.8664085188770572,0.8664085188770572,0.8673765730880929,0.8673765730880929,0.8731848983543078,0.8731848983543078,0.8789932236205228,0.8789932236205228,0.8818973862536302,0.8818973862536302,0.8838334946757018,0.8838334946757018,0.8848015488867377,0.8848015488867377,0.8857696030977735,0.8857696030977735,0.8867376573088093,0.8867376573088093,0.8906098741529526,0.8906098741529526,0.8925459825750242,0.8925459825750242,0.8944820909970959,0.8944820909970959,0.8973862536302033,0.8973862536302033,0.8983543078412392,0.8983543078412392,0.8993223620522749,0.8993223620522749,0.9070667957405615,0.9070667957405615,0.9119070667957405,0.9119070667957405,0.914811229428848,0.914811229428848,0.9177153920619555,0.9177153920619555,0.9186834462729913,0.9186834462729913,0.9244917715392061,0.9244917715392061,0.925459825750242,0.925459825750242,0.9370764762826719,0.9370764762826719,0.9380445304937076,0.9380445304937076,0.9419167473378509,0.9419167473378509,0.9438528557599225,0.9438528557599225,0.9448209099709584,0.9448209099709584,0.9515972894482091,0.9515972894482091,0.9535333978702807,0.9535333978702807,0.957405614714424,0.957405614714424,0.9593417231364957,0.9593417231364957,0.9622458857696031,0.9622458857696031,0.9641819941916747,0.9641819941916747,0.9690222652468539,0.9690222652468539,0.9699903194578896,0.9699903194578896,0.9709583736689255,0.9709583736689255,0.9719264278799613,0.9719264278799613,0.972894482090997,0.972894482090997,0.9738625363020329,0.9738625363020329,0.9748305905130688,0.9748305905130688,0.9757986447241046,0.9757986447241046,0.9767666989351403,0.9767666989351403,0.9777347531461762,0.9777347531461762,0.978702807357212,0.978702807357212,0.9806389157792836,0.9806389157792836,0.9835430784123911,0.9835430784123911,0.9854791868344628,0.9854791868344628,0.989351403678606,0.989351403678606,0.9903194578896418,0.9903194578896418,0.9912875121006777,0.9912875121006777,0.9932236205227493,0.9932236205227493,0.9941916747337851,0.9941916747337851,0.995159728944821,0.995159728944821,0.9961277831558567,0.9961277831558567,0.9970958373668926,0.9970958373668926,0.9980638915779284,0.9980638915779284,0.9990319457889641,0.9990319457889641,0.9990319457889641,0.9990319457889641,1,1],"yaxis":"y"}],"layout":{"height":600,"legend":{"tracegroupgap":0},"shapes":[{"line":{"dash":"dash"},"type":"line","x0":0,"x1":1,"y0":0,"y1":1}],"showlegend":false,"template":{"data":{"bar":[{"error_x":{"color":"#2a3f5f"},"error_y":{"color":"#2a3f5f"},"marker":{"line":{"color":"white","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"barpolar":[{"marker":{"line":{"color":"white","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"carpet":[{"aaxis":{"endlinecolor":"#2a3f5f","gridcolor":"#C8D4E3","linecolor":"#C8D4E3","minorgridcolor":"#C8D4E3","startlinecolor":"#2a3f5f"},"baxis":{"endlinecolor":"#2a3f5f","gridcolor":"#C8D4E3","linecolor":"#C8D4E3","minorgridcolor":"#C8D4E3","startlinecolor":"#2a3f5f"},"type":"carpet"}],"choropleth":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"choropleth"}],"contour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"contour"}],"contourcarpet":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"contourcarpet"}],"heatmap":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmap"}],"heatmapgl":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmapgl"}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"histogram2d":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2d"}],"histogram2dcontour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2dcontour"}],"mesh3d":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"mesh3d"}],"parcoords":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"parcoords"}],"pie":[{"automargin":true,"type":"pie"}],"scatter":[{"fillpattern":{"fillmode":"overlay","size":10,"solidity":0.2},"type":"scatter"}],"scatter3d":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter3d"}],"scattercarpet":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattercarpet"}],"scattergeo":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergeo"}],"scattergl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergl"}],"scattermapbox":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattermapbox"}],"scatterpolar":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolar"}],"scatterpolargl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolargl"}],"scatterternary":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterternary"}],"surface":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"surface"}],"table":[{"cells":{"fill":{"color":"#EBF0F8"},"line":{"color":"white"}},"header":{"fill":{"color":"#C8D4E3"},"line":{"color":"white"}},"type":"table"}]},"layout":{"annotationdefaults":{"arrowcolor":"#2a3f5f","arrowhead":0,"arrowwidth":1},"autotypenumbers":"strict","coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]],"sequential":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"sequentialminus":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]]},"colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#2a3f5f"},"geo":{"bgcolor":"white","lakecolor":"white","landcolor":"white","showlakes":true,"showland":true,"subunitcolor":"#C8D4E3"},"hoverlabel":{"align":"left"},"hovermode":"closest","mapbox":{"style":"light"},"paper_bgcolor":"white","plot_bgcolor":"white","polar":{"angularaxis":{"gridcolor":"#EBF0F8","linecolor":"#EBF0F8","ticks":""},"bgcolor":"white","radialaxis":{"gridcolor":"#EBF0F8","linecolor":"#EBF0F8","ticks":""}},"scene":{"xaxis":{"backgroundcolor":"white","gridcolor":"#DFE8F3","gridwidth":2,"linecolor":"#EBF0F8","showbackground":true,"ticks":"","zerolinecolor":"#EBF0F8"},"yaxis":{"backgroundcolor":"white","gridcolor":"#DFE8F3","gridwidth":2,"linecolor":"#EBF0F8","showbackground":true,"ticks":"","zerolinecolor":"#EBF0F8"},"zaxis":{"backgroundcolor":"white","gridcolor":"#DFE8F3","gridwidth":2,"linecolor":"#EBF0F8","showbackground":true,"ticks":"","zerolinecolor":"#EBF0F8"}},"shapedefaults":{"line":{"color":"#2a3f5f"}},"ternary":{"aaxis":{"gridcolor":"#DFE8F3","linecolor":"#A2B1C6","ticks":""},"baxis":{"gridcolor":"#DFE8F3","linecolor":"#A2B1C6","ticks":""},"bgcolor":"white","caxis":{"gridcolor":"#DFE8F3","linecolor":"#A2B1C6","ticks":""}},"title":{"x":0.05},"xaxis":{"automargin":true,"gridcolor":"#EBF0F8","linecolor":"#EBF0F8","ticks":"","title":{"standoff":15},"zerolinecolor":"#EBF0F8","zerolinewidth":2},"yaxis":{"automargin":true,"gridcolor":"#EBF0F8","linecolor":"#EBF0F8","ticks":"","title":{"standoff":15},"zerolinecolor":"#EBF0F8","zerolinewidth":2}}},"title":{"text":"ROC Curve (AUC=0.9211)"},"width":800,"xaxis":{"anchor":"y","constrain":"domain","domain":[0,1],"title":{"text":"False Positive Rate"}},"yaxis":{"anchor":"x","domain":[0,1],"scaleanchor":"x","scaleratio":1,"title":{"text":"True Positive Rate"}}}},"text/html":["<div>                            <div id=\"5bbf7607-405c-4204-94bb-42c93c995aaf\" class=\"plotly-graph-div\" style=\"height:600px; width:800px;\"></div>            <script type=\"text/javascript\">                require([\"plotly\"], function(Plotly) {                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"5bbf7607-405c-4204-94bb-42c93c995aaf\")) {                    Plotly.newPlot(                        \"5bbf7607-405c-4204-94bb-42c93c995aaf\",                        [{\"fillpattern\":{\"shape\":\"\"},\"hovertemplate\":\"False Positive Rate=%{x}\\u003cbr\\u003eTrue Positive Rate=%{y}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"\",\"line\":{\"color\":\"#636efa\"},\"marker\":{\"symbol\":\"circle\"},\"mode\":\"lines\",\"name\":\"\",\"orientation\":\"v\",\"showlegend\":false,\"stackgroup\":\"1\",\"x\":[0.0,0.0,0.0,0.00267379679144385,0.00267379679144385,0.00267379679144385,0.00267379679144385,0.0053475935828877,0.0053475935828877,0.008021390374331552,0.008021390374331552,0.013368983957219251,0.013368983957219251,0.013368983957219251,0.013368983957219251,0.016042780748663103,0.016042780748663103,0.016042780748663103,0.016042780748663103,0.01871657754010695,0.01871657754010695,0.01871657754010695,0.01871657754010695,0.0213903743315508,0.0213903743315508,0.02406417112299465,0.02406417112299465,0.02406417112299465,0.02406417112299465,0.02406417112299465,0.02406417112299465,0.026737967914438502,0.026737967914438502,0.029411764705882353,0.029411764705882353,0.03208556149732621,0.03208556149732621,0.034759358288770054,0.034759358288770054,0.0374331550802139,0.0374331550802139,0.040106951871657755,0.040106951871657755,0.0427807486631016,0.0427807486631016,0.045454545454545456,0.045454545454545456,0.0481283422459893,0.0481283422459893,0.05080213903743316,0.05080213903743316,0.053475935828877004,0.053475935828877004,0.05614973262032086,0.05614973262032086,0.058823529411764705,0.058823529411764705,0.06149732620320856,0.06149732620320856,0.06417112299465241,0.06417112299465241,0.06684491978609626,0.06684491978609626,0.06951871657754011,0.06951871657754011,0.07219251336898395,0.07219251336898395,0.0748663101604278,0.0748663101604278,0.07754010695187166,0.07754010695187166,0.08021390374331551,0.08021390374331551,0.08288770053475936,0.08288770053475936,0.0855614973262032,0.0855614973262032,0.08823529411764706,0.08823529411764706,0.09090909090909091,0.09090909090909091,0.0962566844919786,0.0962566844919786,0.09893048128342247,0.09893048128342247,0.10160427807486631,0.10160427807486631,0.10695187165775401,0.10695187165775401,0.10962566844919786,0.10962566844919786,0.11229946524064172,0.11229946524064172,0.11497326203208556,0.11497326203208556,0.11764705882352941,0.11764705882352941,0.12032085561497326,0.12032085561497326,0.12566844919786097,0.12566844919786097,0.12834224598930483,0.12834224598930483,0.13101604278074866,0.13101604278074866,0.13368983957219252,0.13368983957219252,0.13636363636363635,0.13636363636363635,0.13903743315508021,0.13903743315508021,0.14171122994652408,0.14171122994652408,0.1443850267379679,0.1443850267379679,0.14705882352941177,0.14705882352941177,0.1497326203208556,0.1497326203208556,0.15240641711229946,0.15240641711229946,0.15508021390374332,0.15508021390374332,0.15775401069518716,0.15775401069518716,0.16042780748663102,0.16042780748663102,0.16310160427807488,0.16310160427807488,0.1657754010695187,0.1657754010695187,0.1711229946524064,0.1711229946524064,0.17379679144385027,0.17379679144385027,0.18449197860962566,0.18449197860962566,0.18983957219251338,0.18983957219251338,0.1925133689839572,0.1925133689839572,0.19786096256684493,0.19786096256684493,0.20053475935828877,0.20053475935828877,0.20320855614973263,0.20320855614973263,0.20588235294117646,0.20588235294117646,0.21122994652406418,0.21122994652406418,0.21390374331550802,0.21390374331550802,0.21657754010695188,0.21657754010695188,0.2192513368983957,0.2192513368983957,0.22192513368983957,0.22192513368983957,0.22459893048128343,0.22459893048128343,0.22727272727272727,0.22727272727272727,0.22994652406417113,0.22994652406417113,0.23796791443850268,0.23796791443850268,0.24064171122994651,0.24064171122994651,0.24331550802139038,0.24331550802139038,0.24598930481283424,0.24598930481283424,0.24866310160427807,0.24866310160427807,0.2540106951871658,0.2540106951871658,0.2620320855614973,0.2620320855614973,0.2647058823529412,0.2647058823529412,0.27540106951871657,0.27540106951871657,0.27807486631016043,0.27807486631016043,0.2807486631016043,0.2807486631016043,0.28342245989304815,0.28342245989304815,0.28609625668449196,0.28609625668449196,0.29411764705882354,0.29411764705882354,0.30213903743315507,0.30213903743315507,0.3155080213903743,0.3155080213903743,0.32085561497326204,0.32085561497326204,0.3235294117647059,0.3235294117647059,0.339572192513369,0.339572192513369,0.3422459893048128,0.3422459893048128,0.34759358288770054,0.34759358288770054,0.3502673796791444,0.3502673796791444,0.36363636363636365,0.36363636363636365,0.3663101604278075,0.3663101604278075,0.37967914438502676,0.37967914438502676,0.3850267379679144,0.3850267379679144,0.3877005347593583,0.3877005347593583,0.39037433155080214,0.39037433155080214,0.393048128342246,0.393048128342246,0.39572192513368987,0.39572192513368987,0.40106951871657753,0.40106951871657753,0.40641711229946526,0.40641711229946526,0.42780748663101603,0.42780748663101603,0.4304812834224599,0.4304812834224599,0.44385026737967914,0.44385026737967914,0.45454545454545453,0.45454545454545453,0.4893048128342246,0.4893048128342246,0.5026737967914439,0.5026737967914439,0.5080213903743316,0.5080213903743316,0.5294117647058824,0.5347593582887701,0.5588235294117647,0.5588235294117647,1.0],\"xaxis\":\"x\",\"y\":[0.0,0.000968054211035818,0.015488867376573089,0.015488867376573089,0.027105517909002903,0.02904162633107454,0.031945788964181994,0.031945788964181994,0.044530493707647625,0.044530493707647625,0.05517909002904162,0.05517909002904162,0.08615682478218781,0.08809293320425944,0.14133591481122942,0.14133591481122942,0.2042594385285576,0.20619554695062922,0.22749273959341723,0.22749273959341723,0.27783155856727976,0.2797676669893514,0.2971926427879961,0.2971926427879961,0.3388189738625363,0.3388189738625363,0.38528557599225555,0.3872216844143272,0.38818973862536305,0.39012584704743464,0.3978702807357212,0.3978702807357212,0.42400774443368827,0.42400774443368827,0.4453049370764763,0.4453049370764763,0.462729912875121,0.462729912875121,0.46950629235237173,0.46950629235237173,0.48596321393998065,0.48596321393998065,0.5014520813165537,0.5014520813165537,0.5091965150048403,0.5091965150048403,0.5450145208131656,0.5450145208131656,0.547918683446273,0.547918683446273,0.5682478218780251,0.5682478218780251,0.5750242013552759,0.5750242013552759,0.5808325266214908,0.5808325266214908,0.643756050338819,0.643756050338819,0.6563407550822846,0.6563407550822846,0.665053242981607,0.665053242981607,0.6766698935140368,0.6766698935140368,0.6795740561471443,0.6795740561471443,0.68054211035818,0.68054211035818,0.6844143272023233,0.6844143272023233,0.6892545982575025,0.6892545982575025,0.6931268151016456,0.6931268151016456,0.6940948693126815,0.6940948693126815,0.7057115198451114,0.7057115198451114,0.707647628267183,0.707647628267183,0.7144240077444337,0.7144240077444337,0.7192642787996127,0.7192642787996127,0.7337850919651501,0.7337850919651501,0.739593417231365,0.739593417231365,0.7473378509196515,0.7473378509196515,0.7512100677637947,0.7512100677637947,0.755082284607938,0.755082284607938,0.7608906098741529,0.7608906098741529,0.7783155856727977,0.7783155856727977,0.7889641819941917,0.7889641819941917,0.7947725072604066,0.7947725072604066,0.7996127783155856,0.7996127783155856,0.8073572120038722,0.8073572120038722,0.8092933204259438,0.8092933204259438,0.814133591481123,0.814133591481123,0.8151016456921588,0.8151016456921588,0.8160696999031946,0.8160696999031946,0.8354307841239109,0.8354307841239109,0.8363988383349468,0.8363988383349468,0.8383349467570184,0.8383349467570184,0.8402710551790901,0.8402710551790901,0.8412391093901258,0.8412391093901258,0.8480154888673765,0.8480154888673765,0.856727976766699,0.856727976766699,0.8586640851887706,0.8586640851887706,0.8664085188770572,0.8664085188770572,0.8673765730880929,0.8673765730880929,0.8731848983543078,0.8731848983543078,0.8789932236205228,0.8789932236205228,0.8818973862536302,0.8818973862536302,0.8838334946757018,0.8838334946757018,0.8848015488867377,0.8848015488867377,0.8857696030977735,0.8857696030977735,0.8867376573088093,0.8867376573088093,0.8906098741529526,0.8906098741529526,0.8925459825750242,0.8925459825750242,0.8944820909970959,0.8944820909970959,0.8973862536302033,0.8973862536302033,0.8983543078412392,0.8983543078412392,0.8993223620522749,0.8993223620522749,0.9070667957405615,0.9070667957405615,0.9119070667957405,0.9119070667957405,0.914811229428848,0.914811229428848,0.9177153920619555,0.9177153920619555,0.9186834462729913,0.9186834462729913,0.9244917715392061,0.9244917715392061,0.925459825750242,0.925459825750242,0.9370764762826719,0.9370764762826719,0.9380445304937076,0.9380445304937076,0.9419167473378509,0.9419167473378509,0.9438528557599225,0.9438528557599225,0.9448209099709584,0.9448209099709584,0.9515972894482091,0.9515972894482091,0.9535333978702807,0.9535333978702807,0.957405614714424,0.957405614714424,0.9593417231364957,0.9593417231364957,0.9622458857696031,0.9622458857696031,0.9641819941916747,0.9641819941916747,0.9690222652468539,0.9690222652468539,0.9699903194578896,0.9699903194578896,0.9709583736689255,0.9709583736689255,0.9719264278799613,0.9719264278799613,0.972894482090997,0.972894482090997,0.9738625363020329,0.9738625363020329,0.9748305905130688,0.9748305905130688,0.9757986447241046,0.9757986447241046,0.9767666989351403,0.9767666989351403,0.9777347531461762,0.9777347531461762,0.978702807357212,0.978702807357212,0.9806389157792836,0.9806389157792836,0.9835430784123911,0.9835430784123911,0.9854791868344628,0.9854791868344628,0.989351403678606,0.989351403678606,0.9903194578896418,0.9903194578896418,0.9912875121006777,0.9912875121006777,0.9932236205227493,0.9932236205227493,0.9941916747337851,0.9941916747337851,0.995159728944821,0.995159728944821,0.9961277831558567,0.9961277831558567,0.9970958373668926,0.9970958373668926,0.9980638915779284,0.9980638915779284,0.9990319457889641,0.9990319457889641,0.9990319457889641,0.9990319457889641,1.0,1.0],\"yaxis\":\"y\",\"type\":\"scatter\"}],                        {\"template\":{\"data\":{\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"white\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"white\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"#C8D4E3\",\"linecolor\":\"#C8D4E3\",\"minorgridcolor\":\"#C8D4E3\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"#C8D4E3\",\"linecolor\":\"#C8D4E3\",\"minorgridcolor\":\"#C8D4E3\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"choropleth\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"choropleth\"}],\"contourcarpet\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"contourcarpet\"}],\"contour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"contour\"}],\"heatmapgl\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmapgl\"}],\"heatmap\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmap\"}],\"histogram2dcontour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2dcontour\"}],\"histogram2d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2d\"}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"mesh3d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"mesh3d\"}],\"parcoords\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"parcoords\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}],\"scatter3d\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatter3d\"}],\"scattercarpet\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattercarpet\"}],\"scattergeo\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergeo\"}],\"scattergl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergl\"}],\"scattermapbox\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattermapbox\"}],\"scatterpolargl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolargl\"}],\"scatterpolar\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolar\"}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"scatterternary\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterternary\"}],\"surface\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"surface\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}]},\"layout\":{\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"autotypenumbers\":\"strict\",\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]],\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]},\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"geo\":{\"bgcolor\":\"white\",\"lakecolor\":\"white\",\"landcolor\":\"white\",\"showlakes\":true,\"showland\":true,\"subunitcolor\":\"#C8D4E3\"},\"hoverlabel\":{\"align\":\"left\"},\"hovermode\":\"closest\",\"mapbox\":{\"style\":\"light\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"white\",\"polar\":{\"angularaxis\":{\"gridcolor\":\"#EBF0F8\",\"linecolor\":\"#EBF0F8\",\"ticks\":\"\"},\"bgcolor\":\"white\",\"radialaxis\":{\"gridcolor\":\"#EBF0F8\",\"linecolor\":\"#EBF0F8\",\"ticks\":\"\"}},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"white\",\"gridcolor\":\"#DFE8F3\",\"gridwidth\":2,\"linecolor\":\"#EBF0F8\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"#EBF0F8\"},\"yaxis\":{\"backgroundcolor\":\"white\",\"gridcolor\":\"#DFE8F3\",\"gridwidth\":2,\"linecolor\":\"#EBF0F8\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"#EBF0F8\"},\"zaxis\":{\"backgroundcolor\":\"white\",\"gridcolor\":\"#DFE8F3\",\"gridwidth\":2,\"linecolor\":\"#EBF0F8\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"#EBF0F8\"}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"ternary\":{\"aaxis\":{\"gridcolor\":\"#DFE8F3\",\"linecolor\":\"#A2B1C6\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"#DFE8F3\",\"linecolor\":\"#A2B1C6\",\"ticks\":\"\"},\"bgcolor\":\"white\",\"caxis\":{\"gridcolor\":\"#DFE8F3\",\"linecolor\":\"#A2B1C6\",\"ticks\":\"\"}},\"title\":{\"x\":0.05},\"xaxis\":{\"automargin\":true,\"gridcolor\":\"#EBF0F8\",\"linecolor\":\"#EBF0F8\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"#EBF0F8\",\"zerolinewidth\":2},\"yaxis\":{\"automargin\":true,\"gridcolor\":\"#EBF0F8\",\"linecolor\":\"#EBF0F8\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"#EBF0F8\",\"zerolinewidth\":2}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"False Positive Rate\"},\"constrain\":\"domain\"},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"True Positive Rate\"},\"scaleanchor\":\"x\",\"scaleratio\":1},\"legend\":{\"tracegroupgap\":0},\"title\":{\"text\":\"ROC Curve (AUC=0.9211)\"},\"height\":600,\"width\":800,\"shapes\":[{\"line\":{\"dash\":\"dash\"},\"type\":\"line\",\"x0\":0,\"x1\":1,\"y0\":0,\"y1\":1}],\"showlegend\":false},                        {\"responsive\": true}                    ).then(function(){\n","                            \n","var gd = document.getElementById('5bbf7607-405c-4204-94bb-42c93c995aaf');\n","var x = new MutationObserver(function (mutations, observer) {{\n","        var display = window.getComputedStyle(gd).display;\n","        if (!display || display === 'none') {{\n","            console.log([gd, 'removed!']);\n","            Plotly.purge(gd);\n","            observer.disconnect();\n","        }}\n","}});\n","\n","// Listen for the removal of the full notebook cells\n","var notebookContainer = gd.closest('#notebook-container');\n","if (notebookContainer) {{\n","    x.observe(notebookContainer, {childList: true});\n","}}\n","\n","// Listen for the clearing of the current output cell\n","var outputEl = gd.closest('.output');\n","if (outputEl) {{\n","    x.observe(outputEl, {childList: true});\n","}}\n","\n","                        })                };                });            </script>        </div>"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.plotly.v1+json":{"config":{"plotlyServerURL":"https://plot.ly"},"data":[{"fillpattern":{"shape":""},"hovertemplate":"Recall=%{x}<br>Precision=%{y}<extra></extra>","legendgroup":"","line":{"color":"#636efa"},"marker":{"symbol":"circle"},"mode":"lines","name":"","orientation":"v","showlegend":false,"stackgroup":"1","type":"scatter","x":[1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,0.9990319457889641,0.9990319457889641,0.9990319457889641,0.9990319457889641,0.9990319457889641,0.9990319457889641,0.9990319457889641,0.9990319457889641,0.9990319457889641,0.9990319457889641,0.9990319457889641,0.9990319457889641,0.9990319457889641,0.9990319457889641,0.9990319457889641,0.9990319457889641,0.9990319457889641,0.9990319457889641,0.9990319457889641,0.9980638915779284,0.9980638915779284,0.9980638915779284,0.9970958373668926,0.9970958373668926,0.9970958373668926,0.9970958373668926,0.9970958373668926,0.9970958373668926,0.9961277831558567,0.9961277831558567,0.9961277831558567,0.9961277831558567,0.9961277831558567,0.9961277831558567,0.9961277831558567,0.9961277831558567,0.9961277831558567,0.9961277831558567,0.9961277831558567,0.9961277831558567,0.9961277831558567,0.9961277831558567,0.995159728944821,0.995159728944821,0.995159728944821,0.995159728944821,0.995159728944821,0.9941916747337851,0.9941916747337851,0.9941916747337851,0.9941916747337851,0.9941916747337851,0.9941916747337851,0.9932236205227493,0.9932236205227493,0.9922555663117134,0.9912875121006777,0.9912875121006777,0.9912875121006777,0.9912875121006777,0.9912875121006777,0.9912875121006777,0.9912875121006777,0.9912875121006777,0.9912875121006777,0.9903194578896418,0.9903194578896418,0.9903194578896418,0.989351403678606,0.989351403678606,0.989351403678606,0.9883833494675702,0.9874152952565344,0.9864472410454985,0.9854791868344628,0.9854791868344628,0.9845111326234269,0.9835430784123911,0.9835430784123911,0.9825750242013552,0.9816069699903195,0.9806389157792836,0.9806389157792836,0.9796708615682478,0.978702807357212,0.978702807357212,0.9777347531461762,0.9777347531461762,0.9777347531461762,0.9767666989351403,0.9767666989351403,0.9767666989351403,0.9767666989351403,0.9767666989351403,0.9767666989351403,0.9757986447241046,0.9757986447241046,0.9748305905130688,0.9748305905130688,0.9748305905130688,0.9748305905130688,0.9748305905130688,0.9748305905130688,0.9738625363020329,0.9738625363020329,0.972894482090997,0.972894482090997,0.972894482090997,0.9719264278799613,0.9719264278799613,0.9709583736689255,0.9709583736689255,0.9709583736689255,0.9709583736689255,0.9709583736689255,0.9709583736689255,0.9709583736689255,0.9699903194578896,0.9699903194578896,0.9690222652468539,0.9690222652468539,0.9690222652468539,0.968054211035818,0.9670861568247822,0.9661181026137464,0.9651500484027106,0.9641819941916747,0.9641819941916747,0.9641819941916747,0.9641819941916747,0.9641819941916747,0.9641819941916747,0.9632139399806389,0.9622458857696031,0.9622458857696031,0.9622458857696031,0.9622458857696031,0.9612778315585673,0.9603097773475314,0.9593417231364957,0.9593417231364957,0.9593417231364957,0.9593417231364957,0.9583736689254598,0.957405614714424,0.957405614714424,0.9564375605033882,0.9554695062923524,0.9545014520813165,0.9535333978702807,0.9535333978702807,0.952565343659245,0.9515972894482091,0.9515972894482091,0.9506292352371732,0.9496611810261375,0.9486931268151017,0.9477250726040658,0.9467570183930301,0.9457889641819942,0.9448209099709584,0.9448209099709584,0.9438528557599225,0.9438528557599225,0.9438528557599225,0.9438528557599225,0.9438528557599225,0.9428848015488868,0.9419167473378509,0.9419167473378509,0.9409486931268151,0.9399806389157793,0.9390125847047435,0.9380445304937076,0.9380445304937076,0.9380445304937076,0.9380445304937076,0.9370764762826719,0.9370764762826719,0.9370764762826719,0.936108422071636,0.9351403678606002,0.9341723136495643,0.9332042594385286,0.9322362052274927,0.9312681510164569,0.9303000968054211,0.9293320425943853,0.9283639883833494,0.9273959341723137,0.9264278799612778,0.925459825750242,0.925459825750242,0.9244917715392061,0.9244917715392061,0.9235237173281704,0.9225556631171346,0.9215876089060987,0.920619554695063,0.9196515004840271,0.9186834462729913,0.9186834462729913,0.9177153920619555,0.9177153920619555,0.9167473378509197,0.9157792836398838,0.914811229428848,0.914811229428848,0.914811229428848,0.914811229428848,0.9138431752178122,0.9128751210067764,0.9119070667957405,0.9119070667957405,0.9109390125847048,0.9099709583736689,0.9090029041626331,0.9080348499515973,0.9070667957405615,0.9070667957405615,0.9060987415295256,0.9051306873184899,0.904162633107454,0.9031945788964182,0.9022265246853823,0.9012584704743466,0.9002904162633107,0.8993223620522749,0.8993223620522749,0.8983543078412392,0.8983543078412392,0.8973862536302033,0.8973862536302033,0.8964181994191674,0.8954501452081317,0.8944820909970959,0.8944820909970959,0.89351403678606,0.8925459825750242,0.8925459825750242,0.8915779283639884,0.8906098741529526,0.8906098741529526,0.8906098741529526,0.8896418199419167,0.888673765730881,0.8877057115198451,0.8867376573088093,0.8867376573088093,0.8857696030977735,0.8857696030977735,0.8848015488867377,0.8848015488867377,0.8838334946757018,0.8838334946757018,0.8838334946757018,0.882865440464666,0.8818973862536302,0.8818973862536302,0.8809293320425944,0.8799612778315585,0.8789932236205228,0.8789932236205228,0.8789932236205228,0.8780251694094869,0.8770571151984511,0.8760890609874153,0.8751210067763795,0.8741529525653436,0.8731848983543078,0.8731848983543078,0.8731848983543078,0.8731848983543078,0.8731848983543078,0.872216844143272,0.8712487899322362,0.8702807357212003,0.8693126815101646,0.8683446272991288,0.8673765730880929,0.8673765730880929,0.8664085188770572,0.8664085188770572,0.8664085188770572,0.8654404646660213,0.8644724104549855,0.8635043562439496,0.8625363020329139,0.861568247821878,0.8606001936108422,0.8596321393998064,0.8586640851887706,0.8586640851887706,0.8576960309777347,0.856727976766699,0.856727976766699,0.8557599225556631,0.8547918683446273,0.8538238141335914,0.8528557599225557,0.8518877057115198,0.850919651500484,0.8499515972894482,0.8489835430784124,0.8480154888673765,0.8480154888673765,0.8470474346563408,0.846079380445305,0.8451113262342691,0.8441432720232332,0.8431752178121975,0.8422071636011617,0.8412391093901258,0.8412391093901258,0.8402710551790901,0.8402710551790901,0.8393030009680542,0.8383349467570184,0.8383349467570184,0.8373668925459826,0.8363988383349468,0.8363988383349468,0.8354307841239109,0.8354307841239109,0.8344627299128751,0.8334946757018393,0.8325266214908035,0.8315585672797676,0.8305905130687319,0.829622458857696,0.8286544046466602,0.8276863504356244,0.8267182962245886,0.8257502420135527,0.8247821878025169,0.8238141335914811,0.8228460793804453,0.8218780251694094,0.8209099709583737,0.8199419167473379,0.818973862536302,0.8180058083252663,0.8170377541142304,0.8160696999031946,0.8160696999031946,0.8151016456921588,0.8151016456921588,0.814133591481123,0.814133591481123,0.8131655372700871,0.8121974830590513,0.8112294288480155,0.8102613746369797,0.8092933204259438,0.8092933204259438,0.8083252662149081,0.8073572120038722,0.8073572120038722,0.8063891577928364,0.8054211035818006,0.8044530493707648,0.8034849951597289,0.8025169409486931,0.8015488867376573,0.8005808325266215,0.7996127783155856,0.7996127783155856,0.7986447241045499,0.797676669893514,0.7967086156824782,0.7957405614714425,0.7947725072604066,0.7947725072604066,0.7938044530493708,0.7928363988383349,0.7918683446272992,0.7909002904162633,0.7899322362052275,0.7889641819941917,0.7889641819941917,0.7889641819941917,0.7879961277831559,0.78702807357212,0.7860600193610843,0.7850919651500484,0.7841239109390126,0.7831558567279767,0.782187802516941,0.7812197483059051,0.7802516940948693,0.7792836398838335,0.7783155856727977,0.7783155856727977,0.7773475314617618,0.7763794772507261,0.7754114230396902,0.7744433688286544,0.7734753146176185,0.7725072604065828,0.771539206195547,0.7705711519845111,0.7696030977734754,0.7686350435624395,0.7676669893514037,0.7666989351403679,0.7657308809293321,0.7647628267182962,0.7637947725072604,0.7628267182962246,0.7618586640851888,0.7608906098741529,0.7608906098741529,0.7599225556631172,0.7589545014520813,0.7579864472410455,0.7570183930300097,0.7560503388189739,0.755082284607938,0.755082284607938,0.7541142303969022,0.7531461761858664,0.7521781219748306,0.7512100677637947,0.7512100677637947,0.750242013552759,0.7492739593417231,0.7483059051306873,0.7473378509196515,0.7473378509196515,0.7463697967086157,0.7454017424975798,0.744433688286544,0.7434656340755083,0.7424975798644724,0.7415295256534365,0.7405614714424008,0.739593417231365,0.739593417231365,0.739593417231365,0.7386253630203291,0.7376573088092934,0.7366892545982575,0.7357212003872217,0.7347531461761858,0.7337850919651501,0.7337850919651501,0.7328170377541142,0.7318489835430784,0.7308809293320426,0.7299128751210068,0.7289448209099709,0.7279767666989352,0.7270087124878993,0.7260406582768635,0.7250726040658277,0.7241045498547919,0.723136495643756,0.7221684414327202,0.7212003872216844,0.7202323330106486,0.7192642787996127,0.7192642787996127,0.718296224588577,0.7173281703775412,0.7163601161665053,0.7153920619554696,0.7144240077444337,0.7144240077444337,0.7144240077444337,0.7134559535333979,0.712487899322362,0.7115198451113263,0.7105517909002904,0.7095837366892546,0.7086156824782188,0.707647628267183,0.707647628267183,0.7066795740561471,0.7057115198451114,0.7057115198451114,0.7047434656340755,0.7037754114230397,0.7028073572120038,0.7018393030009681,0.7008712487899322,0.6999031945788964,0.6989351403678606,0.6979670861568248,0.6969990319457889,0.6960309777347532,0.6950629235237173,0.6940948693126815,0.6940948693126815,0.6931268151016456,0.6931268151016456,0.6921587608906099,0.691190706679574,0.6902226524685382,0.6892545982575025,0.6892545982575025,0.6882865440464666,0.6873184898354308,0.686350435624395,0.6853823814133592,0.6844143272023233,0.6844143272023233,0.6834462729912875,0.6824782187802517,0.6815101645692159,0.68054211035818,0.68054211035818,0.6795740561471443,0.6795740561471443,0.6786060019361084,0.6776379477250726,0.6766698935140368,0.6766698935140368,0.675701839303001,0.6747337850919651,0.6737657308809293,0.6727976766698935,0.6718296224588577,0.6708615682478218,0.6698935140367861,0.6689254598257502,0.6679574056147144,0.6669893514036787,0.6660212971926428,0.665053242981607,0.665053242981607,0.6640851887705711,0.6631171345595354,0.6621490803484995,0.6611810261374637,0.6602129719264279,0.6592449177153921,0.6582768635043562,0.6573088092933205,0.6563407550822846,0.6563407550822846,0.6553727008712488,0.6544046466602129,0.6534365924491772,0.6524685382381413,0.6515004840271055,0.6505324298160697,0.6495643756050339,0.648596321393998,0.6476282671829623,0.6466602129719264,0.6456921587608906,0.6447241045498547,0.643756050338819,0.643756050338819,0.6427879961277831,0.6418199419167473,0.6408518877057116,0.6398838334946757,0.6389157792836399,0.6379477250726041,0.6369796708615683,0.6360116166505324,0.6350435624394967,0.6340755082284608,0.633107454017425,0.6321393998063891,0.6311713455953534,0.6302032913843175,0.6292352371732817,0.6282671829622459,0.6272991287512101,0.6263310745401742,0.6253630203291385,0.6243949661181026,0.6234269119070668,0.6224588576960309,0.6214908034849952,0.6205227492739593,0.6195546950629235,0.6185866408518877,0.6176185866408519,0.616650532429816,0.6156824782187803,0.6147144240077445,0.6137463697967086,0.6127783155856728,0.611810261374637,0.6108422071636012,0.6098741529525653,0.6089060987415296,0.6079380445304937,0.6069699903194579,0.6060019361084221,0.6050338818973863,0.6040658276863504,0.6030977734753146,0.6021297192642788,0.601161665053243,0.6001936108422071,0.5992255566311714,0.5982575024201355,0.5972894482090997,0.5963213939980639,0.5953533397870281,0.5943852855759922,0.5934172313649564,0.5924491771539206,0.5914811229428848,0.590513068731849,0.5895450145208132,0.5885769603097774,0.5876089060987415,0.5866408518877058,0.5856727976766699,0.5847047434656341,0.5837366892545982,0.5827686350435625,0.5818005808325266,0.5808325266214908,0.5808325266214908,0.579864472410455,0.5788964181994192,0.5779283639883833,0.5769603097773476,0.5759922555663117,0.5750242013552759,0.5750242013552759,0.57405614714424,0.5730880929332043,0.5721200387221684,0.5711519845111326,0.5701839303000968,0.569215876089061,0.5682478218780251,0.5682478218780251,0.5672797676669894,0.5663117134559535,0.5653436592449177,0.5643756050338818,0.5634075508228461,0.5624394966118103,0.5614714424007744,0.5605033881897387,0.5595353339787028,0.558567279767667,0.5575992255566312,0.5566311713455954,0.5556631171345595,0.5546950629235237,0.5537270087124879,0.5527589545014521,0.5517909002904162,0.5508228460793805,0.5498547918683446,0.5488867376573088,0.547918683446273,0.547918683446273,0.5469506292352372,0.5459825750242013,0.5450145208131656,0.5450145208131656,0.5440464666021297,0.5430784123910939,0.542110358180058,0.5411423039690223,0.5401742497579864,0.5392061955469506,0.5382381413359149,0.537270087124879,0.5363020329138432,0.5353339787028074,0.5343659244917716,0.5333978702807357,0.5324298160696999,0.5314617618586641,0.5304937076476283,0.5295256534365924,0.5285575992255567,0.5275895450145208,0.526621490803485,0.5256534365924492,0.5246853823814134,0.5237173281703775,0.5227492739593417,0.5217812197483059,0.5208131655372701,0.5198451113262342,0.5188770571151985,0.5179090029041626,0.5169409486931268,0.515972894482091,0.5150048402710552,0.5140367860600193,0.5130687318489835,0.5121006776379478,0.5111326234269119,0.510164569215876,0.5091965150048403,0.5091965150048403,0.5082284607938045,0.5072604065827686,0.5062923523717329,0.505324298160697,0.5043562439496612,0.5033881897386253,0.5024201355275896,0.5014520813165537,0.5014520813165537,0.5004840271055179,0.4995159728944821,0.4985479186834463,0.4975798644724105,0.49661181026137463,0.49564375605033884,0.494675701839303,0.4937076476282672,0.4927395934172314,0.49177153920619554,0.49080348499515974,0.4898354307841239,0.4888673765730881,0.4878993223620523,0.48693126815101645,0.48596321393998065,0.48596321393998065,0.4849951597289448,0.484027105517909,0.4830590513068732,0.48209099709583736,0.48112294288480156,0.4801548886737657,0.4791868344627299,0.4782187802516941,0.47725072604065827,0.4762826718296225,0.4753146176185866,0.4743465634075508,0.47337850919651503,0.4724104549854792,0.4714424007744434,0.47047434656340753,0.46950629235237173,0.46950629235237173,0.46853823814133594,0.4675701839303001,0.4666021297192643,0.46563407550822844,0.46466602129719264,0.46369796708615685,0.462729912875121,0.462729912875121,0.4617618586640852,0.46079380445304935,0.45982575024201355,0.45885769603097776,0.4578896418199419,0.4569215876089061,0.45595353339787026,0.45498547918683446,0.45401742497579867,0.4530493707647628,0.452081316553727,0.45111326234269117,0.45014520813165537,0.4491771539206196,0.4482090997095837,0.44724104549854793,0.4462729912875121,0.4453049370764763,0.4453049370764763,0.4443368828654405,0.44336882865440463,0.44240077444336884,0.441432720232333,0.4404646660212972,0.4394966118102614,0.43852855759922554,0.43756050338818975,0.4365924491771539,0.4356243949661181,0.4346563407550823,0.43368828654404645,0.43272023233301066,0.4317521781219748,0.430784123910939,0.4298160696999032,0.42884801548886736,0.42787996127783157,0.4269119070667957,0.4259438528557599,0.4249757986447241,0.42400774443368827,0.42400774443368827,0.4230396902226525,0.4220716360116166,0.42110358180058083,0.42013552758954503,0.4191674733785092,0.4181994191674734,0.41723136495643753,0.41626331074540174,0.41529525653436594,0.4143272023233301,0.4133591481122943,0.41239109390125844,0.41142303969022265,0.41045498547918685,0.409486931268151,0.4085188770571152,0.4075508228460794,0.40658276863504356,0.40561471442400776,0.4046466602129719,0.4036786060019361,0.4027105517909003,0.40174249757986447,0.40077444336882867,0.3998063891577928,0.398838334946757,0.3978702807357212,0.3978702807357212,0.3969022265246854,0.3959341723136496,0.39496611810261373,0.39399806389157793,0.39303000968054214,0.3920619554695063,0.3910939012584705,0.39012584704743464,0.38818973862536305,0.3872216844143272,0.38528557599225555,0.38431752178121975,0.38334946757018395,0.3823814133591481,0.3814133591481123,0.38044530493707646,0.37947725072604066,0.37850919651500486,0.377541142303969,0.3765730880929332,0.37560503388189737,0.37463697967086157,0.3736689254598258,0.3727008712487899,0.3717328170377541,0.3707647628267183,0.3697967086156825,0.3688286544046467,0.36786060019361083,0.36689254598257504,0.3659244917715392,0.3649564375605034,0.3639883833494676,0.36302032913843174,0.36205227492739595,0.3610842207163601,0.3601161665053243,0.3591481122942885,0.35818005808325265,0.35721200387221685,0.356243949661181,0.3552758954501452,0.3543078412391094,0.35333978702807356,0.35237173281703776,0.3514036786060019,0.3504356243949661,0.3494675701839303,0.34849951597289447,0.3475314617618587,0.3465634075508228,0.345595353339787,0.34462729912875123,0.3436592449177154,0.3426911907066796,0.34172313649564373,0.34075508228460794,0.33978702807357214,0.3388189738625363,0.3388189738625363,0.3378509196515005,0.33688286544046464,0.33591481122942884,0.33494675701839305,0.3339787028073572,0.3330106485963214,0.33204259438528555,0.33107454017424975,0.33010648596321396,0.3291384317521781,0.3281703775411423,0.32720232333010646,0.32623426911907066,0.32526621490803487,0.324298160696999,0.3233301064859632,0.32236205227492737,0.3213939980638916,0.3204259438528558,0.3194578896418199,0.31848983543078413,0.31752178121974833,0.3165537270087125,0.3155856727976767,0.31461761858664083,0.31364956437560504,0.31268151016456924,0.3117134559535334,0.3107454017424976,0.30977734753146174,0.30880929332042595,0.30784123910939015,0.3068731848983543,0.3059051306873185,0.30493707647628265,0.30396902226524686,0.30300096805421106,0.3020329138431752,0.3010648596321394,0.30009680542110356,0.29912875121006777,0.29816069699903197,0.2971926427879961,0.2971926427879961,0.2962245885769603,0.2952565343659245,0.2942884801548887,0.2933204259438529,0.29235237173281703,0.29138431752178123,0.2904162633107454,0.2894482090997096,0.2884801548886738,0.28751210067763794,0.28654404646660214,0.2855759922555663,0.2846079380445305,0.2836398838334947,0.28267182962245885,0.28170377541142305,0.2807357212003872,0.2797676669893514,0.27783155856727976,0.27686350435624396,0.2758954501452081,0.2749273959341723,0.2739593417231365,0.27299128751210067,0.27202323330106487,0.271055179090029,0.2700871248789932,0.2691190706679574,0.2681510164569216,0.2671829622458858,0.26621490803484993,0.26524685382381413,0.26427879961277834,0.2633107454017425,0.2623426911907067,0.26137463697967084,0.26040658276863504,0.25943852855759925,0.2584704743465634,0.2575024201355276,0.25653436592449175,0.25556631171345595,0.25459825750242016,0.2536302032913843,0.2526621490803485,0.25169409486931266,0.25072604065827686,0.24975798644724104,0.24878993223620524,0.24782187802516942,0.2468538238141336,0.24588576960309777,0.24491771539206195,0.24394966118102615,0.24298160696999033,0.2420135527589545,0.24104549854791868,0.24007744433688286,0.23910939012584706,0.23814133591481124,0.2371732817037754,0.2362052274927396,0.23523717328170377,0.23426911907066797,0.23330106485963215,0.23233301064859632,0.2313649564375605,0.23039690222652467,0.22942884801548888,0.22846079380445306,0.22749273959341723,0.22749273959341723,0.2265246853823814,0.22555663117134558,0.2245885769603098,0.22362052274927396,0.22265246853823814,0.22168441432720232,0.2207163601161665,0.2197483059051307,0.21878025169409487,0.21781219748305905,0.21684414327202323,0.2158760890609874,0.2149080348499516,0.21393998063891578,0.21297192642787996,0.21200387221684414,0.2110358180058083,0.21006776379477252,0.2090997095837367,0.20813165537270087,0.20716360116166505,0.20619554695062922,0.2042594385285576,0.20329138431752178,0.20232333010648595,0.20135527589545016,0.20038722168441434,0.1994191674733785,0.1984511132623427,0.19748305905130686,0.19651500484027107,0.19554695062923524,0.19457889641819942,0.1936108422071636,0.19264278799612777,0.19167473378509198,0.19070667957405615,0.18973862536302033,0.1887705711519845,0.18780251694094868,0.1868344627299129,0.18586640851887706,0.18489835430784124,0.18393030009680542,0.1829622458857696,0.1819941916747338,0.18102613746369797,0.18005808325266215,0.17909002904162633,0.1781219748305905,0.1771539206195547,0.17618586640851888,0.17521781219748306,0.17424975798644723,0.1732817037754114,0.17231364956437561,0.1713455953533398,0.17037754114230397,0.16940948693126814,0.16844143272023232,0.16747337850919652,0.1665053242981607,0.16553727008712488,0.16456921587608905,0.16360116166505323,0.16263310745401743,0.1616650532429816,0.1606969990319458,0.15972894482090996,0.15876089060987417,0.15779283639883834,0.15682478218780252,0.1558567279767667,0.15488867376573087,0.15392061955469508,0.15295256534365925,0.15198451113262343,0.1510164569215876,0.15004840271055178,0.14908034849951599,0.14811229428848016,0.14714424007744434,0.14617618586640851,0.1452081316553727,0.1442400774443369,0.14327202323330107,0.14230396902226525,0.14133591481122942,0.14133591481122942,0.1403678606001936,0.1393998063891578,0.13843175217812198,0.13746369796708616,0.13649564375605033,0.1355275895450145,0.1345595353339787,0.1335914811229429,0.13262342691190707,0.13165537270087124,0.13068731848983542,0.12971926427879962,0.1287512100677638,0.12778315585672798,0.12681510164569215,0.12584704743465633,0.12487899322362052,0.12391093901258471,0.12294288480154889,0.12197483059051308,0.12100677637947725,0.12003872216844143,0.11907066795740562,0.1181026137463698,0.11713455953533398,0.11616650532429816,0.11519845111326234,0.11423039690222653,0.1132623426911907,0.1122942884801549,0.11132623426911907,0.11035818005808325,0.10939012584704744,0.10842207163601161,0.1074540174249758,0.10648596321393998,0.10551790900290416,0.10454985479186835,0.10358180058083252,0.10261374636979671,0.10164569215876089,0.10067763794772508,0.09970958373668926,0.09874152952565343,0.09777347531461762,0.0968054211035818,0.09583736689254599,0.09486931268151017,0.09390125847047434,0.09293320425943853,0.09196515004840271,0.0909970958373669,0.09002904162633107,0.08906098741529525,0.08809293320425944,0.08615682478218781,0.08518877057115198,0.08422071636011616,0.08325266214908035,0.08228460793804453,0.08131655372700872,0.0803484995159729,0.07938044530493708,0.07841239109390126,0.07744433688286544,0.07647628267182963,0.0755082284607938,0.07454017424975799,0.07357212003872217,0.07260406582768635,0.07163601161665054,0.07066795740561471,0.0696999031945789,0.06873184898354308,0.06776379477250725,0.06679574056147145,0.06582768635043562,0.06485963213939981,0.06389157792836399,0.06292352371732816,0.061955469506292354,0.06098741529525654,0.060019361084220714,0.0590513068731849,0.05808325266214908,0.057115198451113264,0.05614714424007745,0.05517909002904162,0.05517909002904162,0.05517909002904162,0.05421103581800581,0.05324298160696999,0.05227492739593417,0.051306873184898356,0.05033881897386254,0.049370764762826716,0.0484027105517909,0.04743465634075508,0.046466602129719266,0.04549854791868345,0.044530493707647625,0.044530493707647625,0.04356243949661181,0.04259438528557599,0.041626331074540175,0.04065827686350436,0.03969022265246854,0.03872216844143272,0.0377541142303969,0.036786060019361085,0.03581800580832527,0.03484995159728945,0.03388189738625363,0.03291384317521781,0.031945788964181994,0.031945788964181994,0.030977734753146177,0.030009680542110357,0.02904162633107454,0.027105517909002903,0.026137463697967087,0.02516940948693127,0.02420135527589545,0.023233301064859633,0.022265246853823813,0.021297192642787996,0.02032913843175218,0.01936108422071636,0.018393030009680542,0.017424975798644726,0.016456921587608905,0.015488867376573089,0.015488867376573089,0.01452081316553727,0.013552758954501452,0.012584704743465635,0.011616650532429816,0.010648596321393998,0.00968054211035818,0.008712487899322363,0.007744433688286544,0.006776379477250726,0.005808325266214908,0.00484027105517909,0.003872216844143272,0.002904162633107454,0.001936108422071636,0.000968054211035818,0],"xaxis":"x","y":[0.7341862117981521,0.7347083926031295,0.7352313167259786,0.7357549857549858,0.7362794012829651,0.7368045649072753,0.7373304782298359,0.7378571428571429,0.7383845604002859,0.7389127324749643,0.7394416607015032,0.7399713467048711,0.7405017921146954,0.7410329985652798,0.741564967695621,0.7420977011494253,0.7426312005751258,0.7431654676258993,0.7437005039596832,0.7442363112391931,0.7447728911319395,0.7453102453102453,0.7458483754512636,0.7463872832369942,0.7469269703543022,0.7474674384949349,0.7480086893555394,0.7485507246376811,0.7490935460478607,0.7496371552975326,0.7501815541031227,0.7507267441860465,0.7512727272727273,0.7518195050946143,0.752367079388201,0.7529154518950437,0.7534646243617797,0.754014598540146,0.7545653761869978,0.7551169590643275,0.7556693489392831,0.7562225475841874,0.7567765567765568,0.7573313782991202,0.7578870139398386,0.7584434654919237,0.7590007347538574,0.7595588235294117,0.7601177336276674,0.7606774668630338,0.7612380250552689,0.7617994100294986,0.7623616236162362,0.7629246676514032,0.7634885439763488,0.7640532544378699,0.764618800888231,0.7651851851851852,0.765752409191994,0.766320474777448,0.7668893838158871,0.7674591381872214,0.7680297397769517,0.7686011904761905,0.7691734921816828,0.7697466467958272,0.7703206562266965,0.7708955223880597,0.7714712471994025,0.7720478325859492,0.7726252804786836,0.7732035928143712,0.7737827715355805,0.7743628185907047,0.7749437359339835,0.7755255255255256,0.7761081893313299,0.7766917293233083,0.7772761474793077,0.7778614457831325,0.7784476262245666,0.7790346907993967,0.779622641509434,0.7802114803625377,0.780801209372638,0.7813918305597579,0.7819833459500378,0.7825757575757576,0.7831690674753601,0.783763277693475,0.7843583902809416,0.7849544072948328,0.785551330798479,0.7861491628614916,0.7867479055597868,0.7873475609756098,0.7879481311975591,0.7885496183206107,0.7891520244461421,0.7897553516819572,0.7903596021423106,0.7909647779479326,0.7915708812260537,0.7921779141104295,0.7927858787413661,0.793394777265745,0.7940046118370484,0.7946153846153846,0.7952270977675134,0.7958397534668721,0.7964533538936006,0.7970679012345679,0.7976833976833977,0.7982998454404946,0.7989172467130704,0.7995356037151703,0.8001549186676995,0.8007751937984496,0.8013964313421257,0.8020186335403726,0.8026418026418026,0.8032659409020217,0.8038910505836576,0.8045171339563862,0.8051441932969603,0.8057722308892356,0.8064012490241999,0.80703125,0.8076622361219703,0.8082942097026604,0.8089271730618638,0.8095611285266457,0.8101960784313725,0.8108320251177394,0.8114689709347996,0.8121069182389937,0.8127458693941778,0.8133858267716535,0.814026792750197,0.8146687697160884,0.8153117600631413,0.815955766192733,0.8166007905138339,0.817246835443038,0.8178939034045922,0.8185419968304279,0.8191911181601903,0.8198412698412698,0.8204924543288324,0.8211446740858506,0.8217979315831344,0.822452229299363,0.8231075697211155,0.8237639553429027,0.8244213886671987,0.8250798722044729,0.8257394084732215,0.8264,0.8270616493194556,0.8277243589743589,0.8283881315156375,0.8290529695024077,0.8297188755020081,0.8303858520900321,0.831053901850362,0.8317230273752013,0.8315874294923449,0.832258064516129,0.8329297820823245,0.8336025848142165,0.8342764753435732,0.8349514563106796,0.8356275303643724,0.8363047001620746,0.8369829683698297,0.8376623376623377,0.8390243902439024,0.8397070789259561,0.8403908794788274,0.8410757946210269,0.8417618270799347,0.8424489795918367,0.8431372549019608,0.8438266557645135,0.8445171849427169,0.8443898443898444,0.8450819672131148,0.8457752255947498,0.8456486042692939,0.8463434675431388,0.8470394736842105,0.8477366255144033,0.8484349258649094,0.8491343775762572,0.849009900990099,0.8497109826589595,0.8504132231404958,0.8511166253101737,0.8518211920529801,0.8525269262634632,0.8532338308457711,0.8539419087136929,0.8546511627906976,0.8553615960099751,0.8560732113144759,0.8567860116569526,0.8575,0.8582151793160967,0.8580968280467446,0.858813700918964,0.8595317725752508,0.8602510460251046,0.8609715242881072,0.8608549874266554,0.8615771812080537,0.8623005877413937,0.8630252100840337,0.8637510513036165,0.8644781144781145,0.8643639427127211,0.8650927487352446,0.8649789029535865,0.8648648648648649,0.8655959425190194,0.8663282571912013,0.8670618120237087,0.8677966101694915,0.8685326547921968,0.8692699490662139,0.8700084961767205,0.8707482993197279,0.8706382978723404,0.8713798977853492,0.8721227621483376,0.8720136518771331,0.8727583262169086,0.8735042735042735,0.8733960650128315,0.8732876712328768,0.8731790916880892,0.8730703259005146,0.8738197424892704,0.8737113402061856,0.8736027515047291,0.8743545611015491,0.8742463393626184,0.8741379310344828,0.8740293356341674,0.8747841105354058,0.874675885911841,0.8745674740484429,0.8753246753246753,0.8752166377816292,0.8759757155247181,0.8767361111111112,0.8766290182450044,0.8773913043478261,0.8781549173194082,0.8789198606271778,0.8796861377506539,0.8804537521815009,0.880349344978166,0.8811188811188811,0.8810148731408574,0.8817863397548161,0.8825591586327782,0.8833333333333333,0.884108867427568,0.8848857644991213,0.8847845206684257,0.8855633802816901,0.8854625550660793,0.8862433862433863,0.8870255957634599,0.8869257950530035,0.887709991158267,0.8876106194690265,0.8883968113374667,0.8891843971631206,0.8899733806566105,0.8907637655417406,0.8915555555555555,0.8923487544483986,0.8922528940338379,0.893048128342246,0.8929527207850134,0.89375,0.8945487042001787,0.8944543828264758,0.8943598925693823,0.8942652329749103,0.8941704035874439,0.8940754039497307,0.894878706199461,0.89568345323741,0.8964896489648965,0.8972972972972973,0.8981064021641119,0.898014440433213,0.8979223125564589,0.8987341772151899,0.8995475113122172,0.9003623188405797,0.900271985494107,0.9001814882032668,0.9000908265213442,0.9009090909090909,0.9017288444040037,0.9025500910746812,0.902461257976299,0.9023722627737226,0.9031963470319635,0.903107861060329,0.9030192131747484,0.9029304029304029,0.9028414298808433,0.9036697247706422,0.9035812672176309,0.9034926470588235,0.9043238270469182,0.9042357274401474,0.904147465437788,0.9040590405904059,0.9039704524469068,0.9038817005545287,0.9037927844588344,0.9037037037037037,0.9045412418906394,0.9044526901669759,0.9052924791086351,0.9061338289962825,0.9069767441860465,0.9078212290502793,0.907735321528425,0.9076492537313433,0.9084967320261438,0.908411214953271,0.9083255378858747,0.9082397003745318,0.9081537019681349,0.9090056285178236,0.9098591549295775,0.9107142857142857,0.9106302916274694,0.911487758945386,0.9123468426013195,0.9122641509433962,0.9121813031161473,0.9120982986767486,0.9120151371807,0.9119318181818182,0.9118483412322275,0.9117647058823529,0.9116809116809117,0.9115969581749049,0.9115128449096099,0.9114285714285715,0.9113441372735939,0.9122137404580153,0.9121298949379179,0.9130019120458891,0.9129186602870814,0.9128352490421456,0.912751677852349,0.9126679462571977,0.9125840537944284,0.9125,0.9133782483156881,0.9132947976878613,0.914175506268081,0.9140926640926641,0.9140096618357488,0.913926499032882,0.914811229428848,0.9156976744186046,0.9165858389912707,0.916504854368932,0.9164237123420796,0.9163424124513618,0.9172346640701071,0.9171539961013645,0.9170731707317074,0.9169921875,0.916911045943304,0.9168297455968689,0.9177277179236043,0.9176470588235294,0.9175662414131501,0.9174852652259332,0.9174041297935103,0.9173228346456693,0.9172413793103448,0.9171597633136095,0.9170779861796644,0.9179841897233202,0.9179030662710188,0.9188118811881189,0.9187314172447968,0.9196428571428571,0.9195630585898709,0.9194831013916501,0.9194029850746268,0.9203187250996016,0.9202392821535393,0.9201596806387226,0.9210789210789211,0.921,0.9209209209209209,0.9218436873747495,0.9227683049147443,0.9226907630522089,0.9226130653266331,0.9225352112676056,0.9224572004028198,0.9233870967741935,0.9233097880928355,0.9242424242424242,0.9241658240647118,0.9251012145748988,0.9250253292806484,0.9259634888438134,0.9269035532994924,0.926829268292683,0.9267548321464903,0.9276985743380856,0.9276248725790011,0.9275510204081633,0.9274770173646578,0.9284253578732107,0.9293756397134084,0.9293032786885246,0.9292307692307692,0.9291581108829569,0.9290853031860226,0.9290123456790124,0.9289392378990731,0.9298969072164949,0.9308565531475749,0.9318181818181818,0.9327817993795243,0.932712215320911,0.9326424870466321,0.9325726141078838,0.9325025960539979,0.9324324324324325,0.9323621227887617,0.9333333333333333,0.9332638164754953,0.9342379958246346,0.93521421107628,0.9351464435146444,0.9350785340314136,0.9350104821802935,0.9349422875131165,0.9348739495798319,0.9348054679284963,0.9347368421052632,0.934668071654373,0.9356540084388185,0.9355860612460402,0.9355179704016914,0.9365079365079365,0.9364406779661016,0.936373276776246,0.9363057324840764,0.9362380446333688,0.9361702127659575,0.9361022364217252,0.9360341151385928,0.935965848452508,0.9358974358974359,0.9368983957219251,0.936830835117773,0.9367631296891747,0.9366952789699571,0.9366272824919442,0.9365591397849462,0.9364908503767492,0.9364224137931034,0.9374325782092773,0.937365010799136,0.9383783783783783,0.9383116883116883,0.9382448537378115,0.9392624728850325,0.9391965255157437,0.9391304347826087,0.940152339499456,0.9400871459694989,0.9411123227917121,0.9410480349344978,0.940983606557377,0.9409190371991247,0.940854326396495,0.9407894736842105,0.9407244785949506,0.9406593406593406,0.9405940594059405,0.9405286343612335,0.9404630650496141,0.9403973509933775,0.9403314917127071,0.9402654867256637,0.9401993355481728,0.9401330376940134,0.9400665926748057,0.94,0.9399332591768632,0.9398663697104677,0.939799331103679,0.9408482142857143,0.9407821229050279,0.941834451901566,0.9417693169092946,0.9428251121076233,0.9427609427609428,0.9426966292134832,0.9426321709786277,0.9425675675675675,0.9425028184892897,0.9435665914221218,0.943502824858757,0.9434389140271493,0.9445073612684032,0.9444444444444444,0.9443813847900113,0.9443181818181818,0.944254835039818,0.9441913439635535,0.9441277080957811,0.9440639269406392,0.944,0.9450800915331807,0.9450171821305842,0.944954128440367,0.9448909299655568,0.9448275862068966,0.9447640966628308,0.945852534562212,0.9457900807381776,0.9457274826789839,0.945664739884393,0.9456018518518519,0.9455388180764774,0.9454756380510441,0.9465737514518002,0.9476744186046512,0.9476135040745053,0.9475524475524476,0.9474912485414235,0.947429906542056,0.9473684210526315,0.9473067915690867,0.9472450175849941,0.9471830985915493,0.9471210340775558,0.9470588235294117,0.9469964664310954,0.9481132075471698,0.948051948051948,0.9479905437352246,0.9479289940828403,0.9478672985781991,0.9478054567022538,0.9477434679334917,0.9476813317479191,0.9476190476190476,0.9475566150178785,0.9474940334128878,0.9474313022700119,0.9473684210526315,0.9473053892215569,0.947242206235012,0.9471788715486195,0.9471153846153846,0.9470517448856799,0.946987951807229,0.9481302774427021,0.9480676328502415,0.9480048367593712,0.9479418886198547,0.9478787878787879,0.9478155339805825,0.9477521263669502,0.948905109489051,0.9488428745432399,0.948780487804878,0.9487179487179487,0.9486552567237164,0.9498164014687882,0.9497549019607843,0.9496932515337423,0.9496314496314496,0.949569495694957,0.9507389162561576,0.9506781750924784,0.9506172839506173,0.9505562422744128,0.9504950495049505,0.9504337050805453,0.9503722084367245,0.9503105590062112,0.9502487562189055,0.9514321295143213,0.9526184538653366,0.9525593008739076,0.9525,0.9524405506883604,0.9523809523809523,0.9523212045169385,0.9522613065326633,0.9534591194968554,0.9534005037783375,0.9533417402269861,0.9532828282828283,0.9532237673830595,0.9531645569620253,0.9531051964512041,0.9530456852791879,0.9529860228716646,0.9529262086513995,0.9528662420382166,0.9528061224489796,0.9527458492975734,0.9526854219948849,0.9526248399487837,0.9525641025641025,0.9537869062901155,0.9537275064267352,0.9536679536679536,0.9536082474226805,0.9535483870967741,0.9534883720930233,0.9547218628719275,0.9559585492227979,0.9559014267185474,0.9558441558441558,0.9557867360208062,0.9557291666666666,0.9556714471968709,0.9556135770234987,0.9555555555555556,0.9568062827225131,0.9567496723460026,0.9566929133858267,0.9579500657030223,0.9578947368421052,0.9578392621870883,0.9577836411609498,0.9577278731836195,0.9576719576719577,0.9576158940397351,0.9575596816976127,0.9575033200531209,0.9574468085106383,0.9573901464713716,0.9573333333333334,0.9572763684913218,0.9585561497326203,0.9585006693440429,0.9597855227882037,0.959731543624161,0.9596774193548387,0.9596231493943472,0.9595687331536388,0.9608636977058029,0.9608108108108108,0.9607577807848444,0.9607046070460704,0.9606512890094979,0.9605978260869565,0.9619047619047619,0.9618528610354223,0.9618008185538881,0.9617486338797814,0.9616963064295485,0.963013698630137,0.9629629629629629,0.9642857142857143,0.9642365887207703,0.9641873278236914,0.9641379310344828,0.9654696132596685,0.9654218533886584,0.9653739612188366,0.9653259361997226,0.9652777777777778,0.9652294853963839,0.9651810584958217,0.9651324965132496,0.9650837988826816,0.965034965034965,0.9649859943977591,0.9649368863955119,0.9648876404494382,0.9662447257383966,0.9661971830985916,0.9661495063469676,0.9661016949152542,0.9660537482319661,0.9660056657223796,0.9659574468085106,0.9659090909090909,0.9658605974395448,0.9658119658119658,0.9671897289586305,0.9671428571428572,0.9670958512160229,0.9670487106017192,0.9670014347202296,0.9669540229885057,0.9669064748201439,0.9668587896253602,0.9668109668109668,0.9667630057803468,0.9667149059334298,0.9666666666666667,0.9666182873730044,0.9665697674418605,0.9679767103347889,0.967930029154519,0.9678832116788321,0.9678362573099415,0.9677891654465594,0.967741935483871,0.9676945668135095,0.9676470588235294,0.96759941089838,0.967551622418879,0.9675036927621861,0.9674556213017751,0.9674074074074074,0.9673590504451038,0.9673105497771174,0.9672619047619048,0.9672131147540983,0.9671641791044776,0.9671150971599403,0.9670658682634731,0.967016491754123,0.9669669669669669,0.9669172932330827,0.9668674698795181,0.9668174962292609,0.9667673716012085,0.9667170953101362,0.9666666666666667,0.9666160849772383,0.9665653495440729,0.9665144596651446,0.9664634146341463,0.966412213740458,0.9663608562691132,0.9663093415007658,0.9662576687116564,0.9662058371735791,0.9661538461538461,0.9661016949152542,0.9660493827160493,0.9659969088098919,0.9659442724458205,0.9658914728682171,0.9658385093167702,0.9657853810264385,0.9657320872274143,0.9656786271450858,0.965625,0.9655712050078247,0.9655172413793104,0.9654631083202512,0.9654088050314465,0.9653543307086614,0.9652996845425867,0.9652448657187994,0.9651898734177216,0.96513470681458,0.9650793650793651,0.9650238473767886,0.964968152866242,0.9649122807017544,0.9648562300319489,0.9648,0.9647435897435898,0.9646869983948636,0.9646302250803859,0.966183574879227,0.9661290322580646,0.9660743134087237,0.9660194174757282,0.965964343598055,0.9659090909090909,0.9658536585365853,0.9674267100977199,0.967373572593801,0.9673202614379085,0.967266775777414,0.9672131147540983,0.9671592775041051,0.9671052631578947,0.9670510708401977,0.9686468646864687,0.968595041322314,0.9685430463576159,0.9684908789386402,0.96843853820598,0.9683860232945092,0.9683333333333334,0.9682804674457429,0.9682274247491639,0.9681742043551089,0.9681208053691275,0.9680672268907563,0.968013468013468,0.9679595278246206,0.9679054054054054,0.9678510998307953,0.9677966101694915,0.967741935483871,0.967687074829932,0.9676320272572402,0.9675767918088737,0.9675213675213675,0.9691780821917808,0.9691252144082333,0.9690721649484536,0.9690189328743546,0.9706896551724138,0.9706390328151986,0.9705882352941176,0.9705372616984402,0.9704861111111112,0.9704347826086956,0.9703832752613241,0.9703315881326352,0.9702797202797203,0.9702276707530648,0.9701754385964912,0.9701230228471002,0.9700704225352113,0.9700176366843033,0.9699646643109541,0.9699115044247788,0.9698581560283688,0.9698046181172292,0.9697508896797153,0.9696969696969697,0.9696428571428571,0.9695885509838998,0.9695340501792115,0.9694793536804309,0.9694244604316546,0.9693693693693693,0.9693140794223827,0.969258589511754,0.9692028985507246,0.969147005444646,0.9690909090909091,0.9690346083788707,0.968978102189781,0.9689213893967094,0.9688644688644689,0.9688073394495413,0.96875,0.9686924493554327,0.9704797047970479,0.9704251386321626,0.9703703703703703,0.9703153988868275,0.9702602230483272,0.9702048417132216,0.9701492537313433,0.9700934579439252,0.9700374531835206,0.9718574108818011,0.9718045112781954,0.9717514124293786,0.9716981132075472,0.9716446124763705,0.9715909090909091,0.9715370018975332,0.9714828897338403,0.9714285714285714,0.9713740458015268,0.97131931166348,0.9712643678160919,0.9712092130518234,0.9711538461538461,0.9710982658959537,0.971042471042471,0.9709864603481625,0.9728682170542635,0.9728155339805825,0.9727626459143969,0.9727095516569201,0.97265625,0.9726027397260274,0.9725490196078431,0.9724950884086444,0.9724409448818898,0.9723865877712031,0.9723320158102767,0.9722772277227723,0.9722222222222222,0.9721669980119284,0.9721115537848606,0.9720558882235529,0.972,0.9719438877755511,0.9738955823293173,0.9738430583501007,0.9737903225806451,0.9737373737373738,0.9736842105263158,0.973630831643002,0.9735772357723578,0.9735234215885947,0.9755102040816327,0.9754601226993865,0.9754098360655737,0.9753593429158111,0.9753086419753086,0.9752577319587629,0.9752066115702479,0.9751552795031055,0.975103734439834,0.975051975051975,0.975,0.9749478079331941,0.9748953974895398,0.9748427672955975,0.9747899159663865,0.9747368421052631,0.9746835443037974,0.9746300211416491,0.9745762711864406,0.9766454352441614,0.9765957446808511,0.976545842217484,0.9764957264957265,0.9764453961456103,0.9763948497854077,0.9763440860215054,0.9762931034482759,0.9762419006479481,0.9761904761904762,0.9761388286334056,0.9760869565217392,0.9760348583877996,0.9759825327510917,0.975929978118162,0.9758771929824561,0.9758241758241758,0.9757709251101322,0.9757174392935982,0.9756637168141593,0.975609756097561,0.9755555555555555,0.9755011135857461,0.9776785714285714,0.9776286353467561,0.9775784753363229,0.9775280898876404,0.9774774774774775,0.9774266365688488,0.9773755656108597,0.9773242630385488,0.9772727272727273,0.9772209567198178,0.9771689497716894,0.977116704805492,0.9770642201834863,0.9770114942528736,0.9769585253456221,0.976905311778291,0.9768518518518519,0.9767981438515081,0.9767441860465116,0.9766899766899767,0.9766355140186916,0.9765807962529274,0.9765258215962441,0.9764705882352941,0.9764150943396226,0.9763593380614657,0.976303317535545,0.9762470308788599,0.9785714285714285,0.9785202863961814,0.9784688995215312,0.9784172661870504,0.9783653846153846,0.9783132530120482,0.9782608695652174,0.9782082324455206,0.9781553398058253,0.9780487804878049,0.9779951100244498,0.9778869778869779,0.9778325123152709,0.9777777777777777,0.9777227722772277,0.9776674937965261,0.9776119402985075,0.9775561097256857,0.9775,0.9774436090225563,0.9773869346733668,0.9773299748110831,0.9772727272727273,0.9772151898734177,0.9771573604060914,0.9770992366412213,0.9770408163265306,0.9769820971867008,0.9769230769230769,0.9768637532133676,0.9768041237113402,0.9767441860465116,0.9766839378238342,0.9766233766233766,0.9765625,0.9765013054830287,0.9764397905759162,0.9763779527559056,0.9763157894736842,0.9762532981530343,0.9761904761904762,0.9761273209549072,0.976063829787234,0.976,0.9759358288770054,0.9758713136729222,0.9758064516129032,0.9757412398921833,0.9756756756756757,0.975609756097561,0.9755434782608695,0.9754768392370572,0.9754098360655737,0.9753424657534246,0.9752747252747253,0.9752066115702479,0.9751381215469613,0.9750692520775623,0.975,0.9749303621169917,0.9776536312849162,0.9775910364145658,0.9775280898876404,0.9774647887323944,0.9774011299435028,0.9773371104815864,0.9772727272727273,0.9772079772079773,0.9771428571428571,0.9770773638968482,0.9770114942528736,0.9769452449567724,0.976878612716763,0.9768115942028985,0.9767441860465116,0.9766763848396501,0.9766081871345029,0.9765395894428153,0.9764705882352941,0.976401179941003,0.9763313609467456,0.9762611275964391,0.9761904761904762,0.9761194029850746,0.9760479041916168,0.975975975975976,0.9759036144578314,0.9758308157099698,0.9757575757575757,0.9756838905775076,0.975609756097561,0.9755351681957186,0.9754601226993865,0.9753846153846154,0.9753086419753086,0.9752321981424149,0.9751552795031055,0.9750778816199377,0.975,0.9749216300940439,0.9748427672955975,0.9747634069400631,0.9746835443037974,0.9746031746031746,0.9777070063694268,0.9776357827476039,0.9775641025641025,0.977491961414791,0.9774193548387097,0.9773462783171522,0.9772727272727273,0.9771986970684039,0.9771241830065359,0.9770491803278688,0.9769736842105263,0.976897689768977,0.9768211920529801,0.9767441860465116,0.9766666666666667,0.9765886287625418,0.9765100671140939,0.9764309764309764,0.9763513513513513,0.9761904761904762,0.9761092150170648,0.976027397260274,0.9759450171821306,0.9758620689655172,0.9757785467128027,0.9756944444444444,0.975609756097561,0.9755244755244755,0.9754385964912281,0.9753521126760564,0.9752650176678446,0.975177304964539,0.9750889679715302,0.975,0.974910394265233,0.9748201438848921,0.9747292418772563,0.9746376811594203,0.9745454545454545,0.9744525547445255,0.9743589743589743,0.9742647058823529,0.974169741697417,0.9740740740740741,0.9739776951672863,0.9738805970149254,0.9737827715355806,0.9736842105263158,0.9735849056603774,0.9734848484848485,0.973384030418251,0.9732824427480916,0.9731800766283525,0.9730769230769231,0.972972972972973,0.9728682170542635,0.9727626459143969,0.97265625,0.9725490196078431,0.9724409448818898,0.9723320158102767,0.9722222222222222,0.9721115537848606,0.972,0.9718875502008032,0.9717741935483871,0.97165991902834,0.9715447154471545,0.9714285714285714,0.9713114754098361,0.9711934156378601,0.9710743801652892,0.975103734439834,0.975,0.9748953974895398,0.9747899159663865,0.9746835443037974,0.9745762711864406,0.9744680851063829,0.9743589743589743,0.9742489270386266,0.9741379310344828,0.974025974025974,0.9739130434782609,0.9737991266375546,0.9736842105263158,0.973568281938326,0.9734513274336283,0.9733333333333334,0.9732142857142857,0.9730941704035875,0.972972972972973,0.9728506787330317,0.9727272727272728,0.9726027397260274,0.9723502304147466,0.9722222222222222,0.9720930232558139,0.9719626168224299,0.971830985915493,0.9716981132075472,0.9715639810426541,0.9714285714285714,0.9712918660287081,0.9711538461538461,0.9710144927536232,0.970873786407767,0.9707317073170731,0.9705882352941176,0.9704433497536946,0.9702970297029703,0.9701492537313433,0.97,0.9698492462311558,0.9696969696969697,0.9695431472081218,0.9693877551020408,0.9692307692307692,0.9690721649484536,0.9689119170984456,0.96875,0.9685863874345549,0.968421052631579,0.9682539682539683,0.9680851063829787,0.9679144385026738,0.967741935483871,0.9675675675675676,0.967391304347826,0.9672131147540983,0.967032967032967,0.9668508287292817,0.9666666666666667,0.9664804469273743,0.9662921348314607,0.9661016949152542,0.9659090909090909,0.9657142857142857,0.9655172413793104,0.9653179190751445,0.9651162790697675,0.9649122807017544,0.9647058823529412,0.9644970414201184,0.9642857142857143,0.9640718562874252,0.963855421686747,0.9636363636363636,0.9634146341463414,0.9631901840490797,0.9629629629629629,0.9627329192546584,0.9625,0.9622641509433962,0.9620253164556962,0.9617834394904459,0.9615384615384616,0.9612903225806452,0.961038961038961,0.9607843137254902,0.9605263157894737,0.9668874172185431,0.9666666666666667,0.9664429530201343,0.9662162162162162,0.9659863945578231,0.9657534246575342,0.9655172413793104,0.9652777777777778,0.965034965034965,0.9647887323943662,0.9645390070921985,0.9642857142857143,0.9640287769784173,0.9637681159420289,0.9635036496350365,0.9632352941176471,0.9629629629629629,0.9626865671641791,0.9624060150375939,0.9621212121212122,0.9618320610687023,0.9615384615384616,0.9612403100775194,0.9609375,0.9606299212598425,0.9603174603174603,0.96,0.9596774193548387,0.959349593495935,0.9590163934426229,0.9586776859504132,0.9583333333333334,0.957983193277311,0.9576271186440678,0.9572649572649573,0.9568965517241379,0.9565217391304348,0.956140350877193,0.9557522123893806,0.9553571428571429,0.954954954954955,0.9545454545454546,0.9541284403669725,0.9537037037037037,0.9532710280373832,0.9528301886792453,0.9523809523809523,0.9519230769230769,0.9514563106796117,0.9509803921568627,0.9504950495049505,0.95,0.9494949494949495,0.9489795918367347,0.9484536082474226,0.9479166666666666,0.9468085106382979,0.946236559139785,0.9456521739130435,0.945054945054945,0.9444444444444444,0.9438202247191011,0.9431818181818182,0.9425287356321839,0.9418604651162791,0.9411764705882353,0.9404761904761905,0.9397590361445783,0.9390243902439024,0.9382716049382716,0.9375,0.9367088607594937,0.9358974358974359,0.935064935064935,0.9342105263157895,0.9333333333333333,0.9324324324324325,0.9315068493150684,0.9305555555555556,0.9295774647887324,0.9285714285714286,0.927536231884058,0.9264705882352942,0.9253731343283582,0.9242424242424242,0.9230769230769231,0.921875,0.9206349206349206,0.9193548387096774,0.9344262295081968,0.95,0.9491525423728814,0.9482758620689655,0.9473684210526315,0.9464285714285714,0.9454545454545454,0.9444444444444444,0.9433962264150944,0.9423076923076923,0.9411764705882353,0.94,0.9387755102040817,0.9583333333333334,0.9574468085106383,0.9565217391304348,0.9555555555555556,0.9545454545454546,0.9534883720930233,0.9523809523809523,0.9512195121951219,0.95,0.9487179487179487,0.9473684210526315,0.9459459459459459,0.9444444444444444,0.9428571428571428,0.9705882352941176,0.9696969696969697,0.96875,0.967741935483871,0.9655172413793104,0.9642857142857143,0.9629629629629629,0.9615384615384616,0.96,0.9583333333333334,0.9565217391304348,0.9545454545454546,0.9523809523809523,0.95,0.9473684210526315,0.9444444444444444,0.9411764705882353,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1],"yaxis":"y"}],"layout":{"height":600,"legend":{"tracegroupgap":0},"shapes":[{"line":{"dash":"dash"},"type":"line","x0":0,"x1":1,"y0":1,"y1":0}],"showlegend":false,"template":{"data":{"bar":[{"error_x":{"color":"#2a3f5f"},"error_y":{"color":"#2a3f5f"},"marker":{"line":{"color":"white","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"barpolar":[{"marker":{"line":{"color":"white","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"carpet":[{"aaxis":{"endlinecolor":"#2a3f5f","gridcolor":"#C8D4E3","linecolor":"#C8D4E3","minorgridcolor":"#C8D4E3","startlinecolor":"#2a3f5f"},"baxis":{"endlinecolor":"#2a3f5f","gridcolor":"#C8D4E3","linecolor":"#C8D4E3","minorgridcolor":"#C8D4E3","startlinecolor":"#2a3f5f"},"type":"carpet"}],"choropleth":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"choropleth"}],"contour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"contour"}],"contourcarpet":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"contourcarpet"}],"heatmap":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmap"}],"heatmapgl":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmapgl"}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"histogram2d":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2d"}],"histogram2dcontour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2dcontour"}],"mesh3d":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"mesh3d"}],"parcoords":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"parcoords"}],"pie":[{"automargin":true,"type":"pie"}],"scatter":[{"fillpattern":{"fillmode":"overlay","size":10,"solidity":0.2},"type":"scatter"}],"scatter3d":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter3d"}],"scattercarpet":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattercarpet"}],"scattergeo":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergeo"}],"scattergl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergl"}],"scattermapbox":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattermapbox"}],"scatterpolar":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolar"}],"scatterpolargl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolargl"}],"scatterternary":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterternary"}],"surface":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"surface"}],"table":[{"cells":{"fill":{"color":"#EBF0F8"},"line":{"color":"white"}},"header":{"fill":{"color":"#C8D4E3"},"line":{"color":"white"}},"type":"table"}]},"layout":{"annotationdefaults":{"arrowcolor":"#2a3f5f","arrowhead":0,"arrowwidth":1},"autotypenumbers":"strict","coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]],"sequential":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"sequentialminus":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]]},"colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#2a3f5f"},"geo":{"bgcolor":"white","lakecolor":"white","landcolor":"white","showlakes":true,"showland":true,"subunitcolor":"#C8D4E3"},"hoverlabel":{"align":"left"},"hovermode":"closest","mapbox":{"style":"light"},"paper_bgcolor":"white","plot_bgcolor":"white","polar":{"angularaxis":{"gridcolor":"#EBF0F8","linecolor":"#EBF0F8","ticks":""},"bgcolor":"white","radialaxis":{"gridcolor":"#EBF0F8","linecolor":"#EBF0F8","ticks":""}},"scene":{"xaxis":{"backgroundcolor":"white","gridcolor":"#DFE8F3","gridwidth":2,"linecolor":"#EBF0F8","showbackground":true,"ticks":"","zerolinecolor":"#EBF0F8"},"yaxis":{"backgroundcolor":"white","gridcolor":"#DFE8F3","gridwidth":2,"linecolor":"#EBF0F8","showbackground":true,"ticks":"","zerolinecolor":"#EBF0F8"},"zaxis":{"backgroundcolor":"white","gridcolor":"#DFE8F3","gridwidth":2,"linecolor":"#EBF0F8","showbackground":true,"ticks":"","zerolinecolor":"#EBF0F8"}},"shapedefaults":{"line":{"color":"#2a3f5f"}},"ternary":{"aaxis":{"gridcolor":"#DFE8F3","linecolor":"#A2B1C6","ticks":""},"baxis":{"gridcolor":"#DFE8F3","linecolor":"#A2B1C6","ticks":""},"bgcolor":"white","caxis":{"gridcolor":"#DFE8F3","linecolor":"#A2B1C6","ticks":""}},"title":{"x":0.05},"xaxis":{"automargin":true,"gridcolor":"#EBF0F8","linecolor":"#EBF0F8","ticks":"","title":{"standoff":15},"zerolinecolor":"#EBF0F8","zerolinewidth":2},"yaxis":{"automargin":true,"gridcolor":"#EBF0F8","linecolor":"#EBF0F8","ticks":"","title":{"standoff":15},"zerolinecolor":"#EBF0F8","zerolinewidth":2}}},"title":{"text":"Precision-Recall Curve (AUC=0.9211)"},"width":800,"xaxis":{"anchor":"y","constrain":"domain","domain":[0,1],"title":{"text":"Recall"}},"yaxis":{"anchor":"x","domain":[0,1],"scaleanchor":"x","scaleratio":1,"title":{"text":"Precision"}}}},"text/html":["<div>                            <div id=\"5c6e1325-ae8e-40a4-88b4-90aed2c124f2\" class=\"plotly-graph-div\" style=\"height:600px; width:800px;\"></div>            <script type=\"text/javascript\">                require([\"plotly\"], function(Plotly) {                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"5c6e1325-ae8e-40a4-88b4-90aed2c124f2\")) {                    Plotly.newPlot(                        \"5c6e1325-ae8e-40a4-88b4-90aed2c124f2\",                        [{\"fillpattern\":{\"shape\":\"\"},\"hovertemplate\":\"Recall=%{x}\\u003cbr\\u003ePrecision=%{y}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"\",\"line\":{\"color\":\"#636efa\"},\"marker\":{\"symbol\":\"circle\"},\"mode\":\"lines\",\"name\":\"\",\"orientation\":\"v\",\"showlegend\":false,\"stackgroup\":\"1\",\"x\":[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,0.9990319457889641,0.9990319457889641,0.9990319457889641,0.9990319457889641,0.9990319457889641,0.9990319457889641,0.9990319457889641,0.9990319457889641,0.9990319457889641,0.9990319457889641,0.9990319457889641,0.9990319457889641,0.9990319457889641,0.9990319457889641,0.9990319457889641,0.9990319457889641,0.9990319457889641,0.9990319457889641,0.9990319457889641,0.9980638915779284,0.9980638915779284,0.9980638915779284,0.9970958373668926,0.9970958373668926,0.9970958373668926,0.9970958373668926,0.9970958373668926,0.9970958373668926,0.9961277831558567,0.9961277831558567,0.9961277831558567,0.9961277831558567,0.9961277831558567,0.9961277831558567,0.9961277831558567,0.9961277831558567,0.9961277831558567,0.9961277831558567,0.9961277831558567,0.9961277831558567,0.9961277831558567,0.9961277831558567,0.995159728944821,0.995159728944821,0.995159728944821,0.995159728944821,0.995159728944821,0.9941916747337851,0.9941916747337851,0.9941916747337851,0.9941916747337851,0.9941916747337851,0.9941916747337851,0.9932236205227493,0.9932236205227493,0.9922555663117134,0.9912875121006777,0.9912875121006777,0.9912875121006777,0.9912875121006777,0.9912875121006777,0.9912875121006777,0.9912875121006777,0.9912875121006777,0.9912875121006777,0.9903194578896418,0.9903194578896418,0.9903194578896418,0.989351403678606,0.989351403678606,0.989351403678606,0.9883833494675702,0.9874152952565344,0.9864472410454985,0.9854791868344628,0.9854791868344628,0.9845111326234269,0.9835430784123911,0.9835430784123911,0.9825750242013552,0.9816069699903195,0.9806389157792836,0.9806389157792836,0.9796708615682478,0.978702807357212,0.978702807357212,0.9777347531461762,0.9777347531461762,0.9777347531461762,0.9767666989351403,0.9767666989351403,0.9767666989351403,0.9767666989351403,0.9767666989351403,0.9767666989351403,0.9757986447241046,0.9757986447241046,0.9748305905130688,0.9748305905130688,0.9748305905130688,0.9748305905130688,0.9748305905130688,0.9748305905130688,0.9738625363020329,0.9738625363020329,0.972894482090997,0.972894482090997,0.972894482090997,0.9719264278799613,0.9719264278799613,0.9709583736689255,0.9709583736689255,0.9709583736689255,0.9709583736689255,0.9709583736689255,0.9709583736689255,0.9709583736689255,0.9699903194578896,0.9699903194578896,0.9690222652468539,0.9690222652468539,0.9690222652468539,0.968054211035818,0.9670861568247822,0.9661181026137464,0.9651500484027106,0.9641819941916747,0.9641819941916747,0.9641819941916747,0.9641819941916747,0.9641819941916747,0.9641819941916747,0.9632139399806389,0.9622458857696031,0.9622458857696031,0.9622458857696031,0.9622458857696031,0.9612778315585673,0.9603097773475314,0.9593417231364957,0.9593417231364957,0.9593417231364957,0.9593417231364957,0.9583736689254598,0.957405614714424,0.957405614714424,0.9564375605033882,0.9554695062923524,0.9545014520813165,0.9535333978702807,0.9535333978702807,0.952565343659245,0.9515972894482091,0.9515972894482091,0.9506292352371732,0.9496611810261375,0.9486931268151017,0.9477250726040658,0.9467570183930301,0.9457889641819942,0.9448209099709584,0.9448209099709584,0.9438528557599225,0.9438528557599225,0.9438528557599225,0.9438528557599225,0.9438528557599225,0.9428848015488868,0.9419167473378509,0.9419167473378509,0.9409486931268151,0.9399806389157793,0.9390125847047435,0.9380445304937076,0.9380445304937076,0.9380445304937076,0.9380445304937076,0.9370764762826719,0.9370764762826719,0.9370764762826719,0.936108422071636,0.9351403678606002,0.9341723136495643,0.9332042594385286,0.9322362052274927,0.9312681510164569,0.9303000968054211,0.9293320425943853,0.9283639883833494,0.9273959341723137,0.9264278799612778,0.925459825750242,0.925459825750242,0.9244917715392061,0.9244917715392061,0.9235237173281704,0.9225556631171346,0.9215876089060987,0.920619554695063,0.9196515004840271,0.9186834462729913,0.9186834462729913,0.9177153920619555,0.9177153920619555,0.9167473378509197,0.9157792836398838,0.914811229428848,0.914811229428848,0.914811229428848,0.914811229428848,0.9138431752178122,0.9128751210067764,0.9119070667957405,0.9119070667957405,0.9109390125847048,0.9099709583736689,0.9090029041626331,0.9080348499515973,0.9070667957405615,0.9070667957405615,0.9060987415295256,0.9051306873184899,0.904162633107454,0.9031945788964182,0.9022265246853823,0.9012584704743466,0.9002904162633107,0.8993223620522749,0.8993223620522749,0.8983543078412392,0.8983543078412392,0.8973862536302033,0.8973862536302033,0.8964181994191674,0.8954501452081317,0.8944820909970959,0.8944820909970959,0.89351403678606,0.8925459825750242,0.8925459825750242,0.8915779283639884,0.8906098741529526,0.8906098741529526,0.8906098741529526,0.8896418199419167,0.888673765730881,0.8877057115198451,0.8867376573088093,0.8867376573088093,0.8857696030977735,0.8857696030977735,0.8848015488867377,0.8848015488867377,0.8838334946757018,0.8838334946757018,0.8838334946757018,0.882865440464666,0.8818973862536302,0.8818973862536302,0.8809293320425944,0.8799612778315585,0.8789932236205228,0.8789932236205228,0.8789932236205228,0.8780251694094869,0.8770571151984511,0.8760890609874153,0.8751210067763795,0.8741529525653436,0.8731848983543078,0.8731848983543078,0.8731848983543078,0.8731848983543078,0.8731848983543078,0.872216844143272,0.8712487899322362,0.8702807357212003,0.8693126815101646,0.8683446272991288,0.8673765730880929,0.8673765730880929,0.8664085188770572,0.8664085188770572,0.8664085188770572,0.8654404646660213,0.8644724104549855,0.8635043562439496,0.8625363020329139,0.861568247821878,0.8606001936108422,0.8596321393998064,0.8586640851887706,0.8586640851887706,0.8576960309777347,0.856727976766699,0.856727976766699,0.8557599225556631,0.8547918683446273,0.8538238141335914,0.8528557599225557,0.8518877057115198,0.850919651500484,0.8499515972894482,0.8489835430784124,0.8480154888673765,0.8480154888673765,0.8470474346563408,0.846079380445305,0.8451113262342691,0.8441432720232332,0.8431752178121975,0.8422071636011617,0.8412391093901258,0.8412391093901258,0.8402710551790901,0.8402710551790901,0.8393030009680542,0.8383349467570184,0.8383349467570184,0.8373668925459826,0.8363988383349468,0.8363988383349468,0.8354307841239109,0.8354307841239109,0.8344627299128751,0.8334946757018393,0.8325266214908035,0.8315585672797676,0.8305905130687319,0.829622458857696,0.8286544046466602,0.8276863504356244,0.8267182962245886,0.8257502420135527,0.8247821878025169,0.8238141335914811,0.8228460793804453,0.8218780251694094,0.8209099709583737,0.8199419167473379,0.818973862536302,0.8180058083252663,0.8170377541142304,0.8160696999031946,0.8160696999031946,0.8151016456921588,0.8151016456921588,0.814133591481123,0.814133591481123,0.8131655372700871,0.8121974830590513,0.8112294288480155,0.8102613746369797,0.8092933204259438,0.8092933204259438,0.8083252662149081,0.8073572120038722,0.8073572120038722,0.8063891577928364,0.8054211035818006,0.8044530493707648,0.8034849951597289,0.8025169409486931,0.8015488867376573,0.8005808325266215,0.7996127783155856,0.7996127783155856,0.7986447241045499,0.797676669893514,0.7967086156824782,0.7957405614714425,0.7947725072604066,0.7947725072604066,0.7938044530493708,0.7928363988383349,0.7918683446272992,0.7909002904162633,0.7899322362052275,0.7889641819941917,0.7889641819941917,0.7889641819941917,0.7879961277831559,0.78702807357212,0.7860600193610843,0.7850919651500484,0.7841239109390126,0.7831558567279767,0.782187802516941,0.7812197483059051,0.7802516940948693,0.7792836398838335,0.7783155856727977,0.7783155856727977,0.7773475314617618,0.7763794772507261,0.7754114230396902,0.7744433688286544,0.7734753146176185,0.7725072604065828,0.771539206195547,0.7705711519845111,0.7696030977734754,0.7686350435624395,0.7676669893514037,0.7666989351403679,0.7657308809293321,0.7647628267182962,0.7637947725072604,0.7628267182962246,0.7618586640851888,0.7608906098741529,0.7608906098741529,0.7599225556631172,0.7589545014520813,0.7579864472410455,0.7570183930300097,0.7560503388189739,0.755082284607938,0.755082284607938,0.7541142303969022,0.7531461761858664,0.7521781219748306,0.7512100677637947,0.7512100677637947,0.750242013552759,0.7492739593417231,0.7483059051306873,0.7473378509196515,0.7473378509196515,0.7463697967086157,0.7454017424975798,0.744433688286544,0.7434656340755083,0.7424975798644724,0.7415295256534365,0.7405614714424008,0.739593417231365,0.739593417231365,0.739593417231365,0.7386253630203291,0.7376573088092934,0.7366892545982575,0.7357212003872217,0.7347531461761858,0.7337850919651501,0.7337850919651501,0.7328170377541142,0.7318489835430784,0.7308809293320426,0.7299128751210068,0.7289448209099709,0.7279767666989352,0.7270087124878993,0.7260406582768635,0.7250726040658277,0.7241045498547919,0.723136495643756,0.7221684414327202,0.7212003872216844,0.7202323330106486,0.7192642787996127,0.7192642787996127,0.718296224588577,0.7173281703775412,0.7163601161665053,0.7153920619554696,0.7144240077444337,0.7144240077444337,0.7144240077444337,0.7134559535333979,0.712487899322362,0.7115198451113263,0.7105517909002904,0.7095837366892546,0.7086156824782188,0.707647628267183,0.707647628267183,0.7066795740561471,0.7057115198451114,0.7057115198451114,0.7047434656340755,0.7037754114230397,0.7028073572120038,0.7018393030009681,0.7008712487899322,0.6999031945788964,0.6989351403678606,0.6979670861568248,0.6969990319457889,0.6960309777347532,0.6950629235237173,0.6940948693126815,0.6940948693126815,0.6931268151016456,0.6931268151016456,0.6921587608906099,0.691190706679574,0.6902226524685382,0.6892545982575025,0.6892545982575025,0.6882865440464666,0.6873184898354308,0.686350435624395,0.6853823814133592,0.6844143272023233,0.6844143272023233,0.6834462729912875,0.6824782187802517,0.6815101645692159,0.68054211035818,0.68054211035818,0.6795740561471443,0.6795740561471443,0.6786060019361084,0.6776379477250726,0.6766698935140368,0.6766698935140368,0.675701839303001,0.6747337850919651,0.6737657308809293,0.6727976766698935,0.6718296224588577,0.6708615682478218,0.6698935140367861,0.6689254598257502,0.6679574056147144,0.6669893514036787,0.6660212971926428,0.665053242981607,0.665053242981607,0.6640851887705711,0.6631171345595354,0.6621490803484995,0.6611810261374637,0.6602129719264279,0.6592449177153921,0.6582768635043562,0.6573088092933205,0.6563407550822846,0.6563407550822846,0.6553727008712488,0.6544046466602129,0.6534365924491772,0.6524685382381413,0.6515004840271055,0.6505324298160697,0.6495643756050339,0.648596321393998,0.6476282671829623,0.6466602129719264,0.6456921587608906,0.6447241045498547,0.643756050338819,0.643756050338819,0.6427879961277831,0.6418199419167473,0.6408518877057116,0.6398838334946757,0.6389157792836399,0.6379477250726041,0.6369796708615683,0.6360116166505324,0.6350435624394967,0.6340755082284608,0.633107454017425,0.6321393998063891,0.6311713455953534,0.6302032913843175,0.6292352371732817,0.6282671829622459,0.6272991287512101,0.6263310745401742,0.6253630203291385,0.6243949661181026,0.6234269119070668,0.6224588576960309,0.6214908034849952,0.6205227492739593,0.6195546950629235,0.6185866408518877,0.6176185866408519,0.616650532429816,0.6156824782187803,0.6147144240077445,0.6137463697967086,0.6127783155856728,0.611810261374637,0.6108422071636012,0.6098741529525653,0.6089060987415296,0.6079380445304937,0.6069699903194579,0.6060019361084221,0.6050338818973863,0.6040658276863504,0.6030977734753146,0.6021297192642788,0.601161665053243,0.6001936108422071,0.5992255566311714,0.5982575024201355,0.5972894482090997,0.5963213939980639,0.5953533397870281,0.5943852855759922,0.5934172313649564,0.5924491771539206,0.5914811229428848,0.590513068731849,0.5895450145208132,0.5885769603097774,0.5876089060987415,0.5866408518877058,0.5856727976766699,0.5847047434656341,0.5837366892545982,0.5827686350435625,0.5818005808325266,0.5808325266214908,0.5808325266214908,0.579864472410455,0.5788964181994192,0.5779283639883833,0.5769603097773476,0.5759922555663117,0.5750242013552759,0.5750242013552759,0.57405614714424,0.5730880929332043,0.5721200387221684,0.5711519845111326,0.5701839303000968,0.569215876089061,0.5682478218780251,0.5682478218780251,0.5672797676669894,0.5663117134559535,0.5653436592449177,0.5643756050338818,0.5634075508228461,0.5624394966118103,0.5614714424007744,0.5605033881897387,0.5595353339787028,0.558567279767667,0.5575992255566312,0.5566311713455954,0.5556631171345595,0.5546950629235237,0.5537270087124879,0.5527589545014521,0.5517909002904162,0.5508228460793805,0.5498547918683446,0.5488867376573088,0.547918683446273,0.547918683446273,0.5469506292352372,0.5459825750242013,0.5450145208131656,0.5450145208131656,0.5440464666021297,0.5430784123910939,0.542110358180058,0.5411423039690223,0.5401742497579864,0.5392061955469506,0.5382381413359149,0.537270087124879,0.5363020329138432,0.5353339787028074,0.5343659244917716,0.5333978702807357,0.5324298160696999,0.5314617618586641,0.5304937076476283,0.5295256534365924,0.5285575992255567,0.5275895450145208,0.526621490803485,0.5256534365924492,0.5246853823814134,0.5237173281703775,0.5227492739593417,0.5217812197483059,0.5208131655372701,0.5198451113262342,0.5188770571151985,0.5179090029041626,0.5169409486931268,0.515972894482091,0.5150048402710552,0.5140367860600193,0.5130687318489835,0.5121006776379478,0.5111326234269119,0.510164569215876,0.5091965150048403,0.5091965150048403,0.5082284607938045,0.5072604065827686,0.5062923523717329,0.505324298160697,0.5043562439496612,0.5033881897386253,0.5024201355275896,0.5014520813165537,0.5014520813165537,0.5004840271055179,0.4995159728944821,0.4985479186834463,0.4975798644724105,0.49661181026137463,0.49564375605033884,0.494675701839303,0.4937076476282672,0.4927395934172314,0.49177153920619554,0.49080348499515974,0.4898354307841239,0.4888673765730881,0.4878993223620523,0.48693126815101645,0.48596321393998065,0.48596321393998065,0.4849951597289448,0.484027105517909,0.4830590513068732,0.48209099709583736,0.48112294288480156,0.4801548886737657,0.4791868344627299,0.4782187802516941,0.47725072604065827,0.4762826718296225,0.4753146176185866,0.4743465634075508,0.47337850919651503,0.4724104549854792,0.4714424007744434,0.47047434656340753,0.46950629235237173,0.46950629235237173,0.46853823814133594,0.4675701839303001,0.4666021297192643,0.46563407550822844,0.46466602129719264,0.46369796708615685,0.462729912875121,0.462729912875121,0.4617618586640852,0.46079380445304935,0.45982575024201355,0.45885769603097776,0.4578896418199419,0.4569215876089061,0.45595353339787026,0.45498547918683446,0.45401742497579867,0.4530493707647628,0.452081316553727,0.45111326234269117,0.45014520813165537,0.4491771539206196,0.4482090997095837,0.44724104549854793,0.4462729912875121,0.4453049370764763,0.4453049370764763,0.4443368828654405,0.44336882865440463,0.44240077444336884,0.441432720232333,0.4404646660212972,0.4394966118102614,0.43852855759922554,0.43756050338818975,0.4365924491771539,0.4356243949661181,0.4346563407550823,0.43368828654404645,0.43272023233301066,0.4317521781219748,0.430784123910939,0.4298160696999032,0.42884801548886736,0.42787996127783157,0.4269119070667957,0.4259438528557599,0.4249757986447241,0.42400774443368827,0.42400774443368827,0.4230396902226525,0.4220716360116166,0.42110358180058083,0.42013552758954503,0.4191674733785092,0.4181994191674734,0.41723136495643753,0.41626331074540174,0.41529525653436594,0.4143272023233301,0.4133591481122943,0.41239109390125844,0.41142303969022265,0.41045498547918685,0.409486931268151,0.4085188770571152,0.4075508228460794,0.40658276863504356,0.40561471442400776,0.4046466602129719,0.4036786060019361,0.4027105517909003,0.40174249757986447,0.40077444336882867,0.3998063891577928,0.398838334946757,0.3978702807357212,0.3978702807357212,0.3969022265246854,0.3959341723136496,0.39496611810261373,0.39399806389157793,0.39303000968054214,0.3920619554695063,0.3910939012584705,0.39012584704743464,0.38818973862536305,0.3872216844143272,0.38528557599225555,0.38431752178121975,0.38334946757018395,0.3823814133591481,0.3814133591481123,0.38044530493707646,0.37947725072604066,0.37850919651500486,0.377541142303969,0.3765730880929332,0.37560503388189737,0.37463697967086157,0.3736689254598258,0.3727008712487899,0.3717328170377541,0.3707647628267183,0.3697967086156825,0.3688286544046467,0.36786060019361083,0.36689254598257504,0.3659244917715392,0.3649564375605034,0.3639883833494676,0.36302032913843174,0.36205227492739595,0.3610842207163601,0.3601161665053243,0.3591481122942885,0.35818005808325265,0.35721200387221685,0.356243949661181,0.3552758954501452,0.3543078412391094,0.35333978702807356,0.35237173281703776,0.3514036786060019,0.3504356243949661,0.3494675701839303,0.34849951597289447,0.3475314617618587,0.3465634075508228,0.345595353339787,0.34462729912875123,0.3436592449177154,0.3426911907066796,0.34172313649564373,0.34075508228460794,0.33978702807357214,0.3388189738625363,0.3388189738625363,0.3378509196515005,0.33688286544046464,0.33591481122942884,0.33494675701839305,0.3339787028073572,0.3330106485963214,0.33204259438528555,0.33107454017424975,0.33010648596321396,0.3291384317521781,0.3281703775411423,0.32720232333010646,0.32623426911907066,0.32526621490803487,0.324298160696999,0.3233301064859632,0.32236205227492737,0.3213939980638916,0.3204259438528558,0.3194578896418199,0.31848983543078413,0.31752178121974833,0.3165537270087125,0.3155856727976767,0.31461761858664083,0.31364956437560504,0.31268151016456924,0.3117134559535334,0.3107454017424976,0.30977734753146174,0.30880929332042595,0.30784123910939015,0.3068731848983543,0.3059051306873185,0.30493707647628265,0.30396902226524686,0.30300096805421106,0.3020329138431752,0.3010648596321394,0.30009680542110356,0.29912875121006777,0.29816069699903197,0.2971926427879961,0.2971926427879961,0.2962245885769603,0.2952565343659245,0.2942884801548887,0.2933204259438529,0.29235237173281703,0.29138431752178123,0.2904162633107454,0.2894482090997096,0.2884801548886738,0.28751210067763794,0.28654404646660214,0.2855759922555663,0.2846079380445305,0.2836398838334947,0.28267182962245885,0.28170377541142305,0.2807357212003872,0.2797676669893514,0.27783155856727976,0.27686350435624396,0.2758954501452081,0.2749273959341723,0.2739593417231365,0.27299128751210067,0.27202323330106487,0.271055179090029,0.2700871248789932,0.2691190706679574,0.2681510164569216,0.2671829622458858,0.26621490803484993,0.26524685382381413,0.26427879961277834,0.2633107454017425,0.2623426911907067,0.26137463697967084,0.26040658276863504,0.25943852855759925,0.2584704743465634,0.2575024201355276,0.25653436592449175,0.25556631171345595,0.25459825750242016,0.2536302032913843,0.2526621490803485,0.25169409486931266,0.25072604065827686,0.24975798644724104,0.24878993223620524,0.24782187802516942,0.2468538238141336,0.24588576960309777,0.24491771539206195,0.24394966118102615,0.24298160696999033,0.2420135527589545,0.24104549854791868,0.24007744433688286,0.23910939012584706,0.23814133591481124,0.2371732817037754,0.2362052274927396,0.23523717328170377,0.23426911907066797,0.23330106485963215,0.23233301064859632,0.2313649564375605,0.23039690222652467,0.22942884801548888,0.22846079380445306,0.22749273959341723,0.22749273959341723,0.2265246853823814,0.22555663117134558,0.2245885769603098,0.22362052274927396,0.22265246853823814,0.22168441432720232,0.2207163601161665,0.2197483059051307,0.21878025169409487,0.21781219748305905,0.21684414327202323,0.2158760890609874,0.2149080348499516,0.21393998063891578,0.21297192642787996,0.21200387221684414,0.2110358180058083,0.21006776379477252,0.2090997095837367,0.20813165537270087,0.20716360116166505,0.20619554695062922,0.2042594385285576,0.20329138431752178,0.20232333010648595,0.20135527589545016,0.20038722168441434,0.1994191674733785,0.1984511132623427,0.19748305905130686,0.19651500484027107,0.19554695062923524,0.19457889641819942,0.1936108422071636,0.19264278799612777,0.19167473378509198,0.19070667957405615,0.18973862536302033,0.1887705711519845,0.18780251694094868,0.1868344627299129,0.18586640851887706,0.18489835430784124,0.18393030009680542,0.1829622458857696,0.1819941916747338,0.18102613746369797,0.18005808325266215,0.17909002904162633,0.1781219748305905,0.1771539206195547,0.17618586640851888,0.17521781219748306,0.17424975798644723,0.1732817037754114,0.17231364956437561,0.1713455953533398,0.17037754114230397,0.16940948693126814,0.16844143272023232,0.16747337850919652,0.1665053242981607,0.16553727008712488,0.16456921587608905,0.16360116166505323,0.16263310745401743,0.1616650532429816,0.1606969990319458,0.15972894482090996,0.15876089060987417,0.15779283639883834,0.15682478218780252,0.1558567279767667,0.15488867376573087,0.15392061955469508,0.15295256534365925,0.15198451113262343,0.1510164569215876,0.15004840271055178,0.14908034849951599,0.14811229428848016,0.14714424007744434,0.14617618586640851,0.1452081316553727,0.1442400774443369,0.14327202323330107,0.14230396902226525,0.14133591481122942,0.14133591481122942,0.1403678606001936,0.1393998063891578,0.13843175217812198,0.13746369796708616,0.13649564375605033,0.1355275895450145,0.1345595353339787,0.1335914811229429,0.13262342691190707,0.13165537270087124,0.13068731848983542,0.12971926427879962,0.1287512100677638,0.12778315585672798,0.12681510164569215,0.12584704743465633,0.12487899322362052,0.12391093901258471,0.12294288480154889,0.12197483059051308,0.12100677637947725,0.12003872216844143,0.11907066795740562,0.1181026137463698,0.11713455953533398,0.11616650532429816,0.11519845111326234,0.11423039690222653,0.1132623426911907,0.1122942884801549,0.11132623426911907,0.11035818005808325,0.10939012584704744,0.10842207163601161,0.1074540174249758,0.10648596321393998,0.10551790900290416,0.10454985479186835,0.10358180058083252,0.10261374636979671,0.10164569215876089,0.10067763794772508,0.09970958373668926,0.09874152952565343,0.09777347531461762,0.0968054211035818,0.09583736689254599,0.09486931268151017,0.09390125847047434,0.09293320425943853,0.09196515004840271,0.0909970958373669,0.09002904162633107,0.08906098741529525,0.08809293320425944,0.08615682478218781,0.08518877057115198,0.08422071636011616,0.08325266214908035,0.08228460793804453,0.08131655372700872,0.0803484995159729,0.07938044530493708,0.07841239109390126,0.07744433688286544,0.07647628267182963,0.0755082284607938,0.07454017424975799,0.07357212003872217,0.07260406582768635,0.07163601161665054,0.07066795740561471,0.0696999031945789,0.06873184898354308,0.06776379477250725,0.06679574056147145,0.06582768635043562,0.06485963213939981,0.06389157792836399,0.06292352371732816,0.061955469506292354,0.06098741529525654,0.060019361084220714,0.0590513068731849,0.05808325266214908,0.057115198451113264,0.05614714424007745,0.05517909002904162,0.05517909002904162,0.05517909002904162,0.05421103581800581,0.05324298160696999,0.05227492739593417,0.051306873184898356,0.05033881897386254,0.049370764762826716,0.0484027105517909,0.04743465634075508,0.046466602129719266,0.04549854791868345,0.044530493707647625,0.044530493707647625,0.04356243949661181,0.04259438528557599,0.041626331074540175,0.04065827686350436,0.03969022265246854,0.03872216844143272,0.0377541142303969,0.036786060019361085,0.03581800580832527,0.03484995159728945,0.03388189738625363,0.03291384317521781,0.031945788964181994,0.031945788964181994,0.030977734753146177,0.030009680542110357,0.02904162633107454,0.027105517909002903,0.026137463697967087,0.02516940948693127,0.02420135527589545,0.023233301064859633,0.022265246853823813,0.021297192642787996,0.02032913843175218,0.01936108422071636,0.018393030009680542,0.017424975798644726,0.016456921587608905,0.015488867376573089,0.015488867376573089,0.01452081316553727,0.013552758954501452,0.012584704743465635,0.011616650532429816,0.010648596321393998,0.00968054211035818,0.008712487899322363,0.007744433688286544,0.006776379477250726,0.005808325266214908,0.00484027105517909,0.003872216844143272,0.002904162633107454,0.001936108422071636,0.000968054211035818,0.0],\"xaxis\":\"x\",\"y\":[0.7341862117981521,0.7347083926031295,0.7352313167259786,0.7357549857549858,0.7362794012829651,0.7368045649072753,0.7373304782298359,0.7378571428571429,0.7383845604002859,0.7389127324749643,0.7394416607015032,0.7399713467048711,0.7405017921146954,0.7410329985652798,0.741564967695621,0.7420977011494253,0.7426312005751258,0.7431654676258993,0.7437005039596832,0.7442363112391931,0.7447728911319395,0.7453102453102453,0.7458483754512636,0.7463872832369942,0.7469269703543022,0.7474674384949349,0.7480086893555394,0.7485507246376811,0.7490935460478607,0.7496371552975326,0.7501815541031227,0.7507267441860465,0.7512727272727273,0.7518195050946143,0.752367079388201,0.7529154518950437,0.7534646243617797,0.754014598540146,0.7545653761869978,0.7551169590643275,0.7556693489392831,0.7562225475841874,0.7567765567765568,0.7573313782991202,0.7578870139398386,0.7584434654919237,0.7590007347538574,0.7595588235294117,0.7601177336276674,0.7606774668630338,0.7612380250552689,0.7617994100294986,0.7623616236162362,0.7629246676514032,0.7634885439763488,0.7640532544378699,0.764618800888231,0.7651851851851852,0.765752409191994,0.766320474777448,0.7668893838158871,0.7674591381872214,0.7680297397769517,0.7686011904761905,0.7691734921816828,0.7697466467958272,0.7703206562266965,0.7708955223880597,0.7714712471994025,0.7720478325859492,0.7726252804786836,0.7732035928143712,0.7737827715355805,0.7743628185907047,0.7749437359339835,0.7755255255255256,0.7761081893313299,0.7766917293233083,0.7772761474793077,0.7778614457831325,0.7784476262245666,0.7790346907993967,0.779622641509434,0.7802114803625377,0.780801209372638,0.7813918305597579,0.7819833459500378,0.7825757575757576,0.7831690674753601,0.783763277693475,0.7843583902809416,0.7849544072948328,0.785551330798479,0.7861491628614916,0.7867479055597868,0.7873475609756098,0.7879481311975591,0.7885496183206107,0.7891520244461421,0.7897553516819572,0.7903596021423106,0.7909647779479326,0.7915708812260537,0.7921779141104295,0.7927858787413661,0.793394777265745,0.7940046118370484,0.7946153846153846,0.7952270977675134,0.7958397534668721,0.7964533538936006,0.7970679012345679,0.7976833976833977,0.7982998454404946,0.7989172467130704,0.7995356037151703,0.8001549186676995,0.8007751937984496,0.8013964313421257,0.8020186335403726,0.8026418026418026,0.8032659409020217,0.8038910505836576,0.8045171339563862,0.8051441932969603,0.8057722308892356,0.8064012490241999,0.80703125,0.8076622361219703,0.8082942097026604,0.8089271730618638,0.8095611285266457,0.8101960784313725,0.8108320251177394,0.8114689709347996,0.8121069182389937,0.8127458693941778,0.8133858267716535,0.814026792750197,0.8146687697160884,0.8153117600631413,0.815955766192733,0.8166007905138339,0.817246835443038,0.8178939034045922,0.8185419968304279,0.8191911181601903,0.8198412698412698,0.8204924543288324,0.8211446740858506,0.8217979315831344,0.822452229299363,0.8231075697211155,0.8237639553429027,0.8244213886671987,0.8250798722044729,0.8257394084732215,0.8264,0.8270616493194556,0.8277243589743589,0.8283881315156375,0.8290529695024077,0.8297188755020081,0.8303858520900321,0.831053901850362,0.8317230273752013,0.8315874294923449,0.832258064516129,0.8329297820823245,0.8336025848142165,0.8342764753435732,0.8349514563106796,0.8356275303643724,0.8363047001620746,0.8369829683698297,0.8376623376623377,0.8390243902439024,0.8397070789259561,0.8403908794788274,0.8410757946210269,0.8417618270799347,0.8424489795918367,0.8431372549019608,0.8438266557645135,0.8445171849427169,0.8443898443898444,0.8450819672131148,0.8457752255947498,0.8456486042692939,0.8463434675431388,0.8470394736842105,0.8477366255144033,0.8484349258649094,0.8491343775762572,0.849009900990099,0.8497109826589595,0.8504132231404958,0.8511166253101737,0.8518211920529801,0.8525269262634632,0.8532338308457711,0.8539419087136929,0.8546511627906976,0.8553615960099751,0.8560732113144759,0.8567860116569526,0.8575,0.8582151793160967,0.8580968280467446,0.858813700918964,0.8595317725752508,0.8602510460251046,0.8609715242881072,0.8608549874266554,0.8615771812080537,0.8623005877413937,0.8630252100840337,0.8637510513036165,0.8644781144781145,0.8643639427127211,0.8650927487352446,0.8649789029535865,0.8648648648648649,0.8655959425190194,0.8663282571912013,0.8670618120237087,0.8677966101694915,0.8685326547921968,0.8692699490662139,0.8700084961767205,0.8707482993197279,0.8706382978723404,0.8713798977853492,0.8721227621483376,0.8720136518771331,0.8727583262169086,0.8735042735042735,0.8733960650128315,0.8732876712328768,0.8731790916880892,0.8730703259005146,0.8738197424892704,0.8737113402061856,0.8736027515047291,0.8743545611015491,0.8742463393626184,0.8741379310344828,0.8740293356341674,0.8747841105354058,0.874675885911841,0.8745674740484429,0.8753246753246753,0.8752166377816292,0.8759757155247181,0.8767361111111112,0.8766290182450044,0.8773913043478261,0.8781549173194082,0.8789198606271778,0.8796861377506539,0.8804537521815009,0.880349344978166,0.8811188811188811,0.8810148731408574,0.8817863397548161,0.8825591586327782,0.8833333333333333,0.884108867427568,0.8848857644991213,0.8847845206684257,0.8855633802816901,0.8854625550660793,0.8862433862433863,0.8870255957634599,0.8869257950530035,0.887709991158267,0.8876106194690265,0.8883968113374667,0.8891843971631206,0.8899733806566105,0.8907637655417406,0.8915555555555555,0.8923487544483986,0.8922528940338379,0.893048128342246,0.8929527207850134,0.89375,0.8945487042001787,0.8944543828264758,0.8943598925693823,0.8942652329749103,0.8941704035874439,0.8940754039497307,0.894878706199461,0.89568345323741,0.8964896489648965,0.8972972972972973,0.8981064021641119,0.898014440433213,0.8979223125564589,0.8987341772151899,0.8995475113122172,0.9003623188405797,0.900271985494107,0.9001814882032668,0.9000908265213442,0.9009090909090909,0.9017288444040037,0.9025500910746812,0.902461257976299,0.9023722627737226,0.9031963470319635,0.903107861060329,0.9030192131747484,0.9029304029304029,0.9028414298808433,0.9036697247706422,0.9035812672176309,0.9034926470588235,0.9043238270469182,0.9042357274401474,0.904147465437788,0.9040590405904059,0.9039704524469068,0.9038817005545287,0.9037927844588344,0.9037037037037037,0.9045412418906394,0.9044526901669759,0.9052924791086351,0.9061338289962825,0.9069767441860465,0.9078212290502793,0.907735321528425,0.9076492537313433,0.9084967320261438,0.908411214953271,0.9083255378858747,0.9082397003745318,0.9081537019681349,0.9090056285178236,0.9098591549295775,0.9107142857142857,0.9106302916274694,0.911487758945386,0.9123468426013195,0.9122641509433962,0.9121813031161473,0.9120982986767486,0.9120151371807,0.9119318181818182,0.9118483412322275,0.9117647058823529,0.9116809116809117,0.9115969581749049,0.9115128449096099,0.9114285714285715,0.9113441372735939,0.9122137404580153,0.9121298949379179,0.9130019120458891,0.9129186602870814,0.9128352490421456,0.912751677852349,0.9126679462571977,0.9125840537944284,0.9125,0.9133782483156881,0.9132947976878613,0.914175506268081,0.9140926640926641,0.9140096618357488,0.913926499032882,0.914811229428848,0.9156976744186046,0.9165858389912707,0.916504854368932,0.9164237123420796,0.9163424124513618,0.9172346640701071,0.9171539961013645,0.9170731707317074,0.9169921875,0.916911045943304,0.9168297455968689,0.9177277179236043,0.9176470588235294,0.9175662414131501,0.9174852652259332,0.9174041297935103,0.9173228346456693,0.9172413793103448,0.9171597633136095,0.9170779861796644,0.9179841897233202,0.9179030662710188,0.9188118811881189,0.9187314172447968,0.9196428571428571,0.9195630585898709,0.9194831013916501,0.9194029850746268,0.9203187250996016,0.9202392821535393,0.9201596806387226,0.9210789210789211,0.921,0.9209209209209209,0.9218436873747495,0.9227683049147443,0.9226907630522089,0.9226130653266331,0.9225352112676056,0.9224572004028198,0.9233870967741935,0.9233097880928355,0.9242424242424242,0.9241658240647118,0.9251012145748988,0.9250253292806484,0.9259634888438134,0.9269035532994924,0.926829268292683,0.9267548321464903,0.9276985743380856,0.9276248725790011,0.9275510204081633,0.9274770173646578,0.9284253578732107,0.9293756397134084,0.9293032786885246,0.9292307692307692,0.9291581108829569,0.9290853031860226,0.9290123456790124,0.9289392378990731,0.9298969072164949,0.9308565531475749,0.9318181818181818,0.9327817993795243,0.932712215320911,0.9326424870466321,0.9325726141078838,0.9325025960539979,0.9324324324324325,0.9323621227887617,0.9333333333333333,0.9332638164754953,0.9342379958246346,0.93521421107628,0.9351464435146444,0.9350785340314136,0.9350104821802935,0.9349422875131165,0.9348739495798319,0.9348054679284963,0.9347368421052632,0.934668071654373,0.9356540084388185,0.9355860612460402,0.9355179704016914,0.9365079365079365,0.9364406779661016,0.936373276776246,0.9363057324840764,0.9362380446333688,0.9361702127659575,0.9361022364217252,0.9360341151385928,0.935965848452508,0.9358974358974359,0.9368983957219251,0.936830835117773,0.9367631296891747,0.9366952789699571,0.9366272824919442,0.9365591397849462,0.9364908503767492,0.9364224137931034,0.9374325782092773,0.937365010799136,0.9383783783783783,0.9383116883116883,0.9382448537378115,0.9392624728850325,0.9391965255157437,0.9391304347826087,0.940152339499456,0.9400871459694989,0.9411123227917121,0.9410480349344978,0.940983606557377,0.9409190371991247,0.940854326396495,0.9407894736842105,0.9407244785949506,0.9406593406593406,0.9405940594059405,0.9405286343612335,0.9404630650496141,0.9403973509933775,0.9403314917127071,0.9402654867256637,0.9401993355481728,0.9401330376940134,0.9400665926748057,0.94,0.9399332591768632,0.9398663697104677,0.939799331103679,0.9408482142857143,0.9407821229050279,0.941834451901566,0.9417693169092946,0.9428251121076233,0.9427609427609428,0.9426966292134832,0.9426321709786277,0.9425675675675675,0.9425028184892897,0.9435665914221218,0.943502824858757,0.9434389140271493,0.9445073612684032,0.9444444444444444,0.9443813847900113,0.9443181818181818,0.944254835039818,0.9441913439635535,0.9441277080957811,0.9440639269406392,0.944,0.9450800915331807,0.9450171821305842,0.944954128440367,0.9448909299655568,0.9448275862068966,0.9447640966628308,0.945852534562212,0.9457900807381776,0.9457274826789839,0.945664739884393,0.9456018518518519,0.9455388180764774,0.9454756380510441,0.9465737514518002,0.9476744186046512,0.9476135040745053,0.9475524475524476,0.9474912485414235,0.947429906542056,0.9473684210526315,0.9473067915690867,0.9472450175849941,0.9471830985915493,0.9471210340775558,0.9470588235294117,0.9469964664310954,0.9481132075471698,0.948051948051948,0.9479905437352246,0.9479289940828403,0.9478672985781991,0.9478054567022538,0.9477434679334917,0.9476813317479191,0.9476190476190476,0.9475566150178785,0.9474940334128878,0.9474313022700119,0.9473684210526315,0.9473053892215569,0.947242206235012,0.9471788715486195,0.9471153846153846,0.9470517448856799,0.946987951807229,0.9481302774427021,0.9480676328502415,0.9480048367593712,0.9479418886198547,0.9478787878787879,0.9478155339805825,0.9477521263669502,0.948905109489051,0.9488428745432399,0.948780487804878,0.9487179487179487,0.9486552567237164,0.9498164014687882,0.9497549019607843,0.9496932515337423,0.9496314496314496,0.949569495694957,0.9507389162561576,0.9506781750924784,0.9506172839506173,0.9505562422744128,0.9504950495049505,0.9504337050805453,0.9503722084367245,0.9503105590062112,0.9502487562189055,0.9514321295143213,0.9526184538653366,0.9525593008739076,0.9525,0.9524405506883604,0.9523809523809523,0.9523212045169385,0.9522613065326633,0.9534591194968554,0.9534005037783375,0.9533417402269861,0.9532828282828283,0.9532237673830595,0.9531645569620253,0.9531051964512041,0.9530456852791879,0.9529860228716646,0.9529262086513995,0.9528662420382166,0.9528061224489796,0.9527458492975734,0.9526854219948849,0.9526248399487837,0.9525641025641025,0.9537869062901155,0.9537275064267352,0.9536679536679536,0.9536082474226805,0.9535483870967741,0.9534883720930233,0.9547218628719275,0.9559585492227979,0.9559014267185474,0.9558441558441558,0.9557867360208062,0.9557291666666666,0.9556714471968709,0.9556135770234987,0.9555555555555556,0.9568062827225131,0.9567496723460026,0.9566929133858267,0.9579500657030223,0.9578947368421052,0.9578392621870883,0.9577836411609498,0.9577278731836195,0.9576719576719577,0.9576158940397351,0.9575596816976127,0.9575033200531209,0.9574468085106383,0.9573901464713716,0.9573333333333334,0.9572763684913218,0.9585561497326203,0.9585006693440429,0.9597855227882037,0.959731543624161,0.9596774193548387,0.9596231493943472,0.9595687331536388,0.9608636977058029,0.9608108108108108,0.9607577807848444,0.9607046070460704,0.9606512890094979,0.9605978260869565,0.9619047619047619,0.9618528610354223,0.9618008185538881,0.9617486338797814,0.9616963064295485,0.963013698630137,0.9629629629629629,0.9642857142857143,0.9642365887207703,0.9641873278236914,0.9641379310344828,0.9654696132596685,0.9654218533886584,0.9653739612188366,0.9653259361997226,0.9652777777777778,0.9652294853963839,0.9651810584958217,0.9651324965132496,0.9650837988826816,0.965034965034965,0.9649859943977591,0.9649368863955119,0.9648876404494382,0.9662447257383966,0.9661971830985916,0.9661495063469676,0.9661016949152542,0.9660537482319661,0.9660056657223796,0.9659574468085106,0.9659090909090909,0.9658605974395448,0.9658119658119658,0.9671897289586305,0.9671428571428572,0.9670958512160229,0.9670487106017192,0.9670014347202296,0.9669540229885057,0.9669064748201439,0.9668587896253602,0.9668109668109668,0.9667630057803468,0.9667149059334298,0.9666666666666667,0.9666182873730044,0.9665697674418605,0.9679767103347889,0.967930029154519,0.9678832116788321,0.9678362573099415,0.9677891654465594,0.967741935483871,0.9676945668135095,0.9676470588235294,0.96759941089838,0.967551622418879,0.9675036927621861,0.9674556213017751,0.9674074074074074,0.9673590504451038,0.9673105497771174,0.9672619047619048,0.9672131147540983,0.9671641791044776,0.9671150971599403,0.9670658682634731,0.967016491754123,0.9669669669669669,0.9669172932330827,0.9668674698795181,0.9668174962292609,0.9667673716012085,0.9667170953101362,0.9666666666666667,0.9666160849772383,0.9665653495440729,0.9665144596651446,0.9664634146341463,0.966412213740458,0.9663608562691132,0.9663093415007658,0.9662576687116564,0.9662058371735791,0.9661538461538461,0.9661016949152542,0.9660493827160493,0.9659969088098919,0.9659442724458205,0.9658914728682171,0.9658385093167702,0.9657853810264385,0.9657320872274143,0.9656786271450858,0.965625,0.9655712050078247,0.9655172413793104,0.9654631083202512,0.9654088050314465,0.9653543307086614,0.9652996845425867,0.9652448657187994,0.9651898734177216,0.96513470681458,0.9650793650793651,0.9650238473767886,0.964968152866242,0.9649122807017544,0.9648562300319489,0.9648,0.9647435897435898,0.9646869983948636,0.9646302250803859,0.966183574879227,0.9661290322580646,0.9660743134087237,0.9660194174757282,0.965964343598055,0.9659090909090909,0.9658536585365853,0.9674267100977199,0.967373572593801,0.9673202614379085,0.967266775777414,0.9672131147540983,0.9671592775041051,0.9671052631578947,0.9670510708401977,0.9686468646864687,0.968595041322314,0.9685430463576159,0.9684908789386402,0.96843853820598,0.9683860232945092,0.9683333333333334,0.9682804674457429,0.9682274247491639,0.9681742043551089,0.9681208053691275,0.9680672268907563,0.968013468013468,0.9679595278246206,0.9679054054054054,0.9678510998307953,0.9677966101694915,0.967741935483871,0.967687074829932,0.9676320272572402,0.9675767918088737,0.9675213675213675,0.9691780821917808,0.9691252144082333,0.9690721649484536,0.9690189328743546,0.9706896551724138,0.9706390328151986,0.9705882352941176,0.9705372616984402,0.9704861111111112,0.9704347826086956,0.9703832752613241,0.9703315881326352,0.9702797202797203,0.9702276707530648,0.9701754385964912,0.9701230228471002,0.9700704225352113,0.9700176366843033,0.9699646643109541,0.9699115044247788,0.9698581560283688,0.9698046181172292,0.9697508896797153,0.9696969696969697,0.9696428571428571,0.9695885509838998,0.9695340501792115,0.9694793536804309,0.9694244604316546,0.9693693693693693,0.9693140794223827,0.969258589511754,0.9692028985507246,0.969147005444646,0.9690909090909091,0.9690346083788707,0.968978102189781,0.9689213893967094,0.9688644688644689,0.9688073394495413,0.96875,0.9686924493554327,0.9704797047970479,0.9704251386321626,0.9703703703703703,0.9703153988868275,0.9702602230483272,0.9702048417132216,0.9701492537313433,0.9700934579439252,0.9700374531835206,0.9718574108818011,0.9718045112781954,0.9717514124293786,0.9716981132075472,0.9716446124763705,0.9715909090909091,0.9715370018975332,0.9714828897338403,0.9714285714285714,0.9713740458015268,0.97131931166348,0.9712643678160919,0.9712092130518234,0.9711538461538461,0.9710982658959537,0.971042471042471,0.9709864603481625,0.9728682170542635,0.9728155339805825,0.9727626459143969,0.9727095516569201,0.97265625,0.9726027397260274,0.9725490196078431,0.9724950884086444,0.9724409448818898,0.9723865877712031,0.9723320158102767,0.9722772277227723,0.9722222222222222,0.9721669980119284,0.9721115537848606,0.9720558882235529,0.972,0.9719438877755511,0.9738955823293173,0.9738430583501007,0.9737903225806451,0.9737373737373738,0.9736842105263158,0.973630831643002,0.9735772357723578,0.9735234215885947,0.9755102040816327,0.9754601226993865,0.9754098360655737,0.9753593429158111,0.9753086419753086,0.9752577319587629,0.9752066115702479,0.9751552795031055,0.975103734439834,0.975051975051975,0.975,0.9749478079331941,0.9748953974895398,0.9748427672955975,0.9747899159663865,0.9747368421052631,0.9746835443037974,0.9746300211416491,0.9745762711864406,0.9766454352441614,0.9765957446808511,0.976545842217484,0.9764957264957265,0.9764453961456103,0.9763948497854077,0.9763440860215054,0.9762931034482759,0.9762419006479481,0.9761904761904762,0.9761388286334056,0.9760869565217392,0.9760348583877996,0.9759825327510917,0.975929978118162,0.9758771929824561,0.9758241758241758,0.9757709251101322,0.9757174392935982,0.9756637168141593,0.975609756097561,0.9755555555555555,0.9755011135857461,0.9776785714285714,0.9776286353467561,0.9775784753363229,0.9775280898876404,0.9774774774774775,0.9774266365688488,0.9773755656108597,0.9773242630385488,0.9772727272727273,0.9772209567198178,0.9771689497716894,0.977116704805492,0.9770642201834863,0.9770114942528736,0.9769585253456221,0.976905311778291,0.9768518518518519,0.9767981438515081,0.9767441860465116,0.9766899766899767,0.9766355140186916,0.9765807962529274,0.9765258215962441,0.9764705882352941,0.9764150943396226,0.9763593380614657,0.976303317535545,0.9762470308788599,0.9785714285714285,0.9785202863961814,0.9784688995215312,0.9784172661870504,0.9783653846153846,0.9783132530120482,0.9782608695652174,0.9782082324455206,0.9781553398058253,0.9780487804878049,0.9779951100244498,0.9778869778869779,0.9778325123152709,0.9777777777777777,0.9777227722772277,0.9776674937965261,0.9776119402985075,0.9775561097256857,0.9775,0.9774436090225563,0.9773869346733668,0.9773299748110831,0.9772727272727273,0.9772151898734177,0.9771573604060914,0.9770992366412213,0.9770408163265306,0.9769820971867008,0.9769230769230769,0.9768637532133676,0.9768041237113402,0.9767441860465116,0.9766839378238342,0.9766233766233766,0.9765625,0.9765013054830287,0.9764397905759162,0.9763779527559056,0.9763157894736842,0.9762532981530343,0.9761904761904762,0.9761273209549072,0.976063829787234,0.976,0.9759358288770054,0.9758713136729222,0.9758064516129032,0.9757412398921833,0.9756756756756757,0.975609756097561,0.9755434782608695,0.9754768392370572,0.9754098360655737,0.9753424657534246,0.9752747252747253,0.9752066115702479,0.9751381215469613,0.9750692520775623,0.975,0.9749303621169917,0.9776536312849162,0.9775910364145658,0.9775280898876404,0.9774647887323944,0.9774011299435028,0.9773371104815864,0.9772727272727273,0.9772079772079773,0.9771428571428571,0.9770773638968482,0.9770114942528736,0.9769452449567724,0.976878612716763,0.9768115942028985,0.9767441860465116,0.9766763848396501,0.9766081871345029,0.9765395894428153,0.9764705882352941,0.976401179941003,0.9763313609467456,0.9762611275964391,0.9761904761904762,0.9761194029850746,0.9760479041916168,0.975975975975976,0.9759036144578314,0.9758308157099698,0.9757575757575757,0.9756838905775076,0.975609756097561,0.9755351681957186,0.9754601226993865,0.9753846153846154,0.9753086419753086,0.9752321981424149,0.9751552795031055,0.9750778816199377,0.975,0.9749216300940439,0.9748427672955975,0.9747634069400631,0.9746835443037974,0.9746031746031746,0.9777070063694268,0.9776357827476039,0.9775641025641025,0.977491961414791,0.9774193548387097,0.9773462783171522,0.9772727272727273,0.9771986970684039,0.9771241830065359,0.9770491803278688,0.9769736842105263,0.976897689768977,0.9768211920529801,0.9767441860465116,0.9766666666666667,0.9765886287625418,0.9765100671140939,0.9764309764309764,0.9763513513513513,0.9761904761904762,0.9761092150170648,0.976027397260274,0.9759450171821306,0.9758620689655172,0.9757785467128027,0.9756944444444444,0.975609756097561,0.9755244755244755,0.9754385964912281,0.9753521126760564,0.9752650176678446,0.975177304964539,0.9750889679715302,0.975,0.974910394265233,0.9748201438848921,0.9747292418772563,0.9746376811594203,0.9745454545454545,0.9744525547445255,0.9743589743589743,0.9742647058823529,0.974169741697417,0.9740740740740741,0.9739776951672863,0.9738805970149254,0.9737827715355806,0.9736842105263158,0.9735849056603774,0.9734848484848485,0.973384030418251,0.9732824427480916,0.9731800766283525,0.9730769230769231,0.972972972972973,0.9728682170542635,0.9727626459143969,0.97265625,0.9725490196078431,0.9724409448818898,0.9723320158102767,0.9722222222222222,0.9721115537848606,0.972,0.9718875502008032,0.9717741935483871,0.97165991902834,0.9715447154471545,0.9714285714285714,0.9713114754098361,0.9711934156378601,0.9710743801652892,0.975103734439834,0.975,0.9748953974895398,0.9747899159663865,0.9746835443037974,0.9745762711864406,0.9744680851063829,0.9743589743589743,0.9742489270386266,0.9741379310344828,0.974025974025974,0.9739130434782609,0.9737991266375546,0.9736842105263158,0.973568281938326,0.9734513274336283,0.9733333333333334,0.9732142857142857,0.9730941704035875,0.972972972972973,0.9728506787330317,0.9727272727272728,0.9726027397260274,0.9723502304147466,0.9722222222222222,0.9720930232558139,0.9719626168224299,0.971830985915493,0.9716981132075472,0.9715639810426541,0.9714285714285714,0.9712918660287081,0.9711538461538461,0.9710144927536232,0.970873786407767,0.9707317073170731,0.9705882352941176,0.9704433497536946,0.9702970297029703,0.9701492537313433,0.97,0.9698492462311558,0.9696969696969697,0.9695431472081218,0.9693877551020408,0.9692307692307692,0.9690721649484536,0.9689119170984456,0.96875,0.9685863874345549,0.968421052631579,0.9682539682539683,0.9680851063829787,0.9679144385026738,0.967741935483871,0.9675675675675676,0.967391304347826,0.9672131147540983,0.967032967032967,0.9668508287292817,0.9666666666666667,0.9664804469273743,0.9662921348314607,0.9661016949152542,0.9659090909090909,0.9657142857142857,0.9655172413793104,0.9653179190751445,0.9651162790697675,0.9649122807017544,0.9647058823529412,0.9644970414201184,0.9642857142857143,0.9640718562874252,0.963855421686747,0.9636363636363636,0.9634146341463414,0.9631901840490797,0.9629629629629629,0.9627329192546584,0.9625,0.9622641509433962,0.9620253164556962,0.9617834394904459,0.9615384615384616,0.9612903225806452,0.961038961038961,0.9607843137254902,0.9605263157894737,0.9668874172185431,0.9666666666666667,0.9664429530201343,0.9662162162162162,0.9659863945578231,0.9657534246575342,0.9655172413793104,0.9652777777777778,0.965034965034965,0.9647887323943662,0.9645390070921985,0.9642857142857143,0.9640287769784173,0.9637681159420289,0.9635036496350365,0.9632352941176471,0.9629629629629629,0.9626865671641791,0.9624060150375939,0.9621212121212122,0.9618320610687023,0.9615384615384616,0.9612403100775194,0.9609375,0.9606299212598425,0.9603174603174603,0.96,0.9596774193548387,0.959349593495935,0.9590163934426229,0.9586776859504132,0.9583333333333334,0.957983193277311,0.9576271186440678,0.9572649572649573,0.9568965517241379,0.9565217391304348,0.956140350877193,0.9557522123893806,0.9553571428571429,0.954954954954955,0.9545454545454546,0.9541284403669725,0.9537037037037037,0.9532710280373832,0.9528301886792453,0.9523809523809523,0.9519230769230769,0.9514563106796117,0.9509803921568627,0.9504950495049505,0.95,0.9494949494949495,0.9489795918367347,0.9484536082474226,0.9479166666666666,0.9468085106382979,0.946236559139785,0.9456521739130435,0.945054945054945,0.9444444444444444,0.9438202247191011,0.9431818181818182,0.9425287356321839,0.9418604651162791,0.9411764705882353,0.9404761904761905,0.9397590361445783,0.9390243902439024,0.9382716049382716,0.9375,0.9367088607594937,0.9358974358974359,0.935064935064935,0.9342105263157895,0.9333333333333333,0.9324324324324325,0.9315068493150684,0.9305555555555556,0.9295774647887324,0.9285714285714286,0.927536231884058,0.9264705882352942,0.9253731343283582,0.9242424242424242,0.9230769230769231,0.921875,0.9206349206349206,0.9193548387096774,0.9344262295081968,0.95,0.9491525423728814,0.9482758620689655,0.9473684210526315,0.9464285714285714,0.9454545454545454,0.9444444444444444,0.9433962264150944,0.9423076923076923,0.9411764705882353,0.94,0.9387755102040817,0.9583333333333334,0.9574468085106383,0.9565217391304348,0.9555555555555556,0.9545454545454546,0.9534883720930233,0.9523809523809523,0.9512195121951219,0.95,0.9487179487179487,0.9473684210526315,0.9459459459459459,0.9444444444444444,0.9428571428571428,0.9705882352941176,0.9696969696969697,0.96875,0.967741935483871,0.9655172413793104,0.9642857142857143,0.9629629629629629,0.9615384615384616,0.96,0.9583333333333334,0.9565217391304348,0.9545454545454546,0.9523809523809523,0.95,0.9473684210526315,0.9444444444444444,0.9411764705882353,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0],\"yaxis\":\"y\",\"type\":\"scatter\"}],                        {\"template\":{\"data\":{\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"white\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"white\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"#C8D4E3\",\"linecolor\":\"#C8D4E3\",\"minorgridcolor\":\"#C8D4E3\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"#C8D4E3\",\"linecolor\":\"#C8D4E3\",\"minorgridcolor\":\"#C8D4E3\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"choropleth\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"choropleth\"}],\"contourcarpet\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"contourcarpet\"}],\"contour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"contour\"}],\"heatmapgl\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmapgl\"}],\"heatmap\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmap\"}],\"histogram2dcontour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2dcontour\"}],\"histogram2d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2d\"}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"mesh3d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"mesh3d\"}],\"parcoords\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"parcoords\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}],\"scatter3d\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatter3d\"}],\"scattercarpet\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattercarpet\"}],\"scattergeo\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergeo\"}],\"scattergl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergl\"}],\"scattermapbox\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattermapbox\"}],\"scatterpolargl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolargl\"}],\"scatterpolar\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolar\"}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"scatterternary\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterternary\"}],\"surface\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"surface\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}]},\"layout\":{\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"autotypenumbers\":\"strict\",\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]],\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]},\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"geo\":{\"bgcolor\":\"white\",\"lakecolor\":\"white\",\"landcolor\":\"white\",\"showlakes\":true,\"showland\":true,\"subunitcolor\":\"#C8D4E3\"},\"hoverlabel\":{\"align\":\"left\"},\"hovermode\":\"closest\",\"mapbox\":{\"style\":\"light\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"white\",\"polar\":{\"angularaxis\":{\"gridcolor\":\"#EBF0F8\",\"linecolor\":\"#EBF0F8\",\"ticks\":\"\"},\"bgcolor\":\"white\",\"radialaxis\":{\"gridcolor\":\"#EBF0F8\",\"linecolor\":\"#EBF0F8\",\"ticks\":\"\"}},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"white\",\"gridcolor\":\"#DFE8F3\",\"gridwidth\":2,\"linecolor\":\"#EBF0F8\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"#EBF0F8\"},\"yaxis\":{\"backgroundcolor\":\"white\",\"gridcolor\":\"#DFE8F3\",\"gridwidth\":2,\"linecolor\":\"#EBF0F8\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"#EBF0F8\"},\"zaxis\":{\"backgroundcolor\":\"white\",\"gridcolor\":\"#DFE8F3\",\"gridwidth\":2,\"linecolor\":\"#EBF0F8\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"#EBF0F8\"}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"ternary\":{\"aaxis\":{\"gridcolor\":\"#DFE8F3\",\"linecolor\":\"#A2B1C6\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"#DFE8F3\",\"linecolor\":\"#A2B1C6\",\"ticks\":\"\"},\"bgcolor\":\"white\",\"caxis\":{\"gridcolor\":\"#DFE8F3\",\"linecolor\":\"#A2B1C6\",\"ticks\":\"\"}},\"title\":{\"x\":0.05},\"xaxis\":{\"automargin\":true,\"gridcolor\":\"#EBF0F8\",\"linecolor\":\"#EBF0F8\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"#EBF0F8\",\"zerolinewidth\":2},\"yaxis\":{\"automargin\":true,\"gridcolor\":\"#EBF0F8\",\"linecolor\":\"#EBF0F8\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"#EBF0F8\",\"zerolinewidth\":2}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"Recall\"},\"constrain\":\"domain\"},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"Precision\"},\"scaleanchor\":\"x\",\"scaleratio\":1},\"legend\":{\"tracegroupgap\":0},\"title\":{\"text\":\"Precision-Recall Curve (AUC=0.9211)\"},\"height\":600,\"width\":800,\"shapes\":[{\"line\":{\"dash\":\"dash\"},\"type\":\"line\",\"x0\":0,\"x1\":1,\"y0\":1,\"y1\":0}],\"showlegend\":false},                        {\"responsive\": true}                    ).then(function(){\n","                            \n","var gd = document.getElementById('5c6e1325-ae8e-40a4-88b4-90aed2c124f2');\n","var x = new MutationObserver(function (mutations, observer) {{\n","        var display = window.getComputedStyle(gd).display;\n","        if (!display || display === 'none') {{\n","            console.log([gd, 'removed!']);\n","            Plotly.purge(gd);\n","            observer.disconnect();\n","        }}\n","}});\n","\n","// Listen for the removal of the full notebook cells\n","var notebookContainer = gd.closest('#notebook-container');\n","if (notebookContainer) {{\n","    x.observe(notebookContainer, {childList: true});\n","}}\n","\n","// Listen for the clearing of the current output cell\n","var outputEl = gd.closest('.output');\n","if (outputEl) {{\n","    x.observe(outputEl, {childList: true});\n","}}\n","\n","                        })                };                });            </script>        </div>"]},"metadata":{},"output_type":"display_data"}],"source":["target_score = xgb_clf.predict_proba(features_valid)[:, 1]\n","\n","fpr, tpr, thresholds = roc_curve(target_valid, target_score)\n","\n","fig = px.area(\n","    x=fpr, y=tpr,\n","    title=f'ROC Curve (AUC={auc(fpr, tpr):.4f})',\n","    labels=dict(x='False Positive Rate', y='True Positive Rate'),\n","    width=800, height=600\n",")\n","fig.add_shape(\n","    type='line', line=dict(dash='dash'),\n","    x0=0, x1=1, y0=0, y1=1\n",")\n","\n","fig.update_yaxes(scaleanchor=\"x\", scaleratio=1)\n","fig.update_xaxes(constrain='domain')\n","fig.update_layout(showlegend=False)\n","fig.show()\n","\n","precision, recall, thresholds = precision_recall_curve(target_valid, target_score)\n","\n","fig = px.area(\n","    x=recall, y=precision,\n","    title=f'Precision-Recall Curve (AUC={auc(fpr, tpr):.4f})',\n","    labels=dict(x='Recall', y='Precision'),\n","    width=800, height=600\n",")\n","fig.add_shape(\n","    type='line', line=dict(dash='dash'),\n","    x0=0, x1=1, y0=1, y1=0\n",")\n","fig.update_yaxes(scaleanchor=\"x\", scaleratio=1)\n","fig.update_xaxes(constrain='domain')\n","fig.update_layout(showlegend=False)\n","fig.show()"]},{"cell_type":"markdown","metadata":{},"source":["--------"]},{"cell_type":"markdown","metadata":{},"source":["# Final Evaluation"]},{"cell_type":"markdown","metadata":{},"source":["`Model Selection - Average Score Comparison`\n","\n","`Note`\n","\n","- We've selected LightGBM as our optimal model based on score comparisons, it also performs much better compared to other models when it comes to scoring and speed. XGBClassifier is similar to our LightGBM model but takes considerably longer to compute."]},{"cell_type":"code","execution_count":137,"metadata":{"execution":{"iopub.execute_input":"2023-11-30T17:05:07.620683Z","iopub.status.busy":"2023-11-30T17:05:07.620368Z","iopub.status.idle":"2023-11-30T17:05:07.627835Z","shell.execute_reply":"2023-11-30T17:05:07.626805Z","shell.execute_reply.started":"2023-11-30T17:05:07.620644Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Average Cross Validation Scores:\n","RandomForestClassifier: 0.8760777711552042\n","DecisionTreeClassifier: 0.7530509062867848\n","ExtraTreesClassifier: 0.8479972754101266\n","\n","LogisticRegression: 0.829162458233739\n","RidgeClassifier: 0.8259667628691291\n","\n","LightGBM: 0.9078685635877259\n","XGBoost: 0.908791215308284\n"]}],"source":["print('Average Cross Validation Scores:')\n","print('RandomForestClassifier: {}'.format(forest_scores.mean()))\n","print('DecisionTreeClassifier: {}'.format(tree_scores.mean()))\n","print('ExtraTreesClassifier: {}'.format(extra_trees_scores.mean()))\n","print()\n","print('LogisticRegression: {}'.format(log_scores.mean()))\n","print('RidgeClassifier: {}'.format(ridge_scores.mean()))\n","print()\n","print('LightGBM: {}'.format(lgb_scores.mean()))\n","print('XGBoost: {}'.format(xgb_scores.mean()))"]},{"cell_type":"markdown","metadata":{},"source":["`Model Selection - Feature Importances`"]},{"cell_type":"code","execution_count":138,"metadata":{"execution":{"iopub.execute_input":"2023-11-30T17:05:07.629176Z","iopub.status.busy":"2023-11-30T17:05:07.628927Z","iopub.status.idle":"2023-11-30T17:05:07.659305Z","shell.execute_reply":"2023-11-30T17:05:07.658339Z","shell.execute_reply.started":"2023-11-30T17:05:07.629154Z"},"trusted":true},"outputs":[{"data":{"application/vnd.plotly.v1+json":{"config":{"plotlyServerURL":"https://plot.ly"},"data":[{"fill":"toself","r":[580,493,262,171,117,75,51,37,34,33],"theta":["total_charges","monthly_charges","begin_month","begin_year","begin_dayofweek","payment_method","contract_type","gender","partner","online_backup"],"type":"scatterpolar"}],"layout":{"polar":{"angularaxis":{"direction":"clockwise","period":6},"radialaxis":{"angle":-45}},"template":{"data":{"bar":[{"error_x":{"color":"#2a3f5f"},"error_y":{"color":"#2a3f5f"},"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"barpolar":[{"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"carpet":[{"aaxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"baxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"type":"carpet"}],"choropleth":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"choropleth"}],"contour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"contour"}],"contourcarpet":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"contourcarpet"}],"heatmap":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmap"}],"heatmapgl":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmapgl"}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"histogram2d":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2d"}],"histogram2dcontour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2dcontour"}],"mesh3d":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"mesh3d"}],"parcoords":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"parcoords"}],"pie":[{"automargin":true,"type":"pie"}],"scatter":[{"fillpattern":{"fillmode":"overlay","size":10,"solidity":0.2},"type":"scatter"}],"scatter3d":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter3d"}],"scattercarpet":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattercarpet"}],"scattergeo":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergeo"}],"scattergl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergl"}],"scattermapbox":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattermapbox"}],"scatterpolar":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolar"}],"scatterpolargl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolargl"}],"scatterternary":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterternary"}],"surface":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"surface"}],"table":[{"cells":{"fill":{"color":"#EBF0F8"},"line":{"color":"white"}},"header":{"fill":{"color":"#C8D4E3"},"line":{"color":"white"}},"type":"table"}]},"layout":{"annotationdefaults":{"arrowcolor":"#2a3f5f","arrowhead":0,"arrowwidth":1},"autotypenumbers":"strict","coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]],"sequential":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"sequentialminus":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]]},"colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#2a3f5f"},"geo":{"bgcolor":"white","lakecolor":"white","landcolor":"#E5ECF6","showlakes":true,"showland":true,"subunitcolor":"white"},"hoverlabel":{"align":"left"},"hovermode":"closest","mapbox":{"style":"light"},"paper_bgcolor":"white","plot_bgcolor":"#E5ECF6","polar":{"angularaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","radialaxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"scene":{"xaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"yaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"zaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"}},"shapedefaults":{"line":{"color":"#2a3f5f"}},"ternary":{"aaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"baxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","caxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"title":{"x":0.05},"xaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2},"yaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2}}},"title":{"text":"Top 10 Most Important Features - LightGBM"}}},"text/html":["<div>                            <div id=\"f855fc9e-dd2b-4d7a-9083-dae6882118f0\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                require([\"plotly\"], function(Plotly) {                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"f855fc9e-dd2b-4d7a-9083-dae6882118f0\")) {                    Plotly.newPlot(                        \"f855fc9e-dd2b-4d7a-9083-dae6882118f0\",                        [{\"r\":[580,493,262,171,117,75,51,37,34,33],\"theta\":[\"total_charges\",\"monthly_charges\",\"begin_month\",\"begin_year\",\"begin_dayofweek\",\"payment_method\",\"contract_type\",\"gender\",\"partner\",\"online_backup\"],\"type\":\"scatterpolar\",\"fill\":\"toself\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"polar\":{\"radialaxis\":{\"angle\":-45},\"angularaxis\":{\"direction\":\"clockwise\",\"period\":6}},\"title\":{\"text\":\"Top 10 Most Important Features - LightGBM\"}},                        {\"responsive\": true}                    ).then(function(){\n","                            \n","var gd = document.getElementById('f855fc9e-dd2b-4d7a-9083-dae6882118f0');\n","var x = new MutationObserver(function (mutations, observer) {{\n","        var display = window.getComputedStyle(gd).display;\n","        if (!display || display === 'none') {{\n","            console.log([gd, 'removed!']);\n","            Plotly.purge(gd);\n","            observer.disconnect();\n","        }}\n","}});\n","\n","// Listen for the removal of the full notebook cells\n","var notebookContainer = gd.closest('#notebook-container');\n","if (notebookContainer) {{\n","    x.observe(notebookContainer, {childList: true});\n","}}\n","\n","// Listen for the clearing of the current output cell\n","var outputEl = gd.closest('.output');\n","if (outputEl) {{\n","    x.observe(outputEl, {childList: true});\n","}}\n","\n","                        })                };                });            </script>        </div>"]},"metadata":{},"output_type":"display_data"}],"source":["# series containing feature importances from the model and feature names from the training data\n","# feature_importances = pd.Series(best_lgb.feature_importances_, index=features_train.columns).sort_values(ascending=False)\n","\n","# simple bar chart\n","# feature_importances.plot.bar();\n","# print(feature_importances.nlargest(10))\n","\n","fig = go.Figure(data=go.Scatterpolar(r = [580, 493,262,171,117,75,51,37,34,33],\n","               theta= ['total_charges','monthly_charges','begin_month','begin_year', 'begin_dayofweek', 'payment_method', 'contract_type',\n","                       'gender','partner', 'online_backup']))\n","\n","fig.update_traces(fill='toself')\n","fig.update_layout(\n","polar=dict(\n","radialaxis_angle=-45,\n","angularaxis= dict(\n","direction='clockwise',\n","period=6)))\n","\n","fig.update_layout(title_text='Top 10 Most Important Features - LightGBM')\n","fig.show()"]},{"cell_type":"markdown","metadata":{},"source":["`Model Selection - Confusion Matrix`\n"]},{"cell_type":"code","execution_count":139,"metadata":{"execution":{"iopub.execute_input":"2023-11-30T17:05:07.660752Z","iopub.status.busy":"2023-11-30T17:05:07.660454Z","iopub.status.idle":"2023-11-30T17:05:07.682843Z","shell.execute_reply":"2023-11-30T17:05:07.681934Z","shell.execute_reply.started":"2023-11-30T17:05:07.660729Z"},"trusted":true},"outputs":[{"data":{"application/vnd.plotly.v1+json":{"config":{"plotlyServerURL":"https://plot.ly"},"data":[{"colorscale":[[0,"rgb(247,251,255)"],[0.125,"rgb(222,235,247)"],[0.25,"rgb(198,219,239)"],[0.375,"rgb(158,202,225)"],[0.5,"rgb(107,174,214)"],[0.625,"rgb(66,146,198)"],[0.75,"rgb(33,113,181)"],[0.875,"rgb(8,81,156)"],[1,"rgb(8,48,107)"]],"type":"heatmap","x":["Class: 0","Class: 1"],"y":["Class: 0","Class: 1"],"z":[[244,130],[35,998]]}],"layout":{"template":{"data":{"bar":[{"error_x":{"color":"#2a3f5f"},"error_y":{"color":"#2a3f5f"},"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"barpolar":[{"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"carpet":[{"aaxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"baxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"type":"carpet"}],"choropleth":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"choropleth"}],"contour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"contour"}],"contourcarpet":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"contourcarpet"}],"heatmap":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmap"}],"heatmapgl":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmapgl"}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"histogram2d":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2d"}],"histogram2dcontour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2dcontour"}],"mesh3d":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"mesh3d"}],"parcoords":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"parcoords"}],"pie":[{"automargin":true,"type":"pie"}],"scatter":[{"fillpattern":{"fillmode":"overlay","size":10,"solidity":0.2},"type":"scatter"}],"scatter3d":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter3d"}],"scattercarpet":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattercarpet"}],"scattergeo":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergeo"}],"scattergl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergl"}],"scattermapbox":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattermapbox"}],"scatterpolar":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolar"}],"scatterpolargl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolargl"}],"scatterternary":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterternary"}],"surface":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"surface"}],"table":[{"cells":{"fill":{"color":"#EBF0F8"},"line":{"color":"white"}},"header":{"fill":{"color":"#C8D4E3"},"line":{"color":"white"}},"type":"table"}]},"layout":{"annotationdefaults":{"arrowcolor":"#2a3f5f","arrowhead":0,"arrowwidth":1},"autotypenumbers":"strict","coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]],"sequential":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"sequentialminus":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]]},"colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#2a3f5f"},"geo":{"bgcolor":"white","lakecolor":"white","landcolor":"#E5ECF6","showlakes":true,"showland":true,"subunitcolor":"white"},"hoverlabel":{"align":"left"},"hovermode":"closest","mapbox":{"style":"light"},"paper_bgcolor":"white","plot_bgcolor":"#E5ECF6","polar":{"angularaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","radialaxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"scene":{"xaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"yaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"zaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"}},"shapedefaults":{"line":{"color":"#2a3f5f"}},"ternary":{"aaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"baxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","caxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"title":{"x":0.05},"xaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2},"yaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2}}},"title":{"text":"Confusion Matrix"},"xaxis":{"title":{"text":"Predicted Label"}},"yaxis":{"title":{"text":"True Label"}}}},"text/html":["<div>                            <div id=\"bc77e41b-833e-48d7-933c-071a7d4592c9\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                require([\"plotly\"], function(Plotly) {                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"bc77e41b-833e-48d7-933c-071a7d4592c9\")) {                    Plotly.newPlot(                        \"bc77e41b-833e-48d7-933c-071a7d4592c9\",                        [{\"colorscale\":[[0.0,\"rgb(247,251,255)\"],[0.125,\"rgb(222,235,247)\"],[0.25,\"rgb(198,219,239)\"],[0.375,\"rgb(158,202,225)\"],[0.5,\"rgb(107,174,214)\"],[0.625,\"rgb(66,146,198)\"],[0.75,\"rgb(33,113,181)\"],[0.875,\"rgb(8,81,156)\"],[1.0,\"rgb(8,48,107)\"]],\"x\":[\"Class: 0\",\"Class: 1\"],\"y\":[\"Class: 0\",\"Class: 1\"],\"z\":[[244,130],[35,998]],\"type\":\"heatmap\"}],                        {\"title\":{\"text\":\"Confusion Matrix\"},\"xaxis\":{\"title\":{\"text\":\"Predicted Label\"}},\"yaxis\":{\"title\":{\"text\":\"True Label\"}},\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}}},                        {\"responsive\": true}                    ).then(function(){\n","                            \n","var gd = document.getElementById('bc77e41b-833e-48d7-933c-071a7d4592c9');\n","var x = new MutationObserver(function (mutations, observer) {{\n","        var display = window.getComputedStyle(gd).display;\n","        if (!display || display === 'none') {{\n","            console.log([gd, 'removed!']);\n","            Plotly.purge(gd);\n","            observer.disconnect();\n","        }}\n","}});\n","\n","// Listen for the removal of the full notebook cells\n","var notebookContainer = gd.closest('#notebook-container');\n","if (notebookContainer) {{\n","    x.observe(notebookContainer, {childList: true});\n","}}\n","\n","// Listen for the clearing of the current output cell\n","var outputEl = gd.closest('.output');\n","if (outputEl) {{\n","    x.observe(outputEl, {childList: true});\n","}}\n","\n","                        })                };                });            </script>        </div>"]},"metadata":{},"output_type":"display_data"}],"source":["# create the heatmap\n","\n","cm = confusion_matrix(target_valid, lgb_pred)\n","\n","labels = ['Class: 0', 'Class: 1']\n","heatmap = go.Heatmap(z=cm, x=labels, y=labels, colorscale='Blues')\n","\n","# create the layout\n","layout = go.Layout(title='Confusion Matrix', \n","                   xaxis=dict(title=\"Predicted Label\"),\n","                   yaxis=dict(title=\"True Label\"),\n","                   )\n","\n","# create the figure\n","fig = go.Figure(data=[heatmap], layout=layout)\n","\n","# fig = go.Figure(data=[heatmap], layout=layout)\n","        \n","# show the figure\n","fig.show()"]},{"cell_type":"markdown","metadata":{},"source":["`Model Selection - Final Evaluation`"]},{"cell_type":"markdown","metadata":{},"source":["`Note`\n","\n","After further review, given the ROC curve is calculated by taking each possible probability I've replaced my final ROC AUC Score with predictions using `.predict_proba()` which yields better results (91.56% vs 80.15% in the previously reviewed solution code)."]},{"cell_type":"code","execution_count":140,"metadata":{"execution":{"iopub.execute_input":"2023-11-30T17:05:07.684311Z","iopub.status.busy":"2023-11-30T17:05:07.683993Z","iopub.status.idle":"2023-11-30T17:05:07.817022Z","shell.execute_reply":"2023-11-30T17:05:07.816316Z","shell.execute_reply.started":"2023-11-30T17:05:07.684279Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Final evaluation on test set:\n","\n","[LightGBM] [Info] Number of positive: 3097, number of negative: 1121\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001238 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 570\n","[LightGBM] [Info] Number of data points in the train set: 4218, number of used features: 20\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.734234 -> initscore=1.016213\n","[LightGBM] [Info] Start training from score 1.016213\n","ROC AUC Score: 91.56 %\n"]}],"source":["print('Final evaluation on test set:')\n","print()\n","# model\n","final_model = lgb.LGBMClassifier(random_state=random_state)\n","# train\n","final_model.fit(features_train, target_train)\n","# predict\n","final_predictions_test = final_model.predict(features_test)\n","final_predictions_test_proba = final_model.predict_proba(features_test)[:, 1]\n","# accuracy check\n","# final_model_accuracy = accuracy_score(target_test, final_predictions_test)\n","# final_model_f1 = f1_score(target_test, final_predictions_test)\n","final_model_roc_auc = roc_auc_score(target_test, final_predictions_test_proba)\n","#result\n","print('ROC AUC Score:',   round((final_model_roc_auc * 100), 2),'%') "]},{"cell_type":"markdown","metadata":{},"source":["# Part 3: Solution Report"]},{"cell_type":"markdown","metadata":{},"source":["`What steps of the plan were performed and what steps were skipped (explain why)?`\n","- We performed most of the steps from Part 1 minus the following:\n","    - Time series analyses - this was not needed afterall our general EDA/preprocessing was sufficient but could have been performed to paint a picture on the sign-up dates for each client.\n","\n","`What difficulties did you encounter and how did you manage to solve them?`\n","- Initially, we found some slight difficulties performing the ideal process of scaling and taking into account class imbalancing in models. We took more time to read documentation and found our effective solutions. This was as simple as moving the order of our code, fitting our scaler on just the training dataset and leveraging `RepeatedStratifiedKFold()` in our `RandomizedSearchGridCV()` for each model.\n","\n","- There were also some minor road bumps with some of the missing data which was not perceptible at first glance. We ran an iterative function to find said discrepancies and select our method of handling such (replacing, filling in or removing).\n","\n","`What were some of the key steps to solving the task?`\n","- Putting the project plan together and understanding the business goals first and foremost.\n","- Choosing this scoring metric before the analysis, so there are no distractions when making task decisions.\n","    - Not using an absolute measure (accuracy) but a relative-to-each-class measure (like ROC AUC).\n","- Getting a grasp on what type of data we are working with (especifially the data imbalance point) and how it will be consumed by our multiple model choices.\n","- Taking into account some of the decisions we make in EDA/Preprocessing and how they will later impact our results. Noting such decisions as we go and leaving the door open for futher flexibility in case of issues (like easily being able to amend our choices if we hit a roadblock or being able revert back to original).\n","- On the modeling side of things, having a benchmark model scenario to compare to (a dummy model) as well as deploying any type of gridsearch so we can find optimal model parameters and tune accordingly. \n","- Cross comparing model scores to find the ideal model but also taking into account some of the other values such as computing speed/burden. A compromise will have to be made at times (*e.g. not always selecting the highest scoring model*)\n","\n","\n","`What is your final model and what quality score does it have?`\n","- Our final model is the LightGBM model given there was a good balance of a high `cross_val_score` and computing speed. This originally yielded us an AUC ROC score of 80.15% based on using `.predict()` but after futher review, we've deployed `.predict_proba()` in the score calculation which yielded us a 91.56% score on a new, imbalanced test dataset. The recommendation to Interconnect is to use LightGBM to predict the customer churn rate given the scoring, its speed and the further flexibility the model deploys if modifications are needed as data changes or evolves.\n"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":4018738,"sourceId":6991786,"sourceType":"datasetVersion"}],"dockerImageVersionId":30587,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.6"},"toc":{"base_numbering":1,"nav_menu":{},"number_sections":true,"sideBar":true,"skip_h1_title":true,"title_cell":"Table of Contents","title_sidebar":"Contents","toc_cell":false,"toc_position":{},"toc_section_display":true,"toc_window_display":false}},"nbformat":4,"nbformat_minor":4}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[In progress: The aim of this template is to provide structure for DS projects]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Brief"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Business Statement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requirements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Limitations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# importing flat files using NumPy\n",
    "filename = 'name.txt'\n",
    "data = np.loadtxt(filename, delimiter=',', skiprows=1, usecols=[0,2], dtype=str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# pickled files\n",
    "import pickle \n",
    "with open('name.pkl', 'rb') as file:\n",
    "    data = pickle.load(file)\n",
    "\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# importing excel spreadsheets\n",
    "file = 'name.xlsx'\n",
    "data = pd.ExcelFile(file)\n",
    "\n",
    "print(data.sheet_names)\n",
    "\n",
    "df1 = data.parse('sheet_name') # sheet name, as a string\n",
    "df2 = data.parse(0) # sheet index, as a float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# importing csv files\n",
    "data = pd.read_csv('name.csv')\n",
    "\n",
    "\"\"\"use the below for multiple extractions\n",
    "try:\n",
    "except:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# importing SAS files\n",
    "from sas7bdat import SAS7BDAT\n",
    "with SAS7BDAT('name.sas7bdat') as file:\n",
    "    df_sas = file.to_data_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# importing Stata files\n",
    "data = pd.read_stata('name.dta')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# importing HDF5 files\n",
    "import h5py\n",
    "filename = 'name.hdf5'\n",
    "data = h5py.File(filename, 'r')\n",
    "\n",
    "print(type(data))\n",
    "\n",
    "# structure\n",
    "for key in data.keys():\n",
    "    print(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# importing MATLAB files, keys = MATLAB variable names / values = objects assigned to variables\n",
    "import scipy.io\n",
    "filename = 'name.mat'\n",
    "mat = scipy.io.loadmat(filename)\n",
    "\n",
    "print(type(mat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# importing through SQL (sqlalchemy)\n",
    "\n",
    "from sqlalchemy import create_engine\n",
    "engine = create_engine('sqlite://name.sqlite')\n",
    "\n",
    "table_names = engine.table_names()\n",
    "print(table_names)\n",
    "\n",
    "# querying\n",
    "con = engine.connect()\n",
    "rs - con.execute('SELECT * FROM table')\n",
    "df = pd.DataFrame(rs.fetchall())\n",
    "df.columns = rs.keys() # setting the DF column names\n",
    "con.close()\n",
    "\n",
    "# through context manager\n",
    "with engine.connect() as con:\n",
    "    rs = con.execute('query')\n",
    "    df = pd.DataFrame(rs.fetchmany(size=5)) # fetching specific number\n",
    "    df.columns = rs.keys()\n",
    "\n",
    "# querying directly via pandas\n",
    "df = pd.read_sql_query('query', egine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploration & Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Data type constraints`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# feature classification\n",
    "\n",
    "def classify_features(df):\n",
    "    categorical_features = []\n",
    "    non_categorical_features = []\n",
    "    discrete_features = []\n",
    "    continuous_features = []\n",
    "\n",
    "    for column in df.columns:\n",
    "        if df[column].dtype in ['object', 'bool', 'category']: \n",
    "            if df[column].nunique() < 15:\n",
    "                categorical_features.append(column)\n",
    "            else: \n",
    "                non_categorical_features.append(column)\n",
    "        elif df[column].dtype in ['int64', 'float64']:\n",
    "            if df[column].nunique() < 10:\n",
    "                discrete_features.append(column)\n",
    "            else: \n",
    "                continuous_features.append(column)\n",
    "    return categorical_features, non_categorical_features, discrete_features, continuous_features\n",
    "\n",
    "\n",
    "categorical, non_categorical, discrete, continuous = classify_features(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Cleaning text data`\n",
    "\n",
    "- Data inconsistency\n",
    "- Fixed length violations\n",
    "- Typos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# checking for issues\n",
    "\n",
    "df['col'].value_counts()\n",
    "\n",
    "# using str.replace()\n",
    "\n",
    "df['col'] = df['col'].str.replace('old element','new element')\n",
    "\n",
    "# capitalize or lowercase \n",
    "\n",
    "df['col'] = df['col'].str.upper()\n",
    "df['col'] = df['col'].str.lower()\n",
    "\n",
    "# replacing elements with a condition\n",
    "\n",
    "element_len = df['col'].str.len()\n",
    "df.loc[element_len < number, 'col'] = np.nan # replace those falling under a specific condition with NaN\n",
    "\n",
    "# asserting column does not contain specific, unwanted elements\n",
    "\n",
    "assert df['col'].str.contains(\"element1|element2\").any() == False # either element\n",
    "\n",
    "# cleaning more complicated values: regular expressions\n",
    "\n",
    "df['col'] = df['col'].str.replace(r'regex','replacement')\n",
    "\"\"\"regex\n",
    "  (?x) # Use free-spacing mode.\n",
    "  <    # Match a literal '<'\n",
    "  /?   # Optionally match a '/'\n",
    "  \\[   # Match a literal '['\n",
    "  \\d+  # Match one or more digits\n",
    "  >    # Match a literal '>'\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Handling duplicate values`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# finding duplicate values\n",
    "\n",
    "duplicates = df.duplicated() # by itself, it can lead to misleading results\n",
    "df[duplicates]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# treating not fully duplicate rows (e.g. all but one or two columns are the exact same)\n",
    "\n",
    "# verification\n",
    "\n",
    "duplicates = df.duplicated(subset=column_names, keep=False)\n",
    "df[duplicates].sort_values(by='col1') # sorting to get a better picture of the duplicates\n",
    "\n",
    "# dropping complete records only\n",
    "\n",
    "df.drop_duplicates(inplace=True)\n",
    "\n",
    "# group by column names and produce statistical summaries to combine rows\n",
    "\n",
    "column_names = ['col1', 'col2', 'col3']\n",
    "summaries = {'col1': 'max', 'col2': 'mean'} # create a dictionary in order to merge nearly identical rows\n",
    "df = df.groupby(by=column_names).agg(summaries).reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Handling missing values`\n",
    "\n",
    "**Simple approaches**\n",
    "- Drop missing data\n",
    "- Impute with statistical measures *(mean, median, mode..)*\n",
    "\n",
    "**More complex approaches**\n",
    "- Imputing using an algorithmic approach\n",
    "- Impute with machine learning models\n",
    "\n",
    "**Visualization**\n",
    "- missingno package: package for visualizing and understanding missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import missingno as msno\n",
    "\n",
    "# visualization\n",
    "msno.matrix(df) # how missing values are distributed across columns\n",
    "plt.show()\n",
    "\n",
    "# isolate rows and validate\n",
    "missing = df[df['col'].isna()] # pair with .describe()\n",
    "complete = df[~df['col'].isna()] # pair with .describe()\n",
    "\n",
    "# sort \n",
    "\n",
    "sorted_df = df.sort_values(by = 'col')\n",
    "msno.matrix(sorted_df)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# checking the proportion of missing values to the respective totals\n",
    "\n",
    "df_nan = (df.isnull().sum() / len(df)) * 100\n",
    "df_nan = df_na.drop(df_na[df_na == 0].index).sort_values(ascending=False)[:30]\n",
    "\n",
    "missing_data = pd.DataFrame({'Missing Ratio' :df_na})\n",
    "missing_data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# set a threshold to drop missing values (missing values as a % of total)\n",
    "\n",
    "threshold = len(df) * 0.05\n",
    "\n",
    "cols_to_drop = df.columns[df.isna().sum() <= threhold]\n",
    "\n",
    "df.dropna(subset=cols_to_drop, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# imputation \n",
    "\n",
    "cols_with_missing_values = df.columns[df.isna().sum() > 0]\n",
    "\n",
    "for col in cols_with_missing_values[:-1]:\n",
    "    df[col].fillna(df[col].mode()[0])\n",
    "\n",
    "# option 2: simple imputation\n",
    "\n",
    "col_mean = df['col'].mean()\n",
    "df_imputed = df.fillna({'col': col_mean})\n",
    "\n",
    "# imputation by sub-group\n",
    "\n",
    "df_dict = df.groupby(\"col1\")[\"col2\"].median().to_dict()\n",
    "\n",
    "df[\"col2\"] = df[\"col2\"].fillna(df[\"col1\"].map(df_dict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Handling outliers`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# identifying thresholds\n",
    "\n",
    "# 75th percentile\n",
    "seventy_fifth = df[\"col\"].quantile(0.75)\n",
    "\n",
    "# 25th percentile\n",
    "twenty_fifth = df[\"col\"].quantile(0.25)\n",
    "\n",
    "# Interquartile range\n",
    "df_iqr = seventy_fifth - twenty_fifth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# identifying outliers\n",
    "\n",
    "# Upper threshold\n",
    "upper = seventy_fifth + (1.5 * df_iqr)\n",
    "\n",
    "# Lower threshold\n",
    "lower = twenty_fifth - (1.5 * df_iqr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# subsetting the data\n",
    "\n",
    "df[(df[\"col\"] < lower) | (df[\"col\"] > upper)] \\\n",
    "      [[\"col1\", \"col2\", \"col3\"]] # columns to display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# dropping outliers\n",
    "\n",
    "no_outliers = df[(df[\"col\"] > lower) | (df[\"col\"] < upper)]\n",
    "\n",
    "print(no_outliers[\"col\"].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Membership constraints`\n",
    "\n",
    "- Dropping data\n",
    "- Remapping categories\n",
    "- Inferring categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# finding inconsistent categories\n",
    "inconsistent_categories = set(df['col1']).difference(categories['col1']) # using set() gives us the unique values\n",
    "\n",
    "# get rows with inconsistent categories\n",
    "inconsistent_rows = df['col1'].isin(inconsistent_categories)\n",
    "df[inconsistent_rows] # subset based on boolean values\n",
    "\n",
    "# dropping inconsistent categories\n",
    "consistent_data = df[~inconsistent_rows] # returns everything except inconsistent rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# collapsing data into categories\n",
    "\n",
    "# creating category ranges and names\n",
    "ranges = [0, integer, integer, np.inf]\n",
    "group_names = ['label1','label2','label3']\n",
    "\n",
    "# creating new group column\n",
    "df['new_col'] = pd.cut(df['col1'], bins=ranges,\n",
    "                    labels=group_names)\n",
    "\n",
    "df[['new_col', 'col1']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# mapping categories to fewer ones\n",
    "\n",
    "# mapping dictionary and replace\n",
    "mapping = {'category1':'new_category1', 'category2':'new_category1', 'category3':'new_category1',\n",
    "            'category4':'new_category2', 'category5':'new_category2'}\n",
    "\n",
    "df['col'] = df['col'].replace(mapping)\n",
    "\n",
    "# verification\n",
    "df['col'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Date range constraints`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - Understand dataset before dropping values\n",
    " - Set custom minimums and maximums\n",
    " - Treat as missing and impute\n",
    " - Set custom values depending on business assumptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# where dates are in the future\n",
    "import datetime as dt\n",
    "\n",
    "# check data type to see if it needs conversion\n",
    "\n",
    "df.dtypes\n",
    "df['col'] = pd.to_datetime(df['col']).dt.date\n",
    "\n",
    "today_date = dt.date.today()\n",
    "df[df['col'] > dt.date.today()] # isolate data\n",
    "\n",
    "# option 1: drop the rows\n",
    "\n",
    "df = df[df['col'] < today_date]\n",
    "\n",
    "# option 2: drop values using .drop()\n",
    "\n",
    "df.drop(df[df['col'] > today_date].index, inplace = True)\n",
    "\n",
    "# option 3: hardcode dates with upper limit\n",
    "\n",
    "df.loc[df['col'] > today_date, 'col'] = today_date\n",
    "\n",
    "# assertion\n",
    "\n",
    "assert df.col.max().date() <= today_date # chaining with date() to get a date not a timestamp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Uniformity`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# convert to datetime\n",
    "\n",
    "df['col'] = pd.to_datetime(df['col'], \n",
    "                            # attempt to infer format of each date\n",
    "                            infer_datetime_format=True,\n",
    "                            # return Na for rows where conversion failed\n",
    "                            errors = 'coerce') # NaT\n",
    "\n",
    "# conversion using dt.strftime()\n",
    "\n",
    "df['col'] = df['col'].dt.strftime(\"%d-%m-%Y\") \n",
    "\n",
    "\"\"\"treating ambiguous dates\n",
    "Convert to NA and treat accordingly\n",
    "Infer format by understanding data source\n",
    "Infer format by understanding previous and subsequent data in DataFrame\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Cross field validation`\n",
    "\n",
    "- Use of multiple fields in the dataset to sanity check data integrity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# sum of columns to total sanity check\n",
    "\n",
    "sum_columns = df[['col1', 'col2', 'col3']].sum(axis = 1) # row-wise summing\n",
    "filter = sum_columns == df['total_column']\n",
    "\n",
    "# find and filter out rows with inconsistencies\n",
    "\n",
    "inconsistent_data = df[~filter]\n",
    "consistent_data = df[filter]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Categorical encoding`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# multi-column label encoding\n",
    "\n",
    "class MultiColumnLabelEncoder:\n",
    "    def __init__(self,columns = None):\n",
    "        self.columns = columns # column names to encode\n",
    "\n",
    "    def fit(self,X,y=None):\n",
    "        return self \n",
    "\n",
    "    def transform(self,X):\n",
    "        '''\n",
    "        Transforms columns of X specified in self.columns using\n",
    "        LabelEncoder(). If no columns specified, transforms all\n",
    "        columns in X.\n",
    "        '''\n",
    "        output = X.copy()\n",
    "        if self.columns is not None:\n",
    "            for col in self.columns:\n",
    "                output[col] = LabelEncoder().fit_transform(output[col])\n",
    "        else:\n",
    "            for colname,col in output.iteritems():\n",
    "                output[colname] = LabelEncoder().fit_transform(col)\n",
    "        return output\n",
    "\n",
    "    def fit_transform(self,X,y=None):\n",
    "        return self.fit(X,y).transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Normalization & Standardization`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Insights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Correlation`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# checking the strength of relationship between variables\n",
    "\n",
    "df.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# correlation heatmap\n",
    "\n",
    "sns.heatmap(df.corr(), annot=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Statistical analysis on scalar data`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "labels = [\"category1\", \"category2\", \"category3\", \"category4\"]\n",
    "\n",
    "bins = [0, twenty_fifth, median, seventy_fifth, maximum] # min, median, quartiles and max to be assigned to labels/categories\n",
    "\n",
    "pd.cut(data, # pass the data\n",
    "      labels, # set the labels\n",
    "      bins # provide the bins\n",
    "      )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Data summarization`\n",
    "\n",
    "- Groupings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# adding summary statistics to the dataframe\n",
    "\n",
    "# standard deviation example using lambda\n",
    "df[\"std_dev\"] = df.groupby(\"col\")[\"col1\"].transform(lambda x: x.std()) # for each x, transform to the respective standard deviation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graphs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Building"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Model Preparation`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# verifying class imbalance after data splitting\n",
    "\n",
    "classes = unique(target_train)\n",
    "total = len(target_train)\n",
    "for c in classes:\n",
    "    n_examples = len(target_train[target_train==c])\n",
    "    percent = n_examples / total * 100\n",
    "    print('> Class = %d : %d/%d (%.1f%%)' % (c, n_examples, total, percent))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Model Selection`\n",
    "\n",
    "- Benchmark ('Dummy') model\n",
    "- Classification v. Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Model creation and hyperparameter tuning`\n",
    "\n",
    "- GridsearchCV / RandomizedSearchCV\n",
    "- Feature importance\n",
    "- Metric selection\n",
    "- Bayesan optimization\n",
    "- Keras tuner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
